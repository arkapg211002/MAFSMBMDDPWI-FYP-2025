{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvgclO6bFKbs","executionInfo":{"status":"ok","timestamp":1743750115645,"user_tz":-330,"elapsed":11801,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"bb9b1087-b9a0-461f-edbf-ebc321f140a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.9.0\n"]}],"source":["!pip install tiktoken"]},{"cell_type":"markdown","source":["## Train the BPE tokenizer"],"metadata":{"id":"zJCSj1lCowLo"}},{"cell_type":"code","source":["from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n","from tokenizers.pre_tokenizers import Whitespace\n","from tokenizers.trainers import BpeTrainer\n","\n","# Initialize a tokenizer\n","tokenizer = Tokenizer(models.BPE())\n","\n","# Set pre-tokenizer\n","tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n","\n","# Trainer\n","trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\n","\n","# Train on files\n","tokenizer.train(files=[\"filtered_articles.txt\"], trainer=trainer)\n","\n","# Save tokenizer to directory\n","tokenizer.save(\"encoder.json\")\n","tokenizer.model.save(\".\", \"vocab\")  # This will save vocab.json and merges.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7JRvznwcon2N","executionInfo":{"status":"ok","timestamp":1743750729480,"user_tz":-330,"elapsed":44043,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"1441dfdc-e643-426e-bcae-e70368e77702"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./vocab-vocab.json', './vocab-merges.txt']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["\n","# Import Libraries\n","import tiktoken\n","import torch\n","import torch.nn as nn\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n","\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n","            GELU(), ## Activation\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        # 2*4*768\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x\n","        # 2*4*768\n","\n","class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","def generate_text_simple(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:]\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","        logits = logits[:, -1, :]\n","\n","        # New: Filter logits with top_k sampling\n","        if top_k is not None:\n","            # Keep only top_k values\n","            top_logits, _ = torch.topk(logits, top_k)\n","            min_val = top_logits[:, -1]\n","            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n","\n","        # New: Apply temperature scaling\n","        if temperature > 0.0:\n","            logits = logits / temperature\n","\n","            # Apply softmax to get probabilities\n","            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n","\n","            # Sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n","\n","        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n","        else:\n","            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n","\n","        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n","            break\n","\n","        # Same as before: append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n","\n","    return idx\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","file_path = \"filtered_articles.txt\"\n","\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    text_data = file.read()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","total_characters = len(text_data)\n","total_tokens = len(tokenizer.encode(text_data))\n","\n","print(\"Characters:\", total_characters)\n","print(\"Tokens:\", total_tokens)\n","\n","class GPTDatasetV1(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Tokenize the entire text\n","        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","\n","        # Use a sliding window to chunk the book into overlapping sequences of max_length\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n","\n","def create_dataloader_v1(txt, batch_size=16, max_length=1024,\n","                         stride=512, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader\n","\n","# Train/validation ratio\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","train_data = text_data[:split_idx]\n","val_data = text_data[split_idx:]\n","\n","\n","torch.manual_seed(123)\n","\n","train_loader = create_dataloader_v1(\n","    train_data,\n","    batch_size=4,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    val_data,\n","    batch_size=4,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval();\n","\n","def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # Reduce the number of batches to match the total number of batches in the data loader\n","        # if num_batches exceeds the number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","        else:\n","            break\n","    return total_loss / num_batches\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Epoch {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","        # Print a sample text after each epoch\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context\n","        )\n","\n","    return train_losses, val_losses, track_tokens_seen\n","\n","def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","    model.eval()\n","    with torch.no_grad():\n","        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n","        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n","    model.train()\n","    return train_loss, val_loss\n","\n","def generate_and_print_sample(model, tokenizer, device, start_context):\n","    model.eval()\n","    context_size = model.pos_emb.weight.shape[0]\n","    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n","    with torch.no_grad():\n","        token_ids = generate_text_simple(\n","            model=model, idx=encoded,\n","            max_new_tokens=50, context_size=context_size\n","        )\n","    decoded_text = token_ids_to_text(token_ids, tokenizer)\n","    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n","    model.train()\n","\n","def save_checkpoint(epoch, model, optimizer, save_path=\"model_checkpoint1.pth\"):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, save_path)\n","    print(f\"Checkpoint saved at epoch {epoch + 1}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UH0ladc7FTSn","executionInfo":{"status":"ok","timestamp":1743751048501,"user_tz":-330,"elapsed":72419,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"36360ba1-1562-479d-c6bf-b3a2a148ee4b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Characters: 121892397\n","Tokens: 26316250\n"]}]},{"cell_type":"code","source":["import tiktoken\n","from tokenizers import Tokenizer\n","\n","# Assuming you have saved your tokenizer using Tokenizer.save()\n","tokenizer = Tokenizer.from_file(\"./encoder.json\")\n","\n","# Get the vocabulary size\n","vocab_size = tokenizer.get_vocab_size()\n","\n","print(\"Vocabulary size:\", vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iXw326uFqRhF","executionInfo":{"status":"ok","timestamp":1743751084438,"user_tz":-330,"elapsed":37,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"963c17d4-56a6-4097-b718-70402587d74a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 30000\n"]}]},{"cell_type":"code","source":["from tokenizers import Tokenizer\n","\n","tokenizer = Tokenizer.from_file(\"encoder.json\")\n","vocab_size = tokenizer.get_vocab_size()\n","print(\"Vocabulary size:\", vocab_size)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PCu1GrxZqkbG","executionInfo":{"status":"ok","timestamp":1743751107780,"user_tz":-330,"elapsed":44,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"d32669ed-93f1-4f99-ca98-4238edb8414b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 30000\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"FwonaEfPHr1j"}},{"cell_type":"code","source":["!pip install importlib_metadata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jQbmlgkHs_a","executionInfo":{"status":"ok","timestamp":1743751097620,"user_tz":-330,"elapsed":2722,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"14ca65b0-ebe4-4bb0-f5b0-5e2431e44eaf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (8.6.1)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata) (3.21.0)\n"]}]},{"cell_type":"code","source":["import importlib_metadata\n","\n","print(\"TensorFlow version:\", importlib_metadata.version(\"tensorflow\"))\n","print(\"tqdm version:\", importlib_metadata.version(\"tqdm\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7PFfgt17ILe_","executionInfo":{"status":"ok","timestamp":1743751111171,"user_tz":-330,"elapsed":21,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"0142d367-d37a-425c-bef3-b67776d0ff6d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.18.0\n","tqdm version: 4.67.1\n"]}]},{"cell_type":"code","source":["from gpt_download import download_and_load_gpt2"],"metadata":{"id":"avn0TVgyIe9h","executionInfo":{"status":"ok","timestamp":1743750396124,"user_tz":-330,"elapsed":7166,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lGNK8gMzIoWq","executionInfo":{"status":"ok","timestamp":1743750554872,"user_tz":-330,"elapsed":152930,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"7fd0ef04-df1f-4672-d025-fcbb8474d0e6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 81.4kiB/s]\n","encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 729kiB/s]\n","hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 97.4kiB/s]\n","model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [02:23<00:00, 3.48MiB/s]\n","model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 5.23MiB/s]\n","model.ckpt.meta: 100%|██████████| 471k/471k [00:01<00:00, 432kiB/s]\n","vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 368kiB/s]\n"]}]},{"cell_type":"code","source":["print(\"Settings:\", settings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w17jZYm8I5hV","executionInfo":{"status":"ok","timestamp":1743751114400,"user_tz":-330,"elapsed":16,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"6c994891-e03f-4a59-dad7-c18eccbd79aa"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"]}]},{"cell_type":"code","source":["print(\"Parameter dictionary keys:\", params.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t8Ku9GIUI731","executionInfo":{"status":"ok","timestamp":1743751119287,"user_tz":-330,"elapsed":9,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"e2b25e29-5d8f-4e0e-a8b5-5793d29efd93"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"]}]},{"cell_type":"code","source":["print(params[\"wte\"])\n","print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13LSGCmCI-du","executionInfo":{"status":"ok","timestamp":1743751121689,"user_tz":-330,"elapsed":52,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"5126f83a-e092-4e7c-be7a-c8121eaa9dd3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n","   0.04531523]\n"," [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n","   0.04318958]\n"," [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n","  -0.08785918]\n"," ...\n"," [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n","  -0.06952604]\n"," [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n","  -0.02245961]\n"," [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n","   0.12067825]]\n","Token embedding weight tensor dimensions: (50257, 768)\n"]}]},{"cell_type":"code","source":["# Define model configurations in a dictionary for compactness\n","model_configs = {\n","    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n","    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n","    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n","    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n","}\n","\n","# Copy the base configuration and update with specific model settings\n","model_name = \"gpt2-small (124M)\"  # Example model name\n","NEW_CONFIG = GPT_CONFIG_124M.copy()\n","NEW_CONFIG.update(model_configs[model_name])\n","NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n","\n","gpt = GPTModel(NEW_CONFIG)\n","gpt.eval();"],"metadata":{"id":"UsgKZa0dJB_F","executionInfo":{"status":"ok","timestamp":1743751127502,"user_tz":-330,"elapsed":2808,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["print(NEW_CONFIG)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-X4AdjZSUGI","executionInfo":{"status":"ok","timestamp":1743751129802,"user_tz":-330,"elapsed":8,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"9eaaedfc-78bc-445c-c464-239583602aef"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': True}\n"]}]},{"cell_type":"code","source":["def assign(left, right):\n","    if left.shape != right.shape:\n","        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n","    return torch.nn.Parameter(torch.tensor(right))"],"metadata":{"id":"MWgmiHisJLUe","executionInfo":{"status":"ok","timestamp":1743751131283,"user_tz":-330,"elapsed":2,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def load_weights_into_gpt(gpt, params):\n","    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n","    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n","\n","    for b in range(len(params[\"blocks\"])):\n","        q_w, k_w, v_w = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n","        gpt.trf_blocks[b].att.W_query.weight = assign(\n","            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n","        gpt.trf_blocks[b].att.W_key.weight = assign(\n","            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n","        gpt.trf_blocks[b].att.W_value.weight = assign(\n","            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n","\n","        q_b, k_b, v_b = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n","        gpt.trf_blocks[b].att.W_query.bias = assign(\n","            gpt.trf_blocks[b].att.W_query.bias, q_b)\n","        gpt.trf_blocks[b].att.W_key.bias = assign(\n","            gpt.trf_blocks[b].att.W_key.bias, k_b)\n","        gpt.trf_blocks[b].att.W_value.bias = assign(\n","            gpt.trf_blocks[b].att.W_value.bias, v_b)\n","\n","        gpt.trf_blocks[b].att.out_proj.weight = assign(\n","            gpt.trf_blocks[b].att.out_proj.weight,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].att.out_proj.bias = assign(\n","            gpt.trf_blocks[b].att.out_proj.bias,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[0].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[0].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n","        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[2].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[2].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].norm1.scale = assign(\n","            gpt.trf_blocks[b].norm1.scale,\n","            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n","        gpt.trf_blocks[b].norm1.shift = assign(\n","            gpt.trf_blocks[b].norm1.shift,\n","            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n","        gpt.trf_blocks[b].norm2.scale = assign(\n","            gpt.trf_blocks[b].norm2.scale,\n","            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n","        gpt.trf_blocks[b].norm2.shift = assign(\n","            gpt.trf_blocks[b].norm2.shift,\n","            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n","\n","    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n","    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n","    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n","\n","\n","load_weights_into_gpt(gpt, params)\n","gpt.to(device);"],"metadata":{"id":"0h9AhdxLJSKA","executionInfo":{"status":"ok","timestamp":1743751133521,"user_tz":-330,"elapsed":552,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["print(device)\n","gpt.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49vv3IBiM6bA","executionInfo":{"status":"ok","timestamp":1743751140685,"user_tz":-330,"elapsed":21,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"c62d5511-6f1d-4556-c6b3-e802bf44807b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]},{"output_type":"execute_result","data":{"text/plain":["GPTModel(\n","  (tok_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(1024, 768)\n","  (drop_emb): Dropout(p=0.1, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (1): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (2): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (3): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (4): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (5): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (6): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (7): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (8): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (9): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (10): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (11): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","token_ids = generate_text_simple(\n","    model=gpt,\n","    idx=text_to_token_ids(\"How are you ?\", tokenizer).to(device),\n","    max_new_tokens=25,\n","    context_size=NEW_CONFIG[\"context_length\"],\n","    top_k=50,\n","    temperature=1.5\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M22yiOBzJUiC","executionInfo":{"status":"ok","timestamp":1743742449314,"user_tz":-330,"elapsed":4953,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"d7cf992a-e163-4253-e9d3-e3d05211745b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," How are you ?  Have you enjoyed listening ? In order for such a conversation to not become a real conversation , a reader to know the\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","def text_to_token_ids(text, tokenizer):\n","    # Use the tiktoken tokenizer instead of the 'tokenizers' tokenizer\n","    encoding = tiktoken.get_encoding(\"gpt2\").encode(text)\n","    encoded_tensor = torch.tensor(encoding).unsqueeze(0)  # add batch dimension\n","    return encoded_tensor\n","\n","token_ids = generate_text_simple(\n","    model=gpt,\n","    idx=text_to_token_ids(\"How are you ?\", tokenizer).to(device),\n","    max_new_tokens=25,\n","    context_size=NEW_CONFIG[\"context_length\"],\n","    top_k=50,\n","    temperature=1.5\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iEwMKfdzqwkI","executionInfo":{"status":"ok","timestamp":1743751204072,"user_tz":-330,"elapsed":4909,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"3c13e4b0-a936-40af-ea11-fd779f227cf7"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," cont Ψ ˹ previously İ 開 Highway ˹ immigration electron previously ث 情 ʰ ḵ ţ moral ǒ ν 腰 ţ ♏ moral ដ ţ finite ǒ น ŭ\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"RbpQWfWZQeFP"}},{"cell_type":"code","source":["# Save the model to a file\n","torch.save(gpt.state_dict(), \"foundation_model.pth\")"],"metadata":{"id":"doHutIMBQfyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(gpt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ux4ohvydRe85","executionInfo":{"status":"ok","timestamp":1743744535256,"user_tz":-330,"elapsed":75,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"8c08b6c4-5e47-4fff-b5d5-92b17aeefa77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPTModel(\n","  (tok_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(1024, 768)\n","  (drop_emb): Dropout(p=0.1, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (1): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (2): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (3): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (4): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (5): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (6): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (7): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (8): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (9): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (10): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (11): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",")\n"]}]},{"cell_type":"code","source":["!pip install torchinfo\n","from torchinfo import summary\n","\n","summary(gpt, input_size=(1, 128), dtypes=[torch.int])  # (batch_size, seq_len)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfqZ0dj2R0Ck","executionInfo":{"status":"ok","timestamp":1743744629926,"user_tz":-330,"elapsed":4806,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"92c36317-c500-4a64-ff5d-e18817b518ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]},{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","GPTModel                                 [1, 128, 50257]           --\n","├─Embedding: 1-1                         [1, 128, 768]             38,597,376\n","├─Embedding: 1-2                         [128, 768]                786,432\n","├─Dropout: 1-3                           [1, 128, 768]             --\n","├─Sequential: 1-4                        [1, 128, 768]             --\n","│    └─TransformerBlock: 2-1             [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-1               [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-2      [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-3                 [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-4               [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-5             [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-6                 [1, 128, 768]             --\n","│    └─TransformerBlock: 2-2             [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-7               [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-8      [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-9                 [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-10              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-11            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-12                [1, 128, 768]             --\n","│    └─TransformerBlock: 2-3             [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-13              [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-14     [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-15                [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-16              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-17            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-18                [1, 128, 768]             --\n","│    └─TransformerBlock: 2-4             [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-19              [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-20     [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-21                [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-22              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-23            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-24                [1, 128, 768]             --\n","│    └─TransformerBlock: 2-5             [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-25              [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-26     [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-27                [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-28              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-29            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-30                [1, 128, 768]             --\n","│    └─TransformerBlock: 2-6             [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-31              [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-32     [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-33                [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-34              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-35            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-36                [1, 128, 768]             --\n","│    └─TransformerBlock: 2-7             [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-37              [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-38     [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-39                [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-40              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-41            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-42                [1, 128, 768]             --\n","│    └─TransformerBlock: 2-8             [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-43              [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-44     [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-45                [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-46              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-47            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-48                [1, 128, 768]             --\n","│    └─TransformerBlock: 2-9             [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-49              [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-50     [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-51                [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-52              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-53            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-54                [1, 128, 768]             --\n","│    └─TransformerBlock: 2-10            [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-55              [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-56     [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-57                [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-58              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-59            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-60                [1, 128, 768]             --\n","│    └─TransformerBlock: 2-11            [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-61              [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-62     [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-63                [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-64              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-65            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-66                [1, 128, 768]             --\n","│    └─TransformerBlock: 2-12            [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-67              [1, 128, 768]             1,536\n","│    │    └─MultiHeadAttention: 3-68     [1, 128, 768]             2,362,368\n","│    │    └─Dropout: 3-69                [1, 128, 768]             --\n","│    │    └─LayerNorm: 3-70              [1, 128, 768]             1,536\n","│    │    └─FeedForward: 3-71            [1, 128, 768]             4,722,432\n","│    │    └─Dropout: 3-72                [1, 128, 768]             --\n","├─LayerNorm: 1-5                         [1, 128, 768]             1,536\n","├─Linear: 1-6                            [1, 128, 50257]           38,597,376\n","==========================================================================================\n","Total params: 163,037,184\n","Trainable params: 163,037,184\n","Non-trainable params: 0\n","Total mult-adds (Units.MEGABYTES): 262.88\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 157.63\n","Params size (MB): 652.15\n","Estimated Total Size (MB): 809.78\n","=========================================================================================="]},"metadata":{},"execution_count":22}]}]}