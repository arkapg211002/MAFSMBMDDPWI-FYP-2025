{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Stage 2 : Pretraining and Foundation Model"],"metadata":{"id":"9CkDLljpSz3u"}},{"cell_type":"markdown","source":["## GPT Architecture"],"metadata":{"id":"nwNN-wN7Tnh8"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"5JoWEIaGSqD6","executionInfo":{"status":"ok","timestamp":1743736734318,"user_tz":-330,"elapsed":18,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"outputs":[],"source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","\n","class DummyGPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        # Use a placeholder for TransformerBlock\n","        self.trf_blocks = nn.Sequential(\n","            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        # Use a placeholder for LayerNorm\n","        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","\n","class DummyTransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        # A simple placeholder\n","\n","    def forward(self, x):\n","        # This block does nothing and just returns its input.\n","        return x\n","\n","\n","class DummyLayerNorm(nn.Module):\n","    def __init__(self, normalized_shape, eps=1e-5):\n","        super().__init__()\n","        # The parameters here are just to mimic the LayerNorm interface.\n","\n","    def forward(self, x):\n","        # This layer does nothing and just returns its input.\n","        return x"],"metadata":{"id":"5qR2_x4kTpoK","executionInfo":{"status":"ok","timestamp":1743736750244,"user_tz":-330,"elapsed":3670,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Step 1 : Tokenisation"],"metadata":{"id":"cCW_nvi4TvAb"}},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swmwuUVFTxge","executionInfo":{"status":"ok","timestamp":1743736754249,"user_tz":-330,"elapsed":3214,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"9ead09f5-90ca-44b9-86ad-9b5496a3d982"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.9.0\n"]}]},{"cell_type":"code","source":["import tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","batch = []\n","txt1 = \"Every effort moves you\"\n","txt2 = \"Every day holds a\"\n","batch.append(torch.tensor(tokenizer.encode(txt1)))\n","batch.append(torch.tensor(tokenizer.encode(txt2)))\n","batch = torch.stack(batch, dim=0)\n","print(batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8Ejj-SYT1Xr","executionInfo":{"status":"ok","timestamp":1743736850030,"user_tz":-330,"elapsed":1569,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"c36b947f-0594-4695-87fc-84d267c59b46"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n"]}]},{"cell_type":"markdown","source":["### Step 2 : Create an Instance of dummy GPT model"],"metadata":{"id":"tR1X07x8T5HV"}},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = DummyGPTModel(GPT_CONFIG_124M)\n","logits = model(batch)\n","print(\"Output shape:\", logits.shape)\n","print(logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZfpGgE1T9Mt","executionInfo":{"status":"ok","timestamp":1743736438312,"user_tz":-330,"elapsed":649,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"4d7def48-6b5d-4ed6-d181-2d3825213532"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([2, 4, 50257])\n","tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n","         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n","         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n","         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n","\n","        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n","         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n","         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n","         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n","       grad_fn=<UnsafeViewBackward0>)\n"]}]},{"cell_type":"markdown","source":["## Layer Normalization"],"metadata":{"id":"I2DItnbKUCkI"}},{"cell_type":"code","source":["torch.manual_seed(123)\n","batch_example = torch.randn(2, 5) #A\n","layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n","out = layer(batch_example)\n","print(out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rNr50TD_UE_3","executionInfo":{"status":"ok","timestamp":1743736441639,"user_tz":-330,"elapsed":31,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"4c8fc268-3477-48bd-bda5-195887d65f67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n","        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n","       grad_fn=<ReluBackward0>)\n"]}]},{"cell_type":"code","source":["mean = out.mean(dim=-1, keepdim=True)\n","var = out.var(dim=-1, keepdim=True)\n","print(\"Mean:\\n\", mean)\n","print(\"Variance:\\n\", var)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpqooZLGUIyG","executionInfo":{"status":"ok","timestamp":1743736444292,"user_tz":-330,"elapsed":33,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"e0f36549-9bf0-44a3-bc4c-44eb6d060881"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean:\n"," tensor([[0.1324],\n","        [0.2170]], grad_fn=<MeanBackward1>)\n","Variance:\n"," tensor([[0.0231],\n","        [0.0398]], grad_fn=<VarBackward0>)\n"]}]},{"cell_type":"code","source":["out_norm = (out - mean) / torch.sqrt(var)\n","mean = out_norm.mean(dim=-1, keepdim=True)\n","var = out_norm.var(dim=-1, keepdim=True)\n","print(\"Normalized layer outputs:\\n\", out_norm)\n","print(\"Mean:\\n\", mean)\n","print(\"Variance:\\n\", var)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzPyp5p5ULrY","executionInfo":{"status":"ok","timestamp":1743736445618,"user_tz":-330,"elapsed":37,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"0b350819-f605-4ae1-da78-11e4cb17c359"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Normalized layer outputs:\n"," tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n","        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n","       grad_fn=<DivBackward0>)\n","Mean:\n"," tensor([[9.9341e-09],\n","        [1.9868e-08]], grad_fn=<MeanBackward1>)\n","Variance:\n"," tensor([[1.0000],\n","        [1.0000]], grad_fn=<VarBackward0>)\n"]}]},{"cell_type":"code","source":["torch.set_printoptions(sci_mode=False)\n","print(\"Mean:\\n\", mean)\n","print(\"Variance:\\n\", var)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYUqQl_YUPPH","executionInfo":{"status":"ok","timestamp":1743736448069,"user_tz":-330,"elapsed":28,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"9f186be9-06c6-4d27-bd22-ac456dab3c0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean:\n"," tensor([[    0.0000],\n","        [    0.0000]], grad_fn=<MeanBackward1>)\n","Variance:\n"," tensor([[1.0000],\n","        [1.0000]], grad_fn=<VarBackward0>)\n"]}]},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift"],"metadata":{"id":"2fEyNMFdUSUQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(batch_example)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"egxnG-wfUVGh","executionInfo":{"status":"ok","timestamp":1743736452268,"user_tz":-330,"elapsed":8,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"fba19463-b789-42b3-892e-484f3d554c29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n","        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n"]}]},{"cell_type":"code","source":["ln = LayerNorm(emb_dim=5)\n","out_ln = ln(batch_example)\n","mean = out_ln.mean(dim=-1, keepdim=True)\n","var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n","print(\"Mean:\\n\", mean)\n","print(\"Variance:\\n\", var)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"891pID9rUXI4","executionInfo":{"status":"ok","timestamp":1743736453417,"user_tz":-330,"elapsed":60,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"23c2f468-e56f-4b27-bc72-69b01917c7c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean:\n"," tensor([[    -0.0000],\n","        [     0.0000]], grad_fn=<MeanBackward1>)\n","Variance:\n"," tensor([[1.0000],\n","        [1.0000]], grad_fn=<VarBackward0>)\n"]}]},{"cell_type":"markdown","source":["## Feedforward Neural Network with GELU Activation"],"metadata":{"id":"s1W0nCBmUZ1h"}},{"cell_type":"code","source":["class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))"],"metadata":{"id":"KP_JagtIUfIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","gelu, relu = GELU(), nn.ReLU()\n","\n","# Some sample data\n","x = torch.linspace(-3, 3, 100)\n","y_gelu, y_relu = gelu(x), relu(x)\n","\n","plt.figure(figsize=(8, 3))\n","for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n","    plt.subplot(1, 2, i)\n","    plt.plot(x, y)\n","    plt.title(f\"{label} activation function\")\n","    plt.xlabel(\"x\")\n","    plt.ylabel(f\"{label}(x)\")\n","    plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"oZdSaD1zUjGy","executionInfo":{"status":"ok","timestamp":1743736458681,"user_tz":-330,"elapsed":420,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"1e4a2fc9-5361-48dd-a29e-49a8788cfde5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x300 with 2 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAxYAAAEiCAYAAABkykQ1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZ95JREFUeJzt3XlYVGX7B/DvDMuwCYogKCAqKooLIqShuZWKW0Up2aKiZqlh5ZIl/koz36Qyt9ytlCTNfSkzFU1ScwdR0SQXEBc2ZZVlGGbO7w9kEgFl2M6Z4fu5rrned86c5b5nch7uec7zPDJBEAQQERERERFVgVzsAIiIiIiISP+xsCAiIiIioipjYUFERERERFXGwoKIiIiIiKqMhQUREREREVUZCwsiIiIiIqoyFhZERERERFRlLCyIiIiIiKjKWFgQEREREVGVsbAgKsPnn38OmUwmyrVDQ0Mhk8kQHx9f69cuLCzExx9/DBcXF8jlcvj7+9d6DBUh5ntERHXb6NGj0axZM1GuLWbb9ODBA4wbNw6Ojo6QyWSYPHmyKHE8jZjvEbGwqJPi4uIwadIktG7dGhYWFrCwsICHhweCgoJw4cKFEvsW/wMt75GUlAQAiI+Ph0wmw7ffflvudZs1a4YhQ4aU+drZs2chk8kQGhpabXk+TW5uLj7//HNERETU2jUfNW/ePOzatUuUa5dn7dq1mD9/PoYNG4affvoJU6ZMETUeKb5HRIasuGgvfhgbG8PJyQmjR4/GnTt3KnXOiIgIyGQybNu2rdx9ZDIZJk2aVOZr27Ztg0wmq9Xv6rt37+Lzzz9HdHR0rV2zmNhtU3nmzZuH0NBQTJw4EWFhYRg5cqRosUj1PSLAWOwAqHbt2bMHw4cPh7GxMd566y14enpCLpfjypUr2LFjB1auXIm4uDi4urqWOG7lypWwsrIqdb769evXUuTVLzc3F3PmzAEA9O7du8Rrn376KWbMmFGj1583bx6GDRtWqldg5MiReP3116FQKGr0+mX5888/4eTkhEWLFtX6tcsixfeIqC744osv0Lx5c+Tn5+PkyZMIDQ3FsWPHEBMTAzMzM7HDq3F3797FnDlz0KxZM3Tq1KnEa99//z00Gk2NXVvstqk8f/75J5599lnMnj1blOs/SqrvEbGwqFOuX7+O119/Ha6urjh06BAaN25c4vWvv/4aK1asgFxeuiNr2LBhsLOzq61QRWdsbAxjY3H+eRgZGcHIyEiUa6ekpOhFsSjme0RUFwwcOBA+Pj4AgHHjxsHOzg5ff/01fv31V7z22msiRycuExMT0a4tZtuUkpICDw8PUa6tCzHfI+KtUHXKN998g5ycHKxbt65UUQEU/WP84IMP4OLiIkJ0FZOWloaPPvoIHTp0gJWVFaytrTFw4ECcP3++1L75+fn4/PPP0bp1a5iZmaFx48Z49dVXcf36dcTHx8Pe3h4AMGfOHG23/+effw6g9D2a7du3R58+fUpdQ6PRwMnJCcOGDdNu+/bbb9GtWzc0bNgQ5ubm8Pb2LnULgEwmQ05ODn766SfttUePHg2g/PEDK1asQLt27aBQKNCkSRMEBQUhIyOjxD69e/dG+/btcfnyZfTp0wcWFhZwcnLCN99888T3tfhWtsOHD+PSpUvamCIiIrS3MTze5Vx8zKO3r40ePRpWVla4c+cO/P39YWVlBXt7e3z00UdQq9Wl3rslS5agQ4cOMDMzg729PQYMGICzZ89K8j0iqst69OgBoOgHqkdduXIFw4YNg62tLczMzODj44Nff/1VjBBx8+ZNvPfee3B3d4e5uTkaNmyIgICAMsdiZWRkYMqUKWjWrBkUCgWcnZ0xatQo3Lt3DxEREXjmmWcAAGPGjNF+/xR/1z06xkKlUsHW1hZjxowpdY2srCyYmZnho48+AgAUFBRg1qxZ8Pb2ho2NDSwtLdGjRw8cPnxYe4yubRNQNDZu7ty5cHNzg0KhQLNmzTBz5kwolcoS+xXfjnzs2DF06dIFZmZmaNGiBdavX//E97W4DYiLi8Pvv/+ujSk+Pr7c7+Ky2g1dvnurs/2ujfeI/sPCog7Zs2cPWrZsia5du+p8bFpaGu7du1fi8fgfbLXhxo0b2LVrF4YMGYKFCxdi+vTpuHjxInr16oW7d+9q91Or1RgyZAjmzJkDb29vLFiwAB9++CEyMzMRExMDe3t7rFy5EgDwyiuvICwsDGFhYXj11VfLvO7w4cNx5MgR7ZiSYseOHcPdu3fx+uuva7ctWbIEXl5e+OKLLzBv3jwYGxsjICAAv//+u3afsLAwKBQK9OjRQ3vt8ePHl5v3559/jqCgIDRp0gQLFizA0KFDsXr1avTv3x8qlarEvunp6RgwYAA8PT2xYMECtGnTBp988gn++OOPcs9vb2+PsLAwtGnTBs7OztqY2rZtW+4x5VGr1fDz80PDhg3x7bffolevXliwYAHWrFlTYr+3334bkydPhouLC77++mvMmDEDZmZmOHnypCTfI6K6rPgPxwYNGmi3Xbp0Cc8++yz++ecfzJgxAwsWLIClpSX8/f2xc+fOWo/xzJkzOH78OF5//XV89913mDBhAg4dOoTevXsjNzdXu9+DBw/Qo0cPLF26FP3798eSJUswYcIEXLlyBbdv30bbtm3xxRdfAADeffdd7fdPz549S13TxMQEr7zyCnbt2oWCgoISr+3atQtKpVLbPmRlZeGHH35A79698fXXX+Pzzz9Hamoq/Pz8tGM5dG2bgKIepVmzZqFz585YtGgRevXqhZCQkBLtUrFr165h2LBh6NevHxYsWIAGDRpg9OjRuHTpUrnnb9u2LcLCwmBnZ4dOnTppYyr+414XFfnure72uzbeI3qEQHVCZmamAEDw9/cv9Vp6erqQmpqqfeTm5mpfmz17tgCgzIe7u7t2v7i4OAGAMH/+/HJjcHV1FQYPHlzma2fOnBEACOvWrXtiHvn5+YJarS6xLS4uTlAoFMIXX3yh3bZ27VoBgLBw4cJS59BoNIIgCEJqaqoAQJg9e3apfYrzLhYbGysAEJYuXVpiv/fee0+wsrIq8Z49+v8FQRAKCgqE9u3bC88//3yJ7ZaWlkJgYGCpa69bt04AIMTFxQmCIAgpKSmCqamp0L9//xK5L1u2TAAgrF27VrutV69eAgBh/fr12m1KpVJwdHQUhg4dWupaj+vVq5fQrl27EtsOHz4sABAOHz5cYnvxZ/7oZxYYGCgAKPFZCIIgeHl5Cd7e3trnf/75pwBA+OCDD0rFUPz5CII03yMiQ1b8b+vgwYNCamqqcOvWLWHbtm2Cvb29oFAohFu3bmn3feGFF4QOHToI+fn52m0ajUbo1q2b0KpVK+224u+QrVu3lntdAEJQUFCZr23durXM76DHPf7dKwiCcOLEiVL/3mfNmiUAEHbs2FFq/+Lvnye1SYGBgYKrq6v2+f79+wUAwm+//VZiv0GDBgktWrTQPi8sLBSUSmWJfdLT0wUHBwdh7Nix2m26tE3R0dECAGHcuHEl9vvoo48EAMKff/6p3ebq6ioAEI4cOaLdlpKSIigUCmHatGmlrvW4strwx7+Li5XVblT0u7e62+/afI9IENhjUUdkZWUBQJkDsHv37g17e3vtY/ny5aX22b59O8LDw0s81q1bV+NxP06hUGjHgKjVaty/fx9WVlZwd3dHVFRUiXjt7Ozw/vvvlzpHZaaha926NTp16oTNmzdrt6nVamzbtg0vvvgizM3Ntdsf/f/p6enIzMxEjx49SsSni4MHD6KgoACTJ08uMf7lnXfegbW1dYmeEKDoMx4xYoT2uampKbp06YIbN25U6vqVMWHChBLPe/ToUeL627dvh0wmK3MQYGU+H318j4ikrG/fvrC3t4eLiwuGDRsGS0tL/Prrr3B2dgZQ1Iv9559/4rXXXkN2dra2J/v+/fvw8/PD1atXKz2LVGU9+t2rUqlw//59tGzZEvXr1y/VPnh6euKVV14pdY7KfP88//zzsLOzK9E+pKenIzw8HMOHD9duMzIygqmpKYCiW0HT0tJQWFgIHx+fSrcPe/fuBQBMnTq1xPZp06YBQKnvPg8PD+1tbUBRD4m7u3utffdV5Lu3uttvfXuP9B1Ht9QR9erVA1DUBfy41atXIzs7G8nJySX+wT+qZ8+etTJ4+2lfGsX35a9YsQJxcXEl7ttv2LCh9v9fv34d7u7u1TqAa/jw4Zg5cybu3LkDJycnREREICUlpUTDARTdcva///0P0dHRJe7frOy82jdv3gQAuLu7l9huamqKFi1aaF8v5uzsXOpaDRo0KDWVcE0pHi/x+PXT09O1z69fv44mTZrA1ta2Wq6pb+8RkdQtX74crVu3RmZmJtauXYsjR46UmIXt2rVrEAQBn332GT777LMyz5GSkgInJ6dqi+lp36F5eXkICQnBunXrcOfOHQiCoH0tMzNT+/+vX7+OoUOHVltcxsbGGDp0KDZu3AilUgmFQoEdO3ZApVKVah9++uknLFiwAFeuXClxi2bz5s0rde2bN29CLpejZcuWJbY7Ojqifv36pb77mjZtWuocj38/16SKfPdWd/utb++RvmNhUUfY2NigcePGiImJKfVa8ZiLml5szMzMDHl5eWW+Vnz/69OmMZw3bx4+++wzjB07FnPnzoWtrS3kcjkmT55co9P/AUWFRXBwMLZu3YrJkydjy5YtsLGxwYABA7T7HD16FC+99BJ69uyJFStWoHHjxjAxMcG6deuwcePGGo2vWHmzJT3ayOqivMb88cHYT7u+lFT3e0RkaLp06aKdFcrf3x/PPfcc3nzzTcTGxsLKykr7ffvRRx/Bz8+vzHM8/ofckygUiiq3D++//z7WrVuHyZMnw9fXFzY2NpDJZHj99ddrvH14/fXXsXr1avzxxx/w9/fHli1b0KZNG3h6emr3+fnnnzF69Gj4+/tj+vTpaNSoEYyMjBASElJqULyuKvrDlVTbh9r47hXrPaprWFjUIYMHD8YPP/yA06dPo0uXLrV+fVdXV1y+fLnM12JjY7X7PMm2bdvQp08f/PjjjyW2Z2RklOhRcXNzw6lTp6BSqcqdGlDXHoTmzZujS5cu2Lx5MyZNmoQdO3bA39+/xK9427dvh5mZGfbv319ie1m3jVX0+sXvSWxsLFq0aKHdXlBQgLi4OPTt21enPHRVPFjz8cH6j//Kows3Nzfs378faWlpT+y10Jf3iMiQFf/x26dPHyxbtgwzZszQ/jszMTGpln9frq6u2nbgcbq0D4GBgViwYIF2W35+fqnvLjc3tzJ/ZHuUru1Dz5490bhxY2zevBnPPfcc/vzzT/zf//1fqfhatGiBHTt2lDj/47eE6nJtV1dXaDQaXL16tcRkG8nJycjIyHjqe1ZVNdU+VGf7LfZ7VNdwjEUd8vHHH8PCwgJjx45FcnJyqddruhofNGgQbt++XWolZaVSiR9++AGNGjVC586dn3gOIyOjUnFu3bq11L28Q4cOxb1797Bs2bJS5yg+3sLCAkDpL8QnGT58OE6ePIm1a9fi3r17pbq5jYyMIJPJSvxaEx8fX+bq0ZaWlhW6dt++fWFqaorvvvuuRO4//vgjMjMzMXjw4ArHXxmurq4wMjLCkSNHSmxfsWJFpc85dOhQCIKgXeDoUY/mqC/vEZGh6927N7p06YLFixcjPz8fjRo1Qu/evbF69WokJiaW2j81NVWn8w8aNAgnT55EZGRkie0ZGRnYsGEDOnXqBEdHxyeeo6z2YenSpaV+PR86dCjOnz9f5sxVxcdbWlpqr18Rcrkcw4YNw2+//YawsDAUFhaW2T48eg0AOHXqFE6cOFFiP13apkGDBgEAFi9eXGL7woULAaDGv/vc3NwAoET7oFarS80CqIvqbr/Ffo/qGvZY1CGtWrXCxo0b8cYbb8Dd3V278rYgCIiLi8PGjRshl8u1g/MetW3btjIHfvfr1w8ODg7a54cOHUJ+fn6p/fz9/fHuu+9i7dq1CAgIwNixY+Hl5YX79+9j8+bNiImJwfr167UD28ozZMgQfPHFFxgzZgy6deuGixcvYsOGDSV+pQaAUaNGYf369Zg6dSpOnz6NHj16ICcnBwcPHsR7772Hl19+Gebm5vDw8MDmzZvRunVr2Nraon379mjfvn2513/ttdfw0Ucf4aOPPoKtrW2pX+oGDx6MhQsXYsCAAXjzzTeRkpKC5cuXo2XLlqXu3/f29sbBgwexcOFCNGnSBM2bNy9zKmB7e3sEBwdjzpw5GDBgAF566SXExsZixYoVeOaZZ8odF1NdbGxsEBAQgKVLl0Imk8HNzQ179uxBSkpKpc/Zp08fjBw5Et999x2uXr2KAQMGQKPR4OjRo+jTpw8mTZoEQH/eI6K6YPr06QgICEBoaCgmTJiA5cuX47nnnkOHDh3wzjvvoEWLFkhOTsaJEydw+/btUusLbd++HVeuXCl13sDAQMyYMQNbt25Fz549MX78eLRp0wZ3795FaGgoEhMTKzRZyJAhQxAWFgYbGxt4eHjgxIkTOHjwYInxd8V5bNu2TdsWeXt7Iy0tDb/++itWrVoFT09PuLm5oX79+li1ahXq1asHS0tLdO3a9YljIYYPH46lS5di9uzZ6NChQ6npuocMGYIdO3bglVdeweDBgxEXF4dVq1bBw8OjxPhHXdomT09PBAYGYs2aNcjIyECvXr1w+vRp/PTTT/D39y9z/aXq1K5dOzz77LMIDg7W9kBv2rQJhYWFlT5ndbffYr9HdU4tz0JFEnDt2jVh4sSJQsuWLQUzMzPB3NxcaNOmjTBhwgQhOjq6xL5Pmm4Wj0wlVzz1aHmPsLAwQRCKptabMmWK0Lx5c8HExESwtrYW+vTpI/zxxx8Vij0/P1+YNm2a0LhxY8Hc3Fzo3r27cOLECaFXr15Cr169Suybm5sr/N///Z/2Wo6OjsKwYcOE69eva/c5fvy44O3tLZiampaYuu7x6eoe1b179zKnriv2448/Cq1atRIUCoXQpk0bYd26dWWe78qVK0LPnj0Fc3NzAYB2WtXypu9btmyZ0KZNG8HExERwcHAQJk6cKKSnp5fYp6zpYgWh9PSI5Snv+NTUVGHo0KGChYWF0KBBA2H8+PFCTExMmdPNWlpaljq+rPwLCwuF+fPnC23atBFMTU0Fe3t7YeDAgUJkZKR2Hym+R0SGrPjf1pkzZ0q9plarBTc3N8HNzU0oLCwUBEEQrl+/LowaNUpwdHQUTExMBCcnJ2HIkCHCtm3btMcVTz1a3uPo0aOCIAjC7du3hXHjxglOTk6CsbGxYGtrKwwZMkQ4efJkhWJPT08XxowZI9jZ2QlWVlaCn5+fcOXKFcHV1bXUtNX3798XJk2aJDg5OQmmpqaCs7OzEBgYKNy7d0+7z+7duwUPDw/B2Ni4xHdded8VGo1GcHFxEQAI//vf/8p8fd68eYKrq6ugUCgELy8vYc+ePWWeT5e2SaVSCXPmzNG2dS4uLkJwcHCJaYAFofwp38tqP8tS3vHXr18X+vbtKygUCsHBwUGYOXOmEB4eXuZ0sxX97q3u9ru23iMSBJkgcDQKERERERFVDcdYEBERERFRlbGwICIiIiKiKmNhQUREREREVcbCgoiIiIiIqoyFBRERERERVRkLCyIiIiIiqrI6t0CeRqPB3bt3Ua9ePZ2WhCciMmSCICA7OxtNmjSBXF53f3NiG0FEVJIu7UOdKyzu3r0LFxcXscMgIpKkW7duwdnZWewwRMM2goiobBVpH+pcYVGvXj0ARW+OtbW1TseqVCocOHAA/fv3h4mJSU2EVysMIQ/mIB2GkIch5ABULY+srCy4uLhovyPrqrreRjAH6TCEPAwhB8Aw8qit9qHOFRbFXdvW1taVajQsLCxgbW2tt/9hAYaRB3OQDkPIwxByAKonj7p++09dbyOYg3QYQh6GkANgGHnUVvtQd2+kJSIiIiKiasPCgoiIiIiIqkzUwmLlypXo2LGjtsvZ19cXf/zxxxOP2bp1K9q0aQMzMzN06NABe/furaVoiYiotrB9ICLSP6IWFs7Ozvjqq68QGRmJs2fP4vnnn8fLL7+MS5culbn/8ePH8cYbb+Dtt9/GuXPn4O/vD39/f8TExNRy5EREVJPYPhAR6R9RC4sXX3wRgwYNQqtWrdC6dWt8+eWXsLKywsmTJ8vcf8mSJRgwYACmT5+Otm3bYu7cuejcuTOWLVtWy5ETEVFNYvtARKR/JDMrlFqtxtatW5GTkwNfX98y9zlx4gSmTp1aYpufnx927dpV7nmVSiWUSqX2eVZWFoCi0fEqlUqnGIv31/U4qTGEPJiDdBhCHgaRg1qDL/ZcRmt15fKQcu411T4QEdUVR6/ew593ZRgoCDV6HdELi4sXL8LX1xf5+fmwsrLCzp074eHhUea+SUlJcHBwKLHNwcEBSUlJ5Z4/JCQEc+bMKbX9wIEDsLCwqFTM4eHhlTpOagwhD+YgHYaQhz7nsOWGHH8ny9FQYQQb03AY69gfnZubWzOBVUFNtw8Af3x6HHOQDkPIwxByAPQ/j5tpuZi85QKy8o3gcyYBr3dx1el4XfIWvbBwd3dHdHQ0MjMzsW3bNgQGBuKvv/4qt/HQVXBwcIlfsYoX+ejfv3+l5igPDw9Hv3799HYeY8Aw8mAO0mEIeeh7Dj+fSsDfJ65ABuCVZhoM9NM9j+I/qKWkptsHgD8+lYc5SIch5GEIOQD6mYdSDSyKMUJWvgyuVgIsUi5h796yx6qVR5cfnkQvLExNTdGyZUsAgLe3N86cOYMlS5Zg9erVpfZ1dHREcnJyiW3JyclwdHQs9/wKhQIKhaLUdhMTk0r/AVGVY6XEEPJgDtJhCHnoYw5Hr6bif3tjAQDT+rWCy4N/KpWHFPOu6fYB4I9Pj2MO0mEIeRhCDoD+5iEIAiZvuYDE3GQ0tDTF2Na5Nf7Dk+iFxeM0Gk2JbulH+fr64tChQ5g8ebJ2W3h4eLn33BIRGbIbqQ8QtCEKao2AVzs74d0ezfDHH/+IHVaNqYn2gT8+lY05SIch5GEIOQD6l8eqv65jb0wyjOUyLHvDEymXTtT4D0+iFhbBwcEYOHAgmjZtiuzsbGzcuBERERHYv38/AGDUqFFwcnJCSEgIAODDDz9Er169sGDBAgwePBibNm3C2bNnsWbNGjHTICKqdZm5Koz76Syy8gvRuWl9zHulA2TQiB1WtWH7QERUeUf+TcU3+64AAGa/1A4+rg2g4x1QlSJqYZGSkoJRo0YhMTERNjY26NixI/bv349+/foBABISEiCX/zcCsVu3bti4cSM+/fRTzJw5E61atcKuXbvQvn17sVIgIqp1hWoNJv0ShRv3ctDExgyrR/rAzMQIKpXhFBZsH4iIKifhfi7e/+UcNAIQ4O2MEV2borCwsFauLWph8eOPPz7x9YiIiFLbAgICEBAQUEMRERFJ3/9+/wdHr96DuYkRvg/0gX290rfy6Du2D0REusstKMS7YWeRmaeCp0t9zPVvD5lMVmvXF3WBPCIi0s3GUwkIPR4PAFg03BPtmtiIGxAREUmCIAj4ZPtFXEnKhp2VKVaN6AwzE6NajYGFBRGRnjhx/T5m7Y4BAEzr1xoD2jcWOSIiIpKKH47G4bfzd2Esl2HFW95obGNe6zGwsCAi0gMJ93MxcUMkCjUCXvRsgknPtxQ7JCIikohjV+8h5OGsgJ8N8UCX5raixMHCgohI4rLzVRi3/gwyclXo6GyD+cM61uo9s0REJF230nIx6ZcoaARgmLczRvnqtrJ2dWJhQUQkYWqNgMmbovFv8gM4WCvw/SifWr9nloiIpCmvQI3xYZHaH57+V8uDtR/HwoKISMLm74/FoSspUBjLsWakDxyszcQOiYiIJEAQBMzYcQGXE7PQ0NIUq0Z4i/7DEwsLIiKJ2hF1G6v+ug4A+GZYR3i61Bc3ICIikowfj8Vhd/RdGMllWP5WZzSpX/uDtR/HwoKISILOJaRjxo6LAICgPm54uZOTyBEREZFUHL92DyF/FK2s/engtni2RUORIyrCwoKISGISM/PwblgkCgo16OfhgGn93MUOiYiIJOJ2ei4m/XIOao2AVzs7YXS3ZmKHpMXCgohIQvJVary7PhKp2Uq0cayHxcM7QS7nDFBERFTURowPi0RaTgHaO1lj3isdJDVLIAsLIiKJEAQB07ddwMU7mbC1NMX3o3xgqTAWOywiIpIAQRAwc8dFXLqbBVuJDNZ+HAsLIiKJWBFx/ZFVUzvDxdZC7JCIiEgiQo/HY8e5OzCSy7DsTS84N5BeG8HCgohIAsIvJ+PbA7EAgDkvt5PMQDwiIhLfyRv38b/fi1bWnjmoLbq52YkcUdlYWBARiSw2KRuTN52DIACjfF3xVlfxVk0lIiJpuZORh6ANUVBrBPh3aoKx3ZuJHVK5WFgQEYkoPacA49afQU6BGr4tGuKzIR5ih0RERBKRr1Jj4s+RuJ9TAI/G1gh5taOkBms/joUFEZFIVGoN3tsQhVtpeXCxNceKtzrDxIhfy0REVDRY+/92xuDC7Uw0sDDB6pHeMDeV1mDtx7EFIyISyf/2XMaJG/dhaWqEH0Y9gwaWpmKHREREErH+xE1sj7oNuQxY9qZ+TOjBwoKISAS/nE7ATyduAgAWDe8Ed8d6IkdERERScerGfczdcxkAEDywLbq3lOZg7ceJWliEhITgmWeeQb169dCoUSP4+/sjNjb2iceEhoZCJpOVeJiZmdVSxEREVXcmPg2zdscAAD7q3xr92zmKHBEREUlFYmYegjZGoVAj4CXPJhjXo7nYIVWYqIXFX3/9haCgIJw8eRLh4eFQqVTo378/cnJynnictbU1EhMTtY+bN2/WUsRERFVzJyMPE8IioVILGNyxMYL6tBQ7JCIikoh8lRoTwiJx70EB2ja2xtdDpT1Y+3GiFhb79u3D6NGj0a5dO3h6eiI0NBQJCQmIjIx84nEymQyOjo7ah4ODQy1FTERUeXkFaowPO6ud3WP+MP1qMGoTe7SJqK4RBAGf7YrB+duZsDE3weoR0h+s/ThJjbHIzMwEANja2j5xvwcPHsDV1RUuLi54+eWXcenSpdoIj4io0gRBwCfbLyDmThZsLU2xZpQ3LEyNxQ5LstijTUR1zc+nErA1sniwtheaNpT+YO3HSaZV02g0mDx5Mrp374727duXu5+7uzvWrl2Ljh07IjMzE99++y26deuGS5cuwdnZudT+SqUSSqVS+zwrKwsAoFKpoFKpdIqxeH9dj5MaQ8iDOUiHIeRRGzmsORqHX8/fhbFchu+Gd4SDlUm1X68qeUjt89u3b1+J56GhoWjUqBEiIyPRs2fPco8r7tEmItInZ+LTMOfXoh/KPxnQBj1a2YscUeVIprAICgpCTEwMjh079sT9fH194evrq33erVs3tG3bFqtXr8bcuXNL7R8SEoI5c+aU2n7gwAFYWFSuEgwPD6/UcVJjCHkwB+kwhDxqKofL6TKsuSIHIIO/ayHu/3MSe/+pkUsBqFweubm5NRBJ9dG1R1uj0aBz586YN28e2rVrVxshEhFVSnJWPt7bUDRYe3DHxni3ZwuxQ6o0SRQWkyZNwp49e3DkyJEyex2exMTEBF5eXrh27VqZrwcHB2Pq1Kna51lZWXBxcUH//v1hbW2t07VUKhXCw8PRr18/mJiY6HSslBhCHsxBOgwhj5rMIe5eDj5dfQoCCjHcxxlzX2pbY+MqqpJHcW+uFNVUjzbAXu3HMQfpMIQ8DCEHoGbzUBZqMD7sLFKzlXB3sMKXL7VFYWFhtV+ntnq0RS0sBEHA+++/j507dyIiIgLNm+s+nZZarcbFixcxaNCgMl9XKBRQKBSltpuYmFT6D4iqHCslhpAHc5AOQ8ijunPIzldh4sZoZOcXwse1Aeb6d4Cpcc0PbatMHlL+7GqqRxtgr3Z5mIN0GEIehpADUDN5bLouR3SKHBZGAl5rkoG/Dh2o9ms8qqZ7tEUtLIKCgrBx40bs3r0b9erVQ1JSEgDAxsYG5ubmAIBRo0bByckJISEhAIAvvvgCzz77LFq2bImMjAzMnz8fN2/exLhx40TLg4jocRqNgCmbo3E9NQeNbcywcoR3rRQVhqYme7QB9mo/jjlIhyHkYQg5ADWXx6Yzt3HixGXIZMCyt7zRo1XNLYJXWz3aohYWK1euBAD07t27xPZ169Zh9OjRAICEhATI5f81xunp6XjnnXeQlJSEBg0awNvbG8ePH4eHh0dthU1E9FSLDv6Lg/+kQGEsx+qR3rCvV7rnlMpXGz3aAHu1y8McpMMQ8jCEHIDqzSPyZjq++L1osN10P3c879G4Ws77NDXdoy36rVBPExERUeL5okWLsGjRohqKiIio6v64mIilfxb9Sh7yagd0dK4vbkB6iD3aRGSokrPyMfHnooVSB3VwxMRebmKHVG0kMXibiMhQXEnKwrSt5wEAbz/XHK921u32HSrCHm0iMkQFhRpM/DkSKdlKtHawwvxhnga1UCoLCyKiapKRW4B310cit0CNbm4NETywjdgh6S32aBORIZrz2yVEJWTA2swYa0b6wFJhWH+KcyQhEVE1UGsEvP/LOSSk5cK5gTmWvdkZxkb8iiUioiKbTidgw6kEyGTAkte90MzOUuyQqh1bPSKiajB/fyyOXr0HMxM51oz0ga2lqdghERGRREQlpGPW7qKVtT/q744+bRqJHFHNYGFBRFRFey7cxaq/rgMA5g/zhEcT3aYpJSIiw5WSXTRYu0CtwYB2jnivt+EM1n4cCwsioir4JzEL07deAACM79UCL3o2ETkiIiKSioJCDYI2RCE5S4lWjazw7WuGNVj7cSwsiIgqKSO3AOPDIpGnUqNHKzt87MfB2kRE9J+5ey7jTHw66imMsXqkN6wMbLD241hYEBFVgloj4INN0UhIy4WLrTmWvuEFI7nh/gpFRES62XLmFsJO3iwarP1GJ7SwtxI7pBrHwoKIqBIWHIjFkX9TYWYix+oRPqhvwcHaRERUJPpWBj7dFQMAmNK3NZ5v4yByRLWDhQURkY7+uJiIFRFFg7W/HtqRg7WJiEgrNVuJCWFFg7X7ezhgUp+WYodUa1hYEBHp4GpyNj56uLL2uOea4+VOTiJHREREUqFSFw3WTsrKh5u9JRa85gl5HbpNloUFEVEFZeWrMD4sEjkPV9aewZW1iYjoEV/+/g9Ox6fBSmGMNaN8UM/MROyQahULCyKiCtBoBEzdfB437uXAqX7RYG2urE1ERMW2Rd5G6PF4AMCi4Z3gVgcGaz+OrSIRUQUsO3wNB/9JhqmxHCtHdEZDK4XYIRERkURcuJ2BmTsvAgAm922Ffh51Y7D241hYEBE9xeErKVh08F8AwP/826Ojc31xAyIiIsm49+DhYO1CDfq2bYQPnm8ldkiiYWFBRPQEN+/n4MNN5yAIwFtdm+I1HxexQyIiIokoHqx9NzMfLewtsXB4pzo1WPtxLCyIiMqRV6DGhJ+jkJVfCK+m9THrRQ+xQyIiIgmZt/cfnIp7OFh7pA+s69hg7cexsCAiKoMgCJi58yL+ScyCnZUpVr7lDYWxkdhhERGRROyIuo11f8cDABa85omWjereYO3HsbAgIirD+hM3sfPcHRjJZVj2Zmc42piJHRIREUlEzJ1MBO8oGqz9wfMt4dfOUeSIpEHUwiIkJATPPPMM6tWrh0aNGsHf3x+xsbFPPW7r1q1o06YNzMzM0KFDB+zdu7cWoiWiuiLyZhrm7rkMAAge2AbPtmgockRERCQV9x8oMT4sEspCDV5o0wiT+7YWOyTJELWw+OuvvxAUFISTJ08iPDwcKpUK/fv3R05OTrnHHD9+HG+88QbefvttnDt3Dv7+/vD390dMTEwtRk5EhiolOx/vbYhCoUbA4I6N8fZzzcUOiYiIJKJQrcGkjedwJyMPze04WPtxxmJefN++fSWeh4aGolGjRoiMjETPnj3LPGbJkiUYMGAApk+fDgCYO3cuwsPDsWzZMqxatarGYyYiw6V62GAkZynRqpEVvhnaETIZGwwiIioS8scVnLhxH5amRlg90hs25nV7sPbjRC0sHpeZmQkAsLW1LXefEydOYOrUqSW2+fn5YdeuXWXur1QqoVQqtc+zsrIAACqVCiqVSqf4ivfX9TipMYQ8mIN0GEIexbF/sy8Wp+PSYKkwwtLXPWEqF/Qqr6p8FlLLMyQkBDt27MCVK1dgbm6Obt264euvv4a7u/sTj9u6dSs+++wzxMfHo1WrVvj6668xaNCgWoqaiAzZ7ui7+PFYHICiwdqtHeqJHJH0SKaw0Gg0mDx5Mrp374727duXu19SUhIcHEquZujg4ICkpKQy9w8JCcGcOXNKbT9w4AAsLCwqFWt4eHiljpMaQ8iDOUiHvudx7r4Mof/eAgAMdy1A7Jm/8PQRX9JUmc8iNze3BiKpvOJbZZ955hkUFhZi5syZ6N+/Py5fvgxLS8syjym+VTYkJARDhgzBxo0b4e/vj6ioqCe2K0RET3M7B/hud9HYu0l9WmJA+8YiRyRNkiksgoKCEBMTg2PHjlXreYODg0v0cGRlZcHFxQX9+/eHtbW1TudSqVQIDw9Hv379YGKiv11fhpAHc5AOQ8gjNjEDH686BQAY91wzfOKnnwPxqvJZFPfmSgVvlSUiqUjLKcCPsUZQFmrQ290eU/rpZxtRGyRRWEyaNAl79uzBkSNH4Ozs/MR9HR0dkZycXGJbcnIyHB3LnuZLoVBAoVCU2m5iYlLpP4KqcqyUGEIezEE69DWPHGUhJm+9BKVGhi7NGmDGwLYwNtLvmbgr81lI/bOriVtliYieplCtwZQtF5CmlKGprTmWDPeCEQdrl0vUwkIQBLz//vvYuXMnIiIi0Lz502df8fX1xaFDhzB58mTttvDwcPj6+tZgpERkiARBwIwdF3EtNQfWJgIWv9ZR74sKQ1RTt8oCHIf3OOYgHYaQhyHk8NW+WBy/kQZTuYClr7WHhYl+5lNbY/BELSyCgoKwceNG7N69G/Xq1dN++dvY2MDc3BwAMGrUKDg5OSEkJAQA8OGHH6JXr15YsGABBg8ejE2bNuHs2bNYs2aNaHkQkX766Xg8fjt/F8ZyGca0LoR9vdK9myS+mrpVFuA4vPIwB+kwhDz0NYeoezL8dNUIAPBWSw3iz59A/HmRg6qimh6DJ2phsXLlSgBA7969S2xft24dRo8eDQBISEiAXP7fL4jdunXDxo0b8emnn2LmzJlo1aoVdu3axYF5RKSTqIR0fLn3HwDAx36t4ZBxSeSIqCw1easswHF4j2MO0mEIeehzDv8kZuOT708B0GBc96booLmhl3kUq60xeKLfCvU0ERERpbYFBAQgICCgBiIiorrg/gMlgjZEQaUWMLhDY4z2bYo//mBhISW1dassx+GVjTlIhyHkoW85pOcUIGhTNPJVGvRoZYeP+rtj/74bepdHWWp6DJ4kBm8TEdUWtUbA5M3RSMzMRwt7S3w1tAO4Bp708FZZIhJDoVqDDzadw620PDS1tcDSNzhYWxccpUhEdcqSQ1dx9Oo9mJsYYdUIb9Qz0+9fnwzVypUrkZmZid69e6Nx48bax+bNm7X7JCQkIDExUfu8+FbZNWvWwNPTE9u2beOtskSkk/kHYrVtxOqR3qhvYSp2SHqlUj0WcXFxOHr0KG7evInc3FzY29vDy8sLvr6+MDMzq+4YiYiqRURsCpb+eRUAMO/V9lw1VcJ4qywR1bY9F+5i9V83AADzAzqibWPdxlmRjoXFhg0bsGTJEpw9exYODg5o0qQJzM3NkZaWhuvXr8PMzAxvvfUWPvnkE7i6utZUzEREOruTkYfJm6MhCMBbXZviFa8nDwQmIqK645/ELEzfegEAML5nCwzp2ETkiPRThQsLLy8vmJqaYvTo0di+fTtcXFxKvK5UKnHixAls2rQJPj4+WLFiBX81IiJJKCjU4L0NUcjIVaGjsw1mveghdkgGjb3aRKRPMnILMD4sEnkqNXq0ssPHA9qIHZLeqnBh8dVXX8HPz6/c1xUKBXr37o3evXvjyy+/RHx8fHXER0RUZfP2/oPztzJgY26C5W92hsLYSOyQDBJ7tYlI36g1Aj7YFI2EtFw4NzDHd69zsHZVVLiweFJR8biGDRuiYcOGlQqIiKg6/X4hEaHH4wEAC1/zhItt5RY9oydjrzYR6aMFB2Jx5N9UmJnIsXqkNxpYcrB2VVRqVqjQ0NAytxcWFiI4OLgq8RARVZsbqQ/wyfaie2Yn9nbDC20dRI7IcH311Vc4deoU3nvvvVJFBfBfr/aqVatw5coVtGjRQoQoiYj+s/diIlZEXAcAfD20I9o1sRE5Iv1XqcLigw8+QEBAANLT07XbYmNj0bVrV/zyyy/VFhwRUWXlFajx3oYoPFAWoktzW0zr11rskAyarr3a3t7eNRgNEdGTxSZl46Ot5wEA7/Rojpc7OYkckWGoVGFx7tw53L59Gx06dEB4eDiWL1+Ozp07o02bNjh//nx1x0hEpLPZv8bgSlI27KxMsewNLxgbcdme2sJebSKSssxcFcaHnUVugRrd3BriEw7WrjaVamnd3Nzw999/49VXX8WAAQMwZcoU/PDDD9iwYQNsbNiNRETi2nr2FracvQ25DPjudS80suZMRLWJvdpEJFVqjYAPN59D/P1cONU3x7I3O/OHp2pU6Xfy999/x6ZNm+Dr64v69evjxx9/xN27d6szNiIincUmZeOz3TEAgCl9W6NbSzuRI6p72KtNRFK1KPxfRMSmQmFcNFjbloO1q1WlCovx48cjICAAn3zyCY4ePYoLFy7A1NQUHTp0wJYtW6o7RiKiCslRFmLihkjkqzTo2doeQX1aih1SncRebSKSon0xiVh2+BoA4KuhHdDeid9H1a1ShcXff/+NU6dOYdq0aZDJZHB0dMTevXvxxRdfYOzYsdUdIxHRUwmCgJk7L+JGag4crc2weHgnyDkXuWjYq01EUnI1ORvTthT1mI7t3hyveDmLHJFhqlRhERkZCU9Pz1Lbg4KCEBkZWeWgiIh09cvpW9gdfRdGchmWvenF7m0RsVebiKQkM0+Fd8MikVOgxrMtbBE8iIO1a0qFF8h7lEKhKPc1d3f3SgdDRFQZMXcy8flvlwAAH/u5w6eZrcgR1W3FvdrFP0AV92ovX74cY8eOxWuvvSZyhERUV2g0AqZsjkbcvRw0sTHD8jc7w4SDtWtMhd/ZAQMG4OTJk0/dLzs7G19//TWWL19epcCIiCoiO1+FSRujUFCowQttGuGdHlx4TWzs1SYiqVh86Cr+vJLycLC2Dxpalf/jOFVdhXssAgICMHToUNjY2ODFF1+Ej48PmjRpAjMzM6Snp+Py5cs4duwY9u7di8GDB2P+/Pk1GTcREQRBwIwdF7XTBi54zZPjKiSAvdpEJAX7LyXhu0NXAQDzXumADs4crF3TKtxj8fbbb+PGjRuYOXMmLl++jHfffRc9evTAM888Az8/P3z//fdo2rQpzpw5g82bN6Np06ZPPeeRI0fw4osvokmTJpDJZNi1a9cT94+IiIBMJiv1SEpKqmgaRGRAfj55E79fSISxXIalb3qhvgXHVYiFvdpEJCXXUv4brD26WzMM9eZg7dqg0xgLhUKBESNGYMSIEQCAzMxM5OXloWHDhjAxMdH54jk5OfD09MTYsWPx6quvVvi42NhYWFtba583atRI52sTkX67eDsTc/f8AwCYMbANOjdtIHJEdRt7tYlIKrLyiwZrP1AWomtzW/zf4LZih1RnVGrwdjEbG5sqzUk+cOBADBw4UOfjGjVqhPr161f6ukSk37LyVQjaGIUCtQb9PBzw9nPNxQ6pznv77bcxYsQIbN26FZs3b8aaNWuQmZkJAJDJZPDw8ICfnx/OnDmDtm3ZyBNRzdBoBEzdHI0bqTlobGOG5W9xsHZt0qmw+O6778rcbmNjg9atW8PX17dagnqaTp06QalUon379vj888/RvXv3cvdVKpVQKpXa51lZWQAAlUoFlUql03WL99f1OKkxhDyYg3TUdh6CIODjrReQkJYLp/pmCPH3QGFhYZXOyc+ienKv7l5tIiJdfffnVRz8JwWmxnKsGuENOw7WrlU6FRaLFi0qc3tGRgYyMzPRrVs3/Prrr7C1rZmpHhs3boxVq1bBx8cHSqUSP/zwA3r37o1Tp06hc+fOZR4TEhKCOXPmlNp+4MABWFhYVCqO8PDwSh0nNYaQB3OQjtrK42iSDPvijGAkEzDc+QH+Plx9163Ln0Vubm61x1HVXm0iIl2EX07G4oNFg7W/9G8PT5f64gZUB+lUWMTFxZX72o0bNzBixAh8+umnWLFiRZUDK4u7u3uJGUW6deuG69evY9GiRQgLCyvzmODgYEydOlX7PCsrCy4uLujfv3+JcRoVoVKpEB4ejn79+un1r2+GkAdzkI7azOPS3Sx8tOYUAAGfDGiDMd1cq+W8/Cz+682tiuru1T5y5Ajmz5+PyMhIJCYmYufOnfD39y93/4iICPTp06fU9sTERDg6Oup0bSLSL9dTH2Dq5mgAQKCvKwJ8XMQNqI6q0hiLR7Vo0QJfffUVxo4dW12nrJAuXbrg2LFj5b6uUCjKnPrQxMSk0n9AVOVYKTGEPJiDdNR0Hln5Kny45QJUagF92zrgnZ5ukMmqd2rZuvxZVEfe1d2rzQk+iKgisvNVeHf9WWQrC9GlmS0+HeIhdkh1VrUVFgDQtGnTWp/6NTo6Go0bN67VaxJR7RIEAcHbL+Lmw/Uqvg3oWO1FBVVddfdqc4IPInoajUbAtC3ncT01B47WZlj2lhcHa4uoWguLixcvwtW14rcmPHjwANeuXdM+j4uLQ3R0NGxtbdG0aVMEBwfjzp07WL9+PQBg8eLFaN68Odq1a4f8/Hz88MMP+PPPP3HgwIHqTIOIJObnUwn4/WLRehXLuF6FXqrNXm1dJvggIv22/PA1HLicDFMjOVaN9EajemZih1Sn6VRYlHcPbmZmJiIjIzFt2jQEBgZW+Hxnz54tcT9s8ViIwMBAhIaGIjExEQkJCdrXCwoKMG3aNNy5cwcWFhbo2LEjDh48WOY9tURkGGLuZGLub5cBAJ8MaAMvrleht2q6V7syE3xw5sCSmIN0GEIeNZ3D4dhULDz4LwDg8xfbop2jZY1cq65/Froco1NhUb9+/XJvP5DJZBg3bhxmzJhR4fP17t0bgiCU+3poaGiJ5x9//DE+/vjjCp+fiPRbdr4Kkx6uV/FCm0YY14PrVegzXXu1dVWZCT44c2DZmIN0GEIeNZFDSh6w8KIRBEGG7g4aWCafx96956v9Oo+qq5+FLrMG6lRYHD58uMzt1tbWaNWqFczMzJCSkoImTZrocloiolIEQcDMnTGIv5+LJjZm+DbAk+MqJK66e7Wrw9Mm+ODMgSUxB+kwhDxqKocHykIErD6FPHUOvJvWx5oxPjA1rrlxFXX9s9Bl1kCdCotevXo98fXz58+jc+fOUKvVupyWiKiUX07fwm/n78JILsPSN73QwJLjKqSuunu1q8PTJvjgzIFlYw7SYQh5VGcOgiAgeNMFXEvNgYO1AitHesPSvHYWwaurn4Uu+1fr4G0iourwT2IW5vx2CQAw3c8d3q41s+gmVa/q7tXmBB9E9LgVEdex71ISTIxkWDmCg7WlhoUFEUlKjrIQQRujoCzUoLe7Pd7t0ULskKiCqrtXmxN8ENGjDsem4NsDsQCAOS+1R2dO5iE5LCyISDIEQcCnu2Jw4+F85Atf6wS5nOMq6ipO8EFExeLv5eDDX85BEIA3ujTFm12bih0SlUGnwuLChQtPfD02NrZKwRBR3bb17G3sPHcHRnIZvnvDC7YcV0FEVOflKAsxPiwSWfmF8GpaH5+/xJW1pUqnwqJTp06QyWRl/oJUvJ2zthBRZfybnI1Zv8YAAKb2a40uzTmugoiorhMEAR9vu4DY5GzY11Ng1QhvKIyNxA6LyqFTYREXF1dTcRBRHZZbUIigDVHIV2nQo5UdJvZyEzskqgT2ahNRdVv11w38fjGxaLD2W53hYM3B2lKmU2FRkwsbEVHdNXv3JVxNeYBG9RRYNJzjKvQVe7WJqDr99W8qvtl/BQAw+8V28GnGnmyp06mw+Oabb/D+++/D3NwcAPD333/Dx8dHOwd4dnY2PvnkE6xYsaL6IyUig7Q98ja2Rt6GXAYsed0Ldla1Mx85VT/2ahNRdbl5PwcfPBysPdzHBW9xsLZe0KmwCA4OxujRo7WFxcCBAxEdHY0WLYqmg8zNzcXq1atZWBBRhVxLycanu4rGVUzu2xq+bg1Fjoiqgr3aRFQdcguKBmtn5qnQyaU+vvBvx95OPaHT+uePd28/aRpAIqInyStQI2jDOeSp1OjesiGC+rQUOySqRkePHsWIESPg6+uLO3fuAADCwsJw7NgxkSMjIikrHqx9JSkbdlYKrBzRmYO19YhOhQURUXX5/NdLiE0uajgWD/eCEcdVGIzt27fDz88P5ubmOHfuHJRKJQAgMzMT8+bNEzk6IpKy74/ewJ4LiTCWy7ByRGc0tjEXOyTSAQsLIqp1O6JuY/PZW5DJgO9e7wT7ehxXYUj+97//YdWqVfj+++9hYmKi3d69e3dERUWJGBkRSdmxq/fw1R/Fg7U98AwHa+sdnVfe/uGHH2BlZQUAKCwsRGhoKOzs7AAUDd4mInqSaynZ+L+dReMqPnyhFbq1tBM5IqpusbGx6NmzZ6ntNjY2yMjIqP2AiEjybqXlYtIvUdAIwGs+zhjxLMds6SOdCoumTZvi+++/1z53dHREWFhYqX2IiMry6LiKbm4N8f7zrcQOiWqAo6Mjrl27hmbNmpXYfuzYMe1kH0RExfIK1Hg3LBIZuSp4Otvgi5fbc7C2ntKpsIiPj6+hMIioLpj9a8x/4ype78RxFQbqnXfewYcffoi1a9dCJpPh7t27OHHiBKZNm4ZZs2aJHR4RSYggCPhk+wX8k5gFOytTrBrpDTMTDtbWVzoVFvn5+Th48CCGDBkCoGj62eJBeQBgbGyML774AmZmXBWRiEraHnkbW84WrVfx3eud0KgevycM1YwZM6DRaPDCCy8gNzcXPXv2hEKhwPTp0zFu3DixwyMiCfnxWBx+PX8XxnIZlr/Jwdr6TqfB26GhoVi9erX2+bJly3D8+HGcO3cO586dQ1hYmE5rWBw5cgQvvvgimjRpAplMhl27dj31mIiICHTu3BkKhQItW7ZEaGioLikQkQiuJv+3XsWHL7TmuAoDJ5PJ8H//939IS0tDTEwMTp48idTUVNjY2KB58+Zih0dEEnH82j3M2/sPAODTwW3RtQXXMtJ3OhUWGzZswLvvvlti28aNG3H48GEcPnwY8+fPx9atWyt8vpycHHh6emL58uUV2j8uLg6DBw9Gnz59EB0djcmTJ2PcuHHYv3+/LmkQUS3KLSjEexuikKdS47mWdpj0PNerMFRKpRLBwcHw8fFB9+7dsXfvXnh4eODSpUtwd3fHkiVLMGXKFLHDJCIJuJWWi6CNRYO1h3Z2RmC3ZmKHRNVAp1uhrl27hg4dOmifm5mZQS7/rzbp0qULgoKCKny+gQMHYuDAgRXef9WqVWjevDkWLFgAAGjbti2OHTuGRYsWwc/Pr8LnIaLaIQgCPt0Vg6spD2BfT4FFwzmuwpDNmjULq1evRt++fXH8+HEEBARgzJgxOHnyJBYsWICAgAAYGfHeaaK6Lq9AjfFhkUjPVaGjsw2+fIWDtQ2FToVFRkZGiTEVqampJV7XaDQlXq9uJ06cQN++fUts8/Pzw+TJk2vsmkRUeVvP3saOqDuQy4Clb3hxvQoDt3XrVqxfvx4vvfQSYmJi0LFjRxQWFuL8+fP8o4GIABT94DRz50VcTsxCQ0tTrBrBwdqGRKfCwtnZGTExMXB3dy/z9QsXLsDZ2blaAitLUlISHBwcSmxzcHBAVlYW8vLyYG5eesCPUqksUexkZWUBAFQqFVQqlU7XL95f1+OkxhDyYA7SUV4eV5Ky8dnuonEVU15oCW8Xa8nmauifhS7HVsXt27fh7e0NAGjfvj0UCgWmTJnCooKItNb+HY+d5+7ASC7Dsjc7o0l9DtY2JDoVFoMGDcKsWbMwePDgUjM/5eXlYc6cORg8eHC1BlhVISEhmDNnTqntBw4cgIWFRaXOGR4eXtWwJMEQ8mAO0vFoHvlqYMEFIygLZWhbXwPnB1ewd+8VEaOrGEP8LCoqNze3ytdVq9UwNTXVPjc2NtYuqEpEdPx6ycHavm4crG1odCosZs6ciS1btsDd3R2TJk1C69atARStsrps2TIUFhZi5syZNRIoULToUnJycoltycnJsLa2LrO3AiiaEnfq1Kna51lZWXBxcUH//v1hbW2t0/VVKhXCw8PRr18/mJiY6J6ARBhCHsxBOh7PQxAETN5yASn5yXC0VuCnib5oYGH69BOJyFA/C10U9+ZWhSAIGD16NBSKolve8vPzMWHCBFhaWpbYb8eOHVW+FhHplzsZeZi08RzUGgGvejlhNAdrGySdCgsHBwccP34cEydOxIwZMyAIAoCiqQX79euHFStWlLpVqTr5+vpi7969JbaFh4fD19e33GMUCoW2kXuUiYlJpf+AqMqxUmIIeTAH6SjOI/TvOOyNSYaxXIYVI7zRyMby6QdLhKF9FroeU1WBgYElno8YMaJK5zty5Ajmz5+PyMhIJCYmYufOnfD393/iMREREZg6dSouXboEFxcXfPrppxg9enSV4iCiqslXqTEhLBJpOQVo72SNea924C2SBkqnwgIAmjdvjn379iEtLQ3Xrl0DALRs2RK2trY6X/zBgwfacwBF08lGR0fD1tYWTZs2RXBwMO7cuYP169cDACZMmIBly5bh448/xtixY/Hnn39iy5Yt+P3333W+NhFVv6iEdHz5sJt75qC26Ny0gcgRUW1at25dtZ6veErysWPH4tVXX33q/sVTkk+YMAEbNmzAoUOHMG7cODRu3JgzBxKJRBCAWb9exsU7mbDlYG2Dp3NhUczW1hZdunSp0sXPnj2LPn36aJ8X37IUGBiI0NBQJCYmIiEhQft68+bN8fvvv2PKlClYsmQJnJ2d8cMPP7DBIJKAtJwCTNoQBZVawKAOjhjTvZnYIZGe45TkRPrvaJIMO+MTHw7W9oJzg8qNbyX9UOnCojr07t1beztVWcpaVbt37944d+5cDUZFRLrSCMC0bRdxNzMfze0s8fXQjuzmplpXmSnJOXNgScxBOgwhjxPXUrEzvmi9s0/8WuOZpjZ6mY8hfBa1NWugqIUFERmG/bflOHb7PsxM5Fg5ojPqmen/OAXSP5WZkpwzB5aNOUiHvuaRrgS+vWAEDWTwttOgUfol7N17SeywqkRfP4tH1fSsgSwsiKhKjly9h/23i3onQl7tgDaOus22RiQmzhxYEnOQDn3OQ6lS480fz+BBYRacLASseac3rC3Mnn6gROnzZ1GstmYNZGFBRJV2Oz0X07ZehAAZ3uzijFe8am6BTKKnqcyU5Jw5sGzMQTr0LQ9BEDBz12VcuJOF+uYmeNs9D9YWZnqVQ3n07bMoS03PGijXNSAiIqBo+sCJP0chI0+FppYCZg5sI3ZIVMf5+vri0KFDJbY9bUpyIqpeP5+8ia2RtyGXAYuHd0RD/e2ooEpgYUFEOhMEAbN2x+DinUw0sDDBGHc1FMb8OqHq9eDBA0RHRyM6OhrAf1OSF88WGBwcjFGjRmn3nzBhAm7cuIGPP/4YV65cwYoVK7BlyxZMmTJFjPCJ6pzTcWmY89tlAMAnA9qgO1fWrnP4lwAR6WzTmVvYcvbhL1KvdYRt6TtJiKrs7Nmz8PLygpeXF4CiKcm9vLwwa9YsACh3SvLw8HB4enpiwYIFnJKcqJYkZubhvQ2RKNQIeNGzCd7t2ULskEgEHGNBRDo5l5CO2buLZvb4yM8d3dwaYm+syEGRQeKU5ET6IV+lxoSfo3DvQQHaONbD10O5snZdxR4LIqqwlOx8TPw5CgVqDfzaOWBiLzexQyIiIhEJgoDZuy/h/K0M2JibYM1IH1iY8nfruoqFBRFVSEGhBkEbopCUlQ83e0t8G+DJX6SIiOq4DacSsPnsLchlwNI3vNC0IVfWrstYWBBRhXz5+2WciU+HlcIYa0b5cBE8IqI67mx8Gub8VnRr7McD2qBna3uRIyKxsbAgoqfacvYWfjpxEwCwaHgnuNlbiRwRERGJKSkzHxN+joJKLWBwh8YYz8HaBBYWRPQUUQnp+HRnDADgwxdaoZ+Hg8gRERGRmJSFakzcEIl7D5Rwd6iHb4Z15K2xBICFBRE9QXJWPiaERaJArUF/Dwd8+EIrsUMiIiKRff7rJZxLyIC1mTFWj/SGpYKDtakICwsiKlO+So13wyKRkq1EawcrLBzeCXI5f5EiIqrLNp5KwC+nb0EmA757wwvN7CzFDokkhIUFEZUiCAKCd1zUTh/4/SgfWPEXKSKiOi3yZjpm/1p0a+xH/d3R272RyBGR1LCwIKJSVv51HTvP3YGRXIYVb3WGa0P+IkVEVJclZ+Vj4s+RUKkFDGzviPd6cx0jKo2FBRGVcOBSEubvL1pK+/MXPdC9pZ3IERERkZgKCjWY+HPRrbGtGllhPtcxonKwsCAirct3szB5czQEARjxbFOM9G0mdkhERCSyOb9dQlRCBuqZFa1jxFtjqTwsLIgIQFE399s/nUFugRrd3Bpi9ovtxA6JiIhEtul0AjacSigarP26F5pzsDY9gSQKi+XLl6NZs2YwMzND165dcfr06XL3DQ0NhUwmK/EwMzOrxWiJDE9uQSHG/XQWiZn5cLO3xMq3vGFiJImvByIiEklUQjpm7S5aWXtq39bo04aDtenJRP/LYfPmzZg6dSpmz56NqKgoeHp6ws/PDykpKeUeY21tjcTERO3j5s2btRgxkWHRaARM2RyNi3cyYWtpirWjn4GNhYnYYRERkYhSsosGaxeoNfBr54CgPi3FDon0gOiFxcKFC/HOO+9gzJgx8PDwwKpVq2BhYYG1a9eWe4xMJoOjo6P24eDAlYCJKuvLvf9g/6VkmBrJsWakN2eAIiKq4woKNQjaEIXkLCVaNrLCgte4jhFVjKijbwoKChAZGYng4GDtNrlcjr59++LEiRPlHvfgwQO4urpCo9Ggc+fOmDdvHtq1K/t+cKVSCaVSqX2elZUFAFCpVFCpVDrFW7y/rsdJjSHkwRyqR+iJm/jxWBwA4KtX28HTqV6d/HdhCDkAVctD33Mnouozd89lnIlPRz2FMdaM9OZgbaowUf9LuXfvHtRqdakeBwcHB1y5cqXMY9zd3bF27Vp07NgRmZmZ+Pbbb9GtWzdcunQJzs7OpfYPCQnBnDlzSm0/cOAALCwsKhV3eHh4pY6TGkPIgzlU3vn7Mqz7Vw5AhpeaqmF0+xz23j5X6fPxs5COyuSRm5tbA5EQkb7ZcuYWwk4W3WK+aHgntLC3Ejki0id6V4L6+vrC19dX+7xbt25o27YtVq9ejblz55baPzg4GFOnTtU+z8rKgouLC/r37w9ra2udrq1SqRAeHo5+/frBxER/70E3hDyYQ9WcvZmODaGREKDBm12c8fmQtpWek5yfhXRUJY/i3lwiqruib2Xg011FK2tP6dsafT14qznpRtTCws7ODkZGRkhOTi6xPTk5GY6OjhU6h4mJCby8vHDt2rUyX1coFFAoFGUeV9k/IKpyrJQYQh7MQXexSdkY//M5KAs16Nu2Eb54uQOMq2EGKH4W0lGZPAwhbyKqvNRsJSaEFQ3W7ufhgPef52Bt0p2og7dNTU3h7e2NQ4cOabdpNBocOnSoRK/Ek6jValy8eBGNGzeuqTCJDMbt9FyMWnsKWfmF8HZtgKVvdK6WooKIiPSXSq1B0MYoJGXlo4W9JRa+5snB2lQpov9FMXXqVHz//ff46aef8M8//2DixInIycnBmDFjAACjRo0qMbj7iy++wIEDB3Djxg1ERUVhxIgRuHnzJsaNGydWCkR64f4DJUatPY3kLCVaNbLCj4E+MDc1EjssoifiOkdENe/L3//B6bg0WCmMsWakD+qZsQeTKkf0MRbDhw9HamoqZs2ahaSkJHTq1An79u3TDuhOSEiAXP5f/ZOeno533nkHSUlJaNCgAby9vXH8+HF4eHiIlQKR5GXlqzBq7WncSM1BExszrH+7C+pbmIodFtETFa9ztGrVKnTt2hWLFy+Gn58fYmNj0ahR2Qt1WVtbIzY2Vvu8smOHiOqKbZG3EXo8HkDRYO2WjThYmypP9MICACZNmoRJkyaV+VpERESJ54sWLcKiRYtqISoiw5BXoMbboWdw6W4WGlqaImxcVzS2MRc7LKKnenSdIwBYtWoVfv/9d6xduxYzZswo85jidY6I6Oku3s7EzJ0XAQAfvtAK/ThYm6pIEoUFEdUMZaEa43+OLJqP3MwY69/uAjdOHUh6oDbWOQK41tHjmIN01HQe93MK8G7YWRQUavC8uz3e69ms2q/Fz0I6amudIxYWRAaqoFCD936OwpF/U2FuYoTQMc+gXRMbscMiqpDaWOcI4FpH5WEO0lETeag1wIp/5EjMkqORmYD+1onYty+x2q9TjJ+FdNT0OkcsLIgMkEqtwaSNUTh0JQUKYzl+DPSBt6ut2GER1Shd1zkCuNbR45iDdNRkHl/uvYJrWQmwNDXCT+90rbFxFfwspKO21jliYUFkYFRqDT7cdA4HLifD1FiO70f5oFtLO7HDItJJbaxzBHCto/IwB+mo7jx2nruN0BMJAIAFr3VCW6cG1Xbu8vCzkI6aXudI9Olmiaj6FBQW9VTsvZgEUyM5Vo/0Rs/W9mKHRaQzrnNEVP1i7mRixvaiwdqT+rTEgPac6ICqF3ssiAxEvkqN9zZE4c8rKTA1lmPViM7o4172lJxE+mDq1KkIDAyEj48PunTpgsWLF5da58jJyQkhISEAitY5evbZZ9GyZUtkZGRg/vz5XOeI6KG0nAKMD4uEslCDPu72mNKvtdghkQFiYUFkAHILCjE+LBJHr96DmYkca0b6sKeC9B7XOSKqHoUPx93dychDs4YWWPy6F4y4sjbVABYWRHouI7cAY0PPICohAxamRvgx8Bn4ujUUOyyiasF1joiq7qs/ruD49fuwMDXCmlE+sDHX73ECJF0sLIj0WHJWPkb9eBqxydmwNjPGujHPcPYnIiLS2h19Bz8ciwMAfBvgidYO9USOiAwZCwsiPXU99QFGrzuNW2l5aFRPgbC3u8LdkQ0GEREVuXQ3E59svwAAeK+3GwZ14EQGVLNYWBDpoTPxaXhn/Vlk5Krg2tACP7/dFS62lVvMi4iIDE/6w8Ha+SoNerW2x7T+7mKHRHUACwsiPbPnwl1M3XIeBYUadHKpjx8CfWBnVXoefiIiqpsK1Rq8/8s53E7PQ1NbC3zHwdpUS1hYEOkJjUbAkkNXseTQVQCAXzsHLB7uBXNTI5EjIyIiKZm/PxbHrt2DuYkR1ozyho0FB2tT7WBhQaQHcpSFmLblPPZdSgIAjO3eHP83uC1/gSIiohJ+PX8Xq4/cAADMD+iINo7WIkdEdQkLCyKJi7+Xgwk/R+JKUjZMjGT40r8DXnvGReywiIhIYv5JzMLH284DACb0csOQjk1EjojqGhYWRBK2LyYR07deQLayEHZWCqwe2ZnTyRIRUSkZuQV4N+ws8lUa9Ghlh+l+HKxNtY+FBZEEKQvV+GZfLH58OPf4M80aYOkbneFoYyZyZEREJDVqjYD3fzmHW2l5cLE152BtEg0LCyKJuZaSjQ9+icblxCwAwLs9W2C6nztMjOQiR0ZERFI0f38sjl59OFh7pA8aWJqKHRLVUZL4S2X58uVo1qwZzMzM0LVrV5w+ffqJ+2/duhVt2rSBmZkZOnTogL1799ZSpEQ1R6MRsP5EPAZ/dwyXE7PQwMIEa0Z6Y+agtiwqiIioTL9fSMSqv64DAL4e1hFtG3OwNolH9L9WNm/ejKlTp2L27NmIioqCp6cn/Pz8kJKSUub+x48fxxtvvIG3334b586dg7+/P/z9/RETE1PLkRNVn/h7OXjj+5OYtfsSlIVF98fun9wT/ds5ih0aERFJ1JWkLHy0tWiw9rs9W+AlTw7WJnGJXlgsXLgQ77zzDsaMGQMPDw+sWrUKFhYWWLt2bZn7L1myBAMGDMD06dPRtm1bzJ07F507d8ayZctqOXKiqlNrgB+OxWPAkiM4FZcGcxMjzH7RAz+N6YJG1hxPQUREZcvMVWF8WCTyVGo819IOH3OwNkmAqGMsCgoKEBkZieDgYO02uVyOvn374sSJE2Uec+LECUydOrXENj8/P+zatavM/ZVKJZRKpfZ5VlbRfesqlQoqlUqneLdH3sLFFBnyo25BYWICI7kMxnIZjI1kMJLLYGokh7FcBhMj+cOHDCbGcpgayWFqLIfi4cNYLoNMJt6gquK8dc1fSgwhh6P/puCbC0ZIyvsXANCthS3mvuyBprYWUKsLoVaLHGAFGcJnYQg5AFXLQ99zJ6pL1BoBH2w6h5v3c+HcwBxL3/CCMW+ZJQkQtbC4d+8e1Go1HBwcSmx3cHDAlStXyjwmKSmpzP2TkpLK3D8kJARz5swptf3AgQOwsLDQKd45p42QpzbChuv/6HTc42QQYCKH9mEqB0yNHv6vXIDCCEUPOaAwBsyMBJgZAWZGgLkRYG4swNwIsDAGzI2LjqtMnRIeHl6lPKRAH3NIzQP23JIj+r4cgAyWxgJectWgq30KYk6mQF9v6tPHz+JxhpADULk8cnNzayASIqoJC8Nj8de/qTAzkWP1SG8O1ibJMPhZoYKDg0v0cGRlZcHFxQX9+/eHtbVuA5z2Zp5Dwt1k1G/QEAKAQo2AQo0AtUaASi2gUK2BSi1ApdagUFP0vwWFGhQ83F5MgAwFGqBAU9ZVdK8QTI3lqG9ugvrmJmhgaQJbC1PYWpqioaUpbK1MYWdpCvt6CthZmaJRPQWMoEF4eDj69esHExMTna8nBSqVSu9yuPdAiWWHb2Dzhdso1AiQy4DuDhp8M7In7Kx1K3KlRB8/i8cZQg5A1fIo7s0lImn742Iilh9+OFh7aEe0a2IjckRE/xG1sLCzs4ORkRGSk5NLbE9OToajY9mDVh0dHXXaX6FQQKFQlNpuYmKic8O77A0v7N27F4MGPaPzsRqNgAK1BkqVBspCNfJVGuQXqpGvUiOvQI1clRr5BWrkFKiRV1CInAI1cpSFeKAsRI6yENn5xQ8VsvMLkZmnQmaeCoUaAQWFGqRkK5GSrXx6IACszYxhITPC1tQLaFLfHI425mhiY4Ym9c3RpL45nOqbw9zUSKf8xFKZz7G2JWbm4fsjcfjldALyVEX3N/VqbY9pfVsi7txR2FlbSD6HitCHz+JpDCEHoHJ5GELeRIbu3+RsTHs4WHvcc83xcicnkSMiKknUwsLU1BTe3t44dOgQ/P39AQAajQaHDh3CpEmTyjzG19cXhw4dwuTJk7XbwsPD4evrWwsRV55cLoOZ3AhmJkYAqqcBFwQBuQVqpOcWICNXhfTcAqTl/Pe496AA9x4ocf+BEqkPlEjJUkJZqEFWfiGyIEPStfvlntvOyhRODSzg3MAcTW0t4NLAAk1tLeDa0AJN6ptz4Z0K+CcxC6F/x2PHudvaHqtOLvXxyYA28HVrCJVKhbhzIgdJRER6ITNPhXfXn0VugRrd3BpixsA2YodEVIrot0JNnToVgYGB8PHxQZcuXbB48WLk5ORgzJgxAIBRo0bByckJISEhAIAPP/wQvXr1woIFCzB48GBs2rQJZ8+exZo1a8RMQxQymQyWCmNYKozh3ODp+wuCgGxlIe7cf4DfDh6Fa9uOSH2gwt3MfCRl5uNuRh7upOchW1n4sCgpwPlbGaXOY2Ikg0uDoiKjmZ0lmj/yaGJjDnkdLjryVWqEX05G2MmbOB2Xpt3etbktJj3fEs+1tBN14D4REekftUbA5E3nEH8/F071zbHszc4crE2SJHphMXz4cKSmpmLWrFlISkpCp06dsG/fPu0A7YSEBMjl//3j6datGzZu3IhPP/0UM2fORKtWrbBr1y60b99erBT0hkwmg7WZCcwbWcG9voBBXk5l3v6QmafC7fRc3ErLe/i/ubiZlouEtFzcTstDgVqDG/dycONeDhCbWuJYU2M5mje0RAv7hw87K7g1skILe0tYmxnmrRZqjYCohHTsPHcHe87fRVZ+IQDASC7DgHaOGPtcM3i72oocJRER6avFB//F4dhUKIyLBmvbcrA2SZTohQUATJo0qdxbnyIiIkptCwgIQEBAQA1HVXfZmJvAxtymzAFhao2ApKx83LyXg7j7OYi/l4O4e7mIu/cACWm5KCjUIDY5G7HJ2aWOtbNSoIW9JdweFhzN7YqKDxdbC71bWTpHWYhTcfcRfjkZ4ZdTcO/Bf+NbGtuYYZi3M97q6gpHG65FQVQVy5cvx/z585GUlARPT08sXboUXbp0KXf/rVu34rPPPkN8fDxatWqFr7/+GoMGDarFiImq14HLyVj65zUAwFdDO6C9Ewdrk3RJorAg/WEkl8Hp4QDvbi3tSrxWqNbgTkYebqTm4Hrqg6JejdQHuJGag5RsJe49KHo8eotQ8TldGpijmZ0lmjW0hGvDotusmtpawrmB+cNxKeJKyylA9K10nEvIwMkb93EuIQOFmv9m+qpnZox+Hg4Y1tkZz7ZoWKdvByOqLps3b8bUqVOxatUqdO3aFYsXL4afnx9iY2PRqFGjUvsfP34cb7zxBkJCQjBkyBBs3LgR/v7+iIqKYq826aU7OcDy7UWTkI/t3hyveDmLHBHRk7GwoGpjbCSHa0NLuDa0RJ82JRv97HwV4u7l4Ebqw2Lj4f+Pu5eDPJUa8fdzEX8/F0BqqfM2qqeAU4OiYqZJfXM4WpvBztIYN7KAm/dz4djAEpamRlUeu6BSa5CUmY9b6bm4nZ6H66kPcDX5Af5Nzsbt9LxS+ze1tUDP1nbwa+eIrs0bwtRYv3pdiKRu4cKFeOedd7Rj7latWoXff/8da9euxYwZM0rtv2TJEgwYMADTp08HAMydOxfh4eFYtmwZVq1aVauxE1WFslCN5X9ex/KLRlALajzbwhYzB3GwNkkfCwuqFfXMTNDRuT46OtcvsV0QBCRnKXHj3gPcvJ+L+Ie3VyWk5SHhfg5yCtTaqXTPJWQ8dlZjLLl0DEDR2A6bh2t51DMzhoWpMSxMjaAwMYKxXKadxUqjEaAWBOSr1Mh9OKVvRp4K9x8UIDPvySsPu9lbopNLA/g0a4DubnZo2lB/154gkrqCggJERkYiODhYu00ul6Nv3744ceJEmcecOHGixLpFAODn54ddu3aVex2lUgml8r9bGYvX81CpVDqtRn7s2n3suXAXd+7IcWTHxRJjA/WJRqNhDhIQeTMdN+7lApDhOTdbLAjoCEGjhkqjFjs0nRT/G9Ll35IUGUIeVclBl2NYWJCoZDIZHG3M4Ghjhm5uJV8TBAFpOQW483C2qjsZebibkY/krHwkZubhZnI6cjVGyFMVLUSYmq1EagXX8iiPqbEczvXN4dTAHM0aWqK1gxVaOdRDW0dr2FgY5uBzIim6d+8e1Gq1diKPYg4ODrhy5UqZxyQlJZW5f1JSUrnXCQkJwZw5c0ptP3DgACwsKv7jQUSiDDvjjQDIgZTECh8nTcxBCuqZCHi1mQZeDVNw8q+DYodTJeHh4WKHUC0MIY/K5JCbm1vhfVlYkGTJZDI0tFKgoZWiVE+HSqV6uFihHwo0MqTnFvU4ZOaqkK0sRF6BGjkFhSgo1GhXRgcAIzkgl8lgZmIES4URLEyNYW1mAvt6pmhoqYCNuQnHRxDVIcHBwSV6ObKysuDi4oL+/fvD2tq6wudxvp0J16upuHbtKlq2bAUjPf2lXK3RMAcJsFQYY6CHHU4fi0C/fv30dgFLlUqF8PBwvc4BMIw8qpJDcU9uRbCwIL2ny1oeRKQf7OzsYGRkhOTk5BLbk5OT4ejoWOYxjo6OOu0PAAqFAgqFotR2XVcv925uh47ONtib9y8G9Wmp1398MAdpKL79RNf/FqXIEHIADCOPyuSgy/76WcoTEZFBMzU1hbe3Nw4dOqTdptFocOjQIfj6+pZ5jK+vb4n9gaJu//L2JyKi6sUeCyIikqSpU6ciMDAQPj4+6NKlCxYvXoycnBztLFGjRo2Ck5MTQkJCAAAffvghevXqhQULFmDw4MHYtGkTzp49izVr1oiZBhFRncHCgoiIJGn48OFITU3FrFmzkJSUhE6dOmHfvn3aAdoJCQklZv3p1q0bNm7ciE8//RQzZ85Eq1atsGvXLq5hQURUS1hYEBGRZE2aNAmTJk0q87WIiIhS2wICAhAQEFDDURERUVk4xoKIiIiIiKqMhQUREREREVVZnbsVShCK1jPQZU7eYiqVCrm5ucjKytLr6cYMIQ/mIB2GkIch5ABULY/i78Ti78i6qq63EcxBOgwhD0PIATCMPGqrfahzhUV2djYAwMXFReRIiIikJzs7GzY2NmKHIRq2EUREZatI+yAT6tjPUxqNBnfv3kW9evUgk+m2wnLxiqy3bt3SaUVWqTGEPJiDdBhCHoaQA1C1PARBQHZ2Npo0aVJipqW6pq63EcxBOgwhD0PIATCMPGqrfahzPRZyuRzOzs5VOoe1tbXe/of1KEPIgzlIhyHkYQg5AJXPoy73VBRjG1GEOUiHIeRhCDkAhpFHTbcPdfdnKSIiIiIiqjYsLIiIiIiIqMpYWOhAoVBg9uzZUCgUYodSJYaQB3OQDkPIwxByAAwnD31lCO8/c5AOQ8jDEHIADCOP2sqhzg3eJiIiIiKi6sceCyIiIiIiqjIWFkREREREVGUsLIiIiIiIqMpYWFTSSy+9hKZNm8LMzAyNGzfGyJEjcffuXbHD0kl8fDzefvttNG/eHObm5nBzc8Ps2bNRUFAgdmg6+fLLL9GtWzdYWFigfv36YodTYcuXL0ezZs1gZmaGrl274vTp02KHpJMjR47gxRdfRJMmTSCTybBr1y6xQ9JZSEgInnnmGdSrVw+NGjWCv78/YmNjxQ5LJytXrkTHjh21c5P7+vrijz/+EDusOk/f2whDaR8A/Wwj2D6IzxDaB6D22wgWFpXUp08fbNmyBbGxsdi+fTuuX7+OYcOGiR2WTq5cuQKNRoPVq1fj0qVLWLRoEVatWoWZM2eKHZpOCgoKEBAQgIkTJ4odSoVt3rwZU6dOxezZsxEVFQVPT0/4+fkhJSVF7NAqLCcnB56enli+fLnYoVTaX3/9haCgIJw8eRLh4eFQqVTo378/cnJyxA6twpydnfHVV18hMjISZ8+exfPPP4+XX34Zly5dEju0Ok3f2whDaR8A/Wsj2D5IgyG0D4AIbYRA1WL37t2CTCYTCgoKxA6lSr755huhefPmYodRKevWrRNsbGzEDqNCunTpIgQFBWmfq9VqoUmTJkJISIiIUVUeAGHnzp1ih1FlKSkpAgDhr7/+EjuUKmnQoIHwww8/iB0GPcIQ2gh9bh8EQX/aCLYP0mQo7YMg1GwbwR6LapCWloYNGzagW7duMDExETucKsnMzIStra3YYRi0goICREZGom/fvtptcrkcffv2xYkTJ0SMjDIzMwFAb/8NqNVqbNq0CTk5OfD19RU7HHrIUNoItg81j+2DdOl7+wDUThvBwqIKPvnkE1haWqJhw4ZISEjA7t27xQ6pSq5du4alS5di/PjxYodi0O7duwe1Wg0HB4cS2x0cHJCUlCRSVKTRaDB58mR0794d7du3FzscnVy8eBFWVlZQKBSYMGECdu7cCQ8PD7HDqvMMqY1g+1A72D5Ikz63D0DtthEsLB4xY8YMyGSyJz6uXLmi3X/69Ok4d+4cDhw4ACMjI4waNQqCBNYb1DUPALhz5w4GDBiAgIAAvPPOOyJF/p/K5EBUFUFBQYiJicGmTZvEDkVn7u7uiI6OxqlTpzBx4kQEBgbi8uXLYodlcAyhjTCE9gFgG0G1S5/bB6B22wiuvP2I1NRU3L9//4n7tGjRAqampqW23759Gy4uLjh+/LjotyDomsfdu3fRu3dvPPvsswgNDYVcLn69WZnPIjQ0FJMnT0ZGRkYNR1c1BQUFsLCwwLZt2+Dv76/dHhgYiIyMDL38VVMmk2Hnzp0l8tEnkyZNwu7du3HkyBE0b95c7HCqrG/fvnBzc8Pq1avFDsWgGEIbYQjtA2C4bQTbB+kxtPYBqNk2wrjaz6jH7O3tYW9vX6ljNRoNAECpVFZnSJWiSx537txBnz594O3tjXXr1kmm0ajKZyF1pqam8Pb2xqFDh7RftBqNBocOHcKkSZPEDa6OEQQB77//Pnbu3ImIiAiDaTQ0Go0kvosMjSG0EYbQPgCG20awfZAOQ20fgJptI1hYVMKpU6dw5swZPPfcc2jQoAGuX7+Ozz77DG5ubqL3Vujizp076N27N1xdXfHtt98iNTVV+5qjo6OIkekmISEBaWlpSEhIgFqtRnR0NACgZcuWsLKyEje4ckydOhWBgYHw8fFBly5dsHjxYuTk5GDMmDFih1ZhDx48wLVr17TP4+LiEB0dDVtbWzRt2lTEyCouKCgIGzduxO7du1GvXj3tPcw2NjYwNzcXObqKCQ4OxsCBA9G0aVNkZ2dj48aNiIiIwP79+8UOrc4yhDbCUNoHQP/aCLYP0mAI7QMgQhtRI3NNGbgLFy4Iffr0EWxtbQWFQiE0a9ZMmDBhgnD79m2xQ9PJunXrBABlPvRJYGBgmTkcPnxY7NCeaOnSpULTpk0FU1NToUuXLsLJkyfFDkknhw8fLvN9DwwMFDu0Civvv/9169aJHVqFjR07VnB1dRVMTU0Fe3t74YUXXhAOHDggdlh1miG0EYbSPgiCfrYRbB/EZwjtgyDUfhvBMRZERERERFRl0rlhkoiIiIiI9BYLCyIiIiIiqjIWFkREREREVGUsLIiIiIiIqMpYWBARERERUZWxsCAiIiIioipjYUFERERERFXGwoKIiIiIiKqMhQUREREREVUZCwsiIiIiIqoyFhZERERERFRlLCyIallqaiocHR0xb9487bbjx4/D1NQUhw4dEjEyIiISE9sH0ncyQRAEsYMgqmv27t0Lf39/HD9+HO7u7ujUqRNefvllLFy4UOzQiIhIRGwfSJ+xsCASSVBQEA4ePAgfHx9cvHgRZ86cgUKhEDssIiISGdsH0lcsLIhEkpeXh/bt2+PWrVuIjIxEhw4dxA6JiIgkgO0D6SuOsSASyfXr13H37l1oNBrEx8eLHQ4REUkE2wfSV+yxIBJBQUEBunTpgk6dOsHd3R2LFy/GxYsX0ahRI7FDIyIiEbF9IH3GwoJIBNOnT8e2bdtw/vx5WFlZoVevXrCxscGePXvEDo2IiETE9oH0GW+FIqplERERWLx4McLCwmBtbQ25XI6wsDAcPXoUK1euFDs8IiISCdsH0nfssSAiIiIioipjjwUREREREVUZCwsiIiIiIqoyFhZERERERFRlLCyIiIiIiKjKWFgQEREREVGVsbAgIiIiIqIqY2FBRERERERVxsKCiIiIiIiqjIUFERERERFVGQsLIiIiIiKqMhYWRERERERUZSwsiIiIiIioyv4f8bkWo5d7IwsAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"code","source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}"],"metadata":{"id":"Zr7HPyEvUpAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n","            GELU(), ## Activation\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)"],"metadata":{"id":"g32EPKmsUrID"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(GPT_CONFIG_124M[\"emb_dim\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6UzmHOEdUtTs","executionInfo":{"status":"ok","timestamp":1743736464794,"user_tz":-330,"elapsed":47,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"d69e96bf-83d7-4ef5-9ffd-fa7fedc88eb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["768\n"]}]},{"cell_type":"code","source":["ffn = FeedForward(GPT_CONFIG_124M)\n","x = torch.rand(2, 3, 768) #A\n","out = ffn(x)\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fsRTu9FcUv6t","executionInfo":{"status":"ok","timestamp":1743736465941,"user_tz":-330,"elapsed":69,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"02bd9275-d258-4a0c-8770-a42f4735e1cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 768])\n"]}]},{"cell_type":"markdown","source":["## Shortcut Connections"],"metadata":{"id":"0NH9w1DRU2Je"}},{"cell_type":"code","source":["class ExampleDeepNeuralNetwork(nn.Module):\n","    def __init__(self, layer_sizes, use_shortcut):\n","        super().__init__()\n","        self.use_shortcut = use_shortcut\n","        self.layers = nn.ModuleList([\n","            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n","        ])\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            # Compute the output of the current layer\n","            layer_output = layer(x)\n","            # Check if shortcut can be applied\n","            if self.use_shortcut and x.shape == layer_output.shape:\n","                x = x + layer_output\n","            else:\n","                x = layer_output\n","        return x\n"],"metadata":{"id":"BF3EsycUU3_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layer_sizes = [3, 3, 3, 3, 3, 1]\n","sample_input = torch.tensor([[1., 0., -1.]])\n","torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n","model_without_shortcut = ExampleDeepNeuralNetwork(\n","layer_sizes, use_shortcut=False\n",")"],"metadata":{"id":"gE0M9THMU6t-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_gradients(model, x):\n","    # Forward pass\n","    output = model(x)\n","    target = torch.tensor([[0.]])\n","\n","    # Calculate loss based on how close the target\n","    # and output are\n","    loss = nn.MSELoss()\n","    loss = loss(output, target)\n","\n","    # Backward pass to calculate the gradients\n","    loss.backward()\n","\n","    for name, param in model.named_parameters():\n","        if 'weight' in name:\n","            # Print the mean absolute gradient of the weights\n","            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"],"metadata":{"id":"3DkRfg5eU8ve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_gradients(model_without_shortcut, sample_input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0MQHTFmiU_D2","executionInfo":{"status":"ok","timestamp":1743736473980,"user_tz":-330,"elapsed":99,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"ed9c9cc2-2359-4688-ed32-5ba391817966"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["layers.0.0.weight has gradient mean of 0.00020173584925942123\n","layers.1.0.weight has gradient mean of 0.00012011159560643137\n","layers.2.0.weight has gradient mean of 0.0007152040489017963\n","layers.3.0.weight has gradient mean of 0.0013988736318424344\n","layers.4.0.weight has gradient mean of 0.005049645435065031\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model_with_shortcut = ExampleDeepNeuralNetwork(\n","layer_sizes, use_shortcut=True\n",")\n","print_gradients(model_with_shortcut, sample_input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HwjxJqohVCiw","executionInfo":{"status":"ok","timestamp":1743736475421,"user_tz":-330,"elapsed":27,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"dc458662-0ee2-4882-89e1-b7ae357ece1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["layers.0.0.weight has gradient mean of 0.22169791162014008\n","layers.1.0.weight has gradient mean of 0.20694105327129364\n","layers.2.0.weight has gradient mean of 0.32896995544433594\n","layers.3.0.weight has gradient mean of 0.2665732204914093\n","layers.4.0.weight has gradient mean of 1.3258540630340576\n"]}]},{"cell_type":"markdown","source":["## Attention and Linear Layers in a Transformer"],"metadata":{"id":"6xy4hUGFVIwX"}},{"cell_type":"code","source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}"],"metadata":{"id":"RvwPelvpVPmC","executionInfo":{"status":"ok","timestamp":1743736782716,"user_tz":-330,"elapsed":3,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n","\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n","            GELU(), ## Activation\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)"],"metadata":{"id":"fm6DC0PQVTmh","executionInfo":{"status":"ok","timestamp":1743736784067,"user_tz":-330,"elapsed":6,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"4UrpXSVpVp9c","executionInfo":{"status":"ok","timestamp":1743736786251,"user_tz":-330,"elapsed":22,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        # 2*4*768\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x\n","        # 2*4*768"],"metadata":{"id":"QyxRAH9VVWHU","executionInfo":{"status":"ok","timestamp":1743736789665,"user_tz":-330,"elapsed":15,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","x = torch.rand(2, 4, 768) #A\n","block = TransformerBlock(GPT_CONFIG_124M)\n","output = block(x)\n","print(\"Input shape:\", x.shape)\n","print(\"Output shape:\", output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOTJayMaVYj5","executionInfo":{"status":"ok","timestamp":1743736487997,"user_tz":-330,"elapsed":67,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"97b48383-36b2-4187-f7fd-5a2d52919aa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([2, 4, 768])\n","Output shape: torch.Size([2, 4, 768])\n"]}]},{"cell_type":"markdown","source":["## Entire Architecture"],"metadata":{"id":"Vqr5_Mk5Vwhv"}},{"cell_type":"code","source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","class DummyGPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        # Use a placeholder for TransformerBlock\n","        self.trf_blocks = nn.Sequential(\n","            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        # Use a placeholder for LayerNorm\n","        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","\n","class DummyTransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        # A simple placeholder\n","\n","    def forward(self, x):\n","        # This block does nothing and just returns its input.\n","        return x\n","\n","\n","class DummyLayerNorm(nn.Module):\n","    def __init__(self, normalized_shape, eps=1e-5):\n","        super().__init__()\n","        # The parameters here are just to mimic the LayerNorm interface.\n","\n","    def forward(self, x):\n","        # This layer does nothing and just returns its input.\n","        return x\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n","\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n","            GELU(), ## Activation\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        # 2*4*768\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x\n","        # 2*4*768\n","\n","class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits"],"metadata":{"id":"2UpY4V-KVyUm","executionInfo":{"status":"ok","timestamp":1743736793650,"user_tz":-330,"elapsed":61,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","out = model(batch)\n","print(\"Input batch:\\n\", batch)\n","print(\"\\nOutput shape:\", out.shape)\n","print(out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bXrabsoWVfE","executionInfo":{"status":"ok","timestamp":1743736504355,"user_tz":-330,"elapsed":1498,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"ec368687-ff91-41f5-a00c-12f82c494810"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input batch:\n"," tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n","\n","Output shape: torch.Size([2, 4, 50257])\n","tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n","         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n","         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n","         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n","\n","        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n","         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n","         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n","         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n","       grad_fn=<UnsafeViewBackward0>)\n"]}]},{"cell_type":"code","source":["total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Total number of parameters: {total_params:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJFTIXQdWYZq","executionInfo":{"status":"ok","timestamp":1743736506734,"user_tz":-330,"elapsed":9,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"726360b7-057e-4843-ff1a-68bbfac2a0c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of parameters: 163,009,536\n"]}]},{"cell_type":"code","source":["print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n","print(\"Output layer shape:\", model.out_head.weight.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zr_R4fxmWbtc","executionInfo":{"status":"ok","timestamp":1743736507739,"user_tz":-330,"elapsed":9,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"06788c45-2e99-45e0-fd55-68290e3e25a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Token embedding layer shape: torch.Size([50257, 768])\n","Output layer shape: torch.Size([50257, 768])\n"]}]},{"cell_type":"code","source":["total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n","print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gkq-5nGvWfX8","executionInfo":{"status":"ok","timestamp":1743736508681,"user_tz":-330,"elapsed":6,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"c8b156b0-5b5c-4665-a4e1-0e3356ec76df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of trainable parameters considering weight tying: 124,412,160\n"]}]},{"cell_type":"code","source":["total_size_bytes = total_params * 4 #A\n","total_size_mb = total_size_bytes / (1024 * 1024) #B\n","print(f\"Total size of the model: {total_size_mb:.2f} MB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tXPR7BdFWh9E","executionInfo":{"status":"ok","timestamp":1743736510004,"user_tz":-330,"elapsed":8,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"77207891-2097-43ff-8e61-ddb089d67155"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total size of the model: 621.83 MB\n"]}]},{"cell_type":"markdown","source":["## Generating Text from Output Tokens"],"metadata":{"id":"oVxj_rIdWm_t"}},{"cell_type":"code","source":["def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","\n","    ###Input batch:\n"," ###tensor([[6109, 3626, 6100,  345],\n","        ##[6109, 1110, 6622,  257]])\n","\n","    for _ in range(max_new_tokens):\n","\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n","\n","        # Focus only on the last time step\n","        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Apply softmax to get probabilities\n","        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n","\n","        # Get the idx of the vocab entry with the highest probability value\n","        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx"],"metadata":{"id":"czeFIGI4WqDP","executionInfo":{"status":"ok","timestamp":1743736809018,"user_tz":-330,"elapsed":28,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["start_context = \"Hello, I am\"\n","encoded = tokenizer.encode(start_context)\n","print(\"encoded:\", encoded)\n","encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n","print(\"encoded_tensor.shape:\", encoded_tensor.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slTz6GOXWule","executionInfo":{"status":"ok","timestamp":1743695574818,"user_tz":-330,"elapsed":17,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"d592296c-bfae-4727-a235-ea19b2714fce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["encoded: [15496, 11, 314, 716]\n","encoded_tensor.shape: torch.Size([1, 4])\n"]}]},{"cell_type":"code","source":["model.eval() #A\n","#model = GPTModel(GPT_CONFIG_124M)\n","out = generate_text_simple(\n","model=model,\n","idx=encoded_tensor,\n","max_new_tokens=6,\n","context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","print(\"Output:\", out)\n","print(\"Output length:\", len(out[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"env0NYR2WxEi","executionInfo":{"status":"ok","timestamp":1743695584255,"user_tz":-330,"elapsed":715,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"0453f384-21ae-467b-9452-e3e7aed2a1c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n","Output length: 10\n"]}]},{"cell_type":"code","source":["decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n","print(decoded_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"waOZDztdW0J_","executionInfo":{"status":"ok","timestamp":1743695596285,"user_tz":-330,"elapsed":16,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"7aaac67c-e07d-4c3f-93d7-efab9bcd7a5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, I am Featureiman Byeswickattribute argue\n"]}]},{"cell_type":"markdown","source":["## Evaluating Generative Text Model"],"metadata":{"id":"k7idCxaSW4Fv"}},{"cell_type":"code","source":["class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits"],"metadata":{"id":"sbwrSl95W76J","executionInfo":{"status":"ok","timestamp":1743736818595,"user_tz":-330,"elapsed":4,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,   # Vocabulary size\n","    \"context_length\": 256, # Shortened context length (orig: 1024)\n","    \"emb_dim\": 768,        # Embedding dimension\n","    \"n_heads\": 12,         # Number of attention heads\n","    \"n_layers\": 12,        # Number of layers\n","    \"drop_rate\": 0.1,      # Dropout rate\n","    \"qkv_bias\": False      # Query-key-value bias\n","}\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval();  # Disable dropout during inference"],"metadata":{"id":"DsUyT7sdXBSK","executionInfo":{"status":"ok","timestamp":1743736825161,"user_tz":-330,"elapsed":1302,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","start_context = \"Every effort moves you\"\n","\n","\n","\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(start_context, tokenizer),\n","    max_new_tokens=10,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0MH2MCRNXD1R","executionInfo":{"status":"ok","timestamp":1743736874948,"user_tz":-330,"elapsed":1464,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"5b593f9b-cf67-4205-ef53-6663148f275b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"]}]},{"cell_type":"markdown","source":["### Calculating Loss : Cross Entropy and Perplexity"],"metadata":{"id":"ubO0hHYHXHqJ"}},{"cell_type":"code","source":["inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n","                       [40,    1107, 588]])   #  \"I really like\"]\n","\n","targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n","                        [1107,  588, 11311]]) #  \" really like chocolate\"]"],"metadata":{"id":"XEVud7GyXLcw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    logits = model(inputs)\n","\n","probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n","print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"riqbJdLxXOw7","executionInfo":{"status":"ok","timestamp":1743695705279,"user_tz":-330,"elapsed":100,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"227ed18d-6ca9-4946-855d-b2ea181cbcb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 50257])\n"]}]},{"cell_type":"code","source":["token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n","print(\"Token IDs:\\n\", token_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V1CqCw9jXRC1","executionInfo":{"status":"ok","timestamp":1743695714257,"user_tz":-330,"elapsed":27,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"97d49648-aa0b-4b6f-a7a0-e4488b2f542c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Token IDs:\n"," tensor([[[16657],\n","         [  339],\n","         [42826]],\n","\n","        [[49906],\n","         [29669],\n","         [41751]]])\n"]}]},{"cell_type":"code","source":["print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n","print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1IEGEhtKXTZF","executionInfo":{"status":"ok","timestamp":1743695724253,"user_tz":-330,"elapsed":16,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"b52c7de0-a8e8-4024-ab7b-534a1fcb198e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Targets batch 1:  effort moves you\n","Outputs batch 1:  Armed heNetflix\n"]}]},{"cell_type":"code","source":["text_idx = 0\n","target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n","print(\"Text 1:\", target_probas_1)\n","\n","text_idx = 1\n","target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n","print(\"Text 2:\", target_probas_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jvwWxoa7XX8T","executionInfo":{"status":"ok","timestamp":1743695742725,"user_tz":-330,"elapsed":15,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"8fe2f321-ac27-47cf-ef34-54f21550abf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Text 1: tensor([    0.0001,     0.0000,     0.0000])\n","Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"]}]},{"cell_type":"code","source":["# Compute logarithm of all token probabilities\n","log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n","print(log_probas)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gQZ1vSQXaK-","executionInfo":{"status":"ok","timestamp":1743695751995,"user_tz":-330,"elapsed":33,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"ab507f15-5062-4381-c176-018678969386"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"]}]},{"cell_type":"code","source":["# Calculate the average probability for each token\n","avg_log_probas = torch.mean(log_probas)\n","print(avg_log_probas)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTaEWaCYXcj0","executionInfo":{"status":"ok","timestamp":1743695761957,"user_tz":-330,"elapsed":63,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"84c07c54-20ea-4c06-a879-fef04f817e3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(-10.7940)\n"]}]},{"cell_type":"code","source":["neg_avg_log_probas = avg_log_probas * -1\n","print(neg_avg_log_probas)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QeKZsQxiXf-d","executionInfo":{"status":"ok","timestamp":1743695775593,"user_tz":-330,"elapsed":38,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"7c8dbcde-b404-47b1-cd07-39f05563a07f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10.7940)\n"]}]},{"cell_type":"code","source":["# Logits have shape (batch_size, num_tokens, vocab_size)\n","print(\"Logits shape:\", logits.shape)\n","\n","# Targets have shape (batch_size, num_tokens)\n","print(\"Targets shape:\", targets.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGLmC3HnXiJp","executionInfo":{"status":"ok","timestamp":1743695784875,"user_tz":-330,"elapsed":37,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"cd8e83ae-3beb-4686-c5c3-b37f7f207394"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logits shape: torch.Size([2, 3, 50257])\n","Targets shape: torch.Size([2, 3])\n"]}]},{"cell_type":"code","source":["logits_flat = logits.flatten(0, 1)\n","targets_flat = targets.flatten()\n","\n","print(\"Flattened logits:\", logits_flat.shape)\n","print(\"Flattened targets:\", targets_flat.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6VH7hfuNXkOH","executionInfo":{"status":"ok","timestamp":1743695792957,"user_tz":-330,"elapsed":40,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"181cfd84-5c56-4a3b-fa58-1a18d956d3c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Flattened logits: torch.Size([6, 50257])\n","Flattened targets: torch.Size([6])\n"]}]},{"cell_type":"code","source":["loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IqKlljxlXmEY","executionInfo":{"status":"ok","timestamp":1743695800595,"user_tz":-330,"elapsed":36,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"dfe1fde5-f6ce-4df3-f66b-3764fc33899f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10.7940)\n"]}]},{"cell_type":"code","source":["perplexity = torch.exp(loss)\n","print(perplexity)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AmELm5YEXojn","executionInfo":{"status":"ok","timestamp":1743695810958,"user_tz":-330,"elapsed":41,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"3205ee1c-b958-43bb-c68c-23e3d8fef9ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(48725.8203)\n"]}]},{"cell_type":"markdown","source":["### Calculating training and validation set losses"],"metadata":{"id":"e77KhLf3Xqyh"}},{"cell_type":"code","source":["import os\n","import tiktoken\n","\n","file_path = \"filtered_articles.txt\"\n","\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    text_data = file.read()\n","\n","# First 100 characters\n","print(text_data[:99])\n","\n","# Last 100 characters\n","print(text_data[-99:])\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","total_characters = len(text_data)\n","total_tokens = len(tokenizer.encode(text_data))\n","\n","print(\"Characters:\", total_characters)\n","print(\"Tokens:\", total_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OECYZfbvXucv","executionInfo":{"status":"ok","timestamp":1743736900980,"user_tz":-330,"elapsed":13917,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"9573ba98-749d-441a-aacb-da055ba5ea11"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Autism is a neurodevelopmental disorder characterized by difficulties with social interaction and c\n","a\n","Time and motion study\n","University of California, Berkeley alumni\n","Writers from Oakland, California\n","\n","Characters: 121892397\n","Tokens: 26316250\n"]}]},{"cell_type":"markdown","source":["### Implementing Data Loader"],"metadata":{"id":"Fu4jSqVPYZ69"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","\n","class GPTDatasetV1(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Tokenize the entire text\n","        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","\n","        # Use a sliding window to chunk the book into overlapping sequences of max_length\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n","\n","def create_dataloader_v1(txt, batch_size=16, max_length=1024,\n","                         stride=512, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader"],"metadata":{"id":"Hd0Dnyq4YcJJ","executionInfo":{"status":"ok","timestamp":1743736910652,"user_tz":-330,"elapsed":7,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","\n","# Load the text file\n","with open(\"filtered_articles.txt\", \"r\", encoding=\"utf-8\") as file:\n","    text = file.read()\n","\n","# Initialize tiktoken with a specific encoding (e.g., \"gpt-3.5-turbo\")\n","encoding = tiktoken.get_encoding(\"gpt2\")\n","\n","# Encode the text\n","tokens = encoding.encode(text)\n","\n","# Calculate vocabulary size\n","vocab_size = len(set(tokens))\n","\n","print(\"Vocabulary Size:\", vocab_size)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHbEWDAQY5Xw","executionInfo":{"status":"ok","timestamp":1743736930067,"user_tz":-330,"elapsed":14792,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"d9444aaa-36c2-4b7d-88f3-a72dfc91d21e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary Size: 48394\n"]}]},{"cell_type":"code","source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,   # Vocabulary size\n","    \"context_length\": 1024, # Shortened context length (orig: 1024)\n","    \"emb_dim\": 768,        # Embedding dimension\n","    \"n_heads\": 12,         # Number of attention heads\n","    \"n_layers\": 12,        # Number of layers\n","    \"drop_rate\": 0.1,      # Dropout rate\n","    \"qkv_bias\": False      # Query-key-value bias\n","}\n"],"metadata":{"id":"j2fsx2sqYmXG","executionInfo":{"status":"ok","timestamp":1743736934367,"user_tz":-330,"elapsed":3,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Train/validation ratio\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","train_data = text_data[:split_idx]\n","val_data = text_data[split_idx:]\n","\n","\n","torch.manual_seed(123)\n","\n","train_loader = create_dataloader_v1(\n","    train_data,\n","    batch_size=16,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    val_data,\n","    batch_size=16,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")"],"metadata":{"id":"Ln-oqFprZK9O","executionInfo":{"status":"ok","timestamp":1743737418235,"user_tz":-330,"elapsed":24537,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Sanity check\n","\n","if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for the training loader. \"\n","          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n","          \"increase the `training_ratio`\")\n","\n","if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for the validation loader. \"\n","          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n","          \"decrease the `training_ratio`\")"],"metadata":{"id":"lnBPvjaxZotZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Train loader:\")\n","for x, y in train_loader:\n","    print(x.shape, y.shape)\n","\n","print(\"\\nValidation loader:\")\n","for x, y in val_loader:\n","    print(x.shape, y.shape)\n","\n","print(len(train_loader))\n","print(len(val_loader))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R8WqV7uHagMJ","executionInfo":{"status":"ok","timestamp":1743696564152,"user_tz":-330,"elapsed":491,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"bb5b00e7-6d83-4be2-eae6-3b4427896ef8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader:\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","\n","Validation loader:\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([16, 1024]) torch.Size([16, 1024])\n","torch.Size([9, 1024]) torch.Size([9, 1024])\n","1442\n","164\n"]}]},{"cell_type":"code","source":["train_tokens = 0\n","for input_batch, target_batch in train_loader:\n","    train_tokens += input_batch.numel()\n","\n","val_tokens = 0\n","for input_batch, target_batch in val_loader:\n","    val_tokens += input_batch.numel()\n","\n","print(\"Training tokens:\", train_tokens)\n","print(\"Validation tokens:\", val_tokens)\n","print(\"All tokens:\", train_tokens + val_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMCcBGXkalH5","executionInfo":{"status":"ok","timestamp":1743696584118,"user_tz":-330,"elapsed":220,"user":{"displayName":"ARKA PRATIM GHOSH","userId":"11326683730826626337"}},"outputId":"1f44fe8e-c0da-46d6-cbba-d542e07170ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training tokens: 23625728\n","Validation tokens: 2679808\n","All tokens: 26305536\n"]}]},{"cell_type":"markdown","source":["### GPT Model Class we need"],"metadata":{"id":"G_1pCrl6arhC"}},{"cell_type":"code","source":["class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval();  # Disable dropout during inference"],"metadata":{"id":"4xty-u0tatbT","executionInfo":{"status":"ok","timestamp":1743736968140,"user_tz":-330,"elapsed":1292,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["### Functions for calculating loss"],"metadata":{"id":"Q86RlgFUbu8I"}},{"cell_type":"code","source":["def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # Reduce the number of batches to match the total number of batches in the data loader\n","        # if num_batches exceeds the number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","        else:\n","            break\n","    return total_loss / num_batches"],"metadata":{"id":"_NAl3j2Hbzni","executionInfo":{"status":"ok","timestamp":1743736970817,"user_tz":-330,"elapsed":20,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["Setting device"],"metadata":{"id":"gAhqKSpM0t_y"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1IAWo0y0s1b","executionInfo":{"status":"ok","timestamp":1743736997292,"user_tz":-330,"elapsed":490,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"71108376-a146-4007-929c-5c99bbee839f"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPTModel(\n","  (tok_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(1024, 768)\n","  (drop_emb): Dropout(p=0.1, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (1): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (2): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (3): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (4): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (5): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (6): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (7): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (8): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (9): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (10): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (11): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n","\n","with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n","    train_loss = calc_loss_loader(train_loader, model, device)\n","    val_loss = calc_loss_loader(val_loader, model, device)\n","\n","print(\"Training loss:\", train_loss)\n","print(\"Validation loss:\", val_loss)"],"metadata":{"id":"0FIhcVMTza-P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training LOOP"],"metadata":{"id":"elPpT7mzcmxL"}},{"cell_type":"code","source":["def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Epoch {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","        # Print a sample text after each epoch\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context\n","        )\n","\n","    return train_losses, val_losses, track_tokens_seen\n","\n","def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","    model.eval()\n","    with torch.no_grad():\n","        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n","        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n","    model.train()\n","    return train_loss, val_loss\n","\n","def generate_and_print_sample(model, tokenizer, device, start_context):\n","    model.eval()\n","    context_size = model.pos_emb.weight.shape[0]\n","    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n","    with torch.no_grad():\n","        token_ids = generate_text_simple(\n","            model=model, idx=encoded,\n","            max_new_tokens=50, context_size=context_size\n","        )\n","    decoded_text = token_ids_to_text(token_ids, tokenizer)\n","    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n","    model.train()\n","\n","def save_checkpoint(epoch, model, optimizer, save_path=\"model_checkpoint1.pth\"):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, save_path)\n","    print(f\"Checkpoint saved at epoch {epoch + 1}\")"],"metadata":{"id":"6TQPVEQHqaTb","executionInfo":{"status":"ok","timestamp":1743737324054,"user_tz":-330,"elapsed":25,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### Training the model"],"metadata":{"id":"CxP_6BpQq88S"}},{"cell_type":"markdown","source":["RUN THE BELOW CODE WHEN YOU DELETE THE RUNTIME AND START IT NEXT DAY. UPLOAD filtered_articles.txt ."],"metadata":{"id":"9N7RBpbT5Un7"}},{"cell_type":"code","source":["!pip install tiktoken\n","\n","# Import Libraries\n","import tiktoken\n","import torch\n","import torch.nn as nn\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n","\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n","            GELU(), ## Activation\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        # 2*4*768\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x\n","        # 2*4*768\n","\n","class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","def generate_text_simple(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:]\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","        logits = logits[:, -1, :]\n","\n","        # New: Filter logits with top_k sampling\n","        if top_k is not None:\n","            # Keep only top_k values\n","            top_logits, _ = torch.topk(logits, top_k)\n","            min_val = top_logits[:, -1]\n","            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n","\n","        # New: Apply temperature scaling\n","        if temperature > 0.0:\n","            logits = logits / temperature\n","\n","            # Apply softmax to get probabilities\n","            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n","\n","            # Sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n","\n","        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n","        else:\n","            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n","\n","        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n","            break\n","\n","        # Same as before: append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n","\n","    return idx\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","file_path = \"filtered_articles.txt\"\n","\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    text_data = file.read()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","total_characters = len(text_data)\n","total_tokens = len(tokenizer.encode(text_data))\n","\n","print(\"Characters:\", total_characters)\n","print(\"Tokens:\", total_tokens)\n","\n","class GPTDatasetV1(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Tokenize the entire text\n","        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","\n","        # Use a sliding window to chunk the book into overlapping sequences of max_length\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n","\n","def create_dataloader_v1(txt, batch_size=16, max_length=1024,\n","                         stride=512, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader\n","\n","# Train/validation ratio\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","train_data = text_data[:split_idx]\n","val_data = text_data[split_idx:]\n","\n","\n","torch.manual_seed(123)\n","\n","train_loader = create_dataloader_v1(\n","    train_data,\n","    batch_size=4,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    val_data,\n","    batch_size=4,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval();\n","\n","def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # Reduce the number of batches to match the total number of batches in the data loader\n","        # if num_batches exceeds the number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","        else:\n","            break\n","    return total_loss / num_batches\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Epoch {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","        # Print a sample text after each epoch\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context\n","        )\n","\n","    return train_losses, val_losses, track_tokens_seen\n","\n","def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","    model.eval()\n","    with torch.no_grad():\n","        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n","        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n","    model.train()\n","    return train_loss, val_loss\n","\n","def generate_and_print_sample(model, tokenizer, device, start_context):\n","    model.eval()\n","    context_size = model.pos_emb.weight.shape[0]\n","    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n","    with torch.no_grad():\n","        token_ids = generate_text_simple(\n","            model=model, idx=encoded,\n","            max_new_tokens=50, context_size=context_size\n","        )\n","    decoded_text = token_ids_to_text(token_ids, tokenizer)\n","    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n","    model.train()\n","\n","def save_checkpoint(epoch, model, optimizer, save_path=\"model_checkpoint1.pth\"):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, save_path)\n","    print(f\"Checkpoint saved at epoch {epoch + 1}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Put6Xahs5SQs","executionInfo":{"status":"ok","timestamp":1743738950340,"user_tz":-330,"elapsed":55722,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"65666171-a920-4902-de33-85b700235ec8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.9.0\n","Characters: 121892397\n","Tokens: 26316250\n"]}]},{"cell_type":"code","source":["# Note:\n","# Uncomment the following code to calculate the execution time\n","import time\n","start_time = time.time()\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n","\n","num_epochs = 1\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",")\n","\n","# Note:\n","# Uncomment the following code to show the execution time\n","end_time = time.time()\n","execution_time_minutes = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEVzQjx9rAoC","executionInfo":{"status":"ok","timestamp":1743752279945,"user_tz":-330,"elapsed":13287276,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"01368e8e-6379-4135-de6c-cbe9faa1831b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 (Step 000000): Train loss 9.991, Val loss 9.996\n","Epoch 1 (Step 000005): Train loss 8.714, Val loss 8.663\n","Epoch 1 (Step 000010): Train loss 8.117, Val loss 8.022\n","Epoch 1 (Step 000015): Train loss 7.951, Val loss 7.877\n","Epoch 1 (Step 000020): Train loss 7.941, Val loss 7.868\n","Epoch 1 (Step 000025): Train loss 7.912, Val loss 7.802\n","Epoch 1 (Step 000030): Train loss 7.788, Val loss 7.761\n","Epoch 1 (Step 000035): Train loss 7.680, Val loss 7.688\n","Epoch 1 (Step 000040): Train loss 7.682, Val loss 7.648\n","Epoch 1 (Step 000045): Train loss 7.604, Val loss 7.597\n","Epoch 1 (Step 000050): Train loss 7.576, Val loss 7.550\n","Epoch 1 (Step 000055): Train loss 7.574, Val loss 7.502\n","Epoch 1 (Step 000060): Train loss 7.488, Val loss 7.486\n","Epoch 1 (Step 000065): Train loss 7.651, Val loss 7.480\n","Epoch 1 (Step 000070): Train loss 7.482, Val loss 7.479\n","Epoch 1 (Step 000075): Train loss 7.473, Val loss 7.423\n","Epoch 1 (Step 000080): Train loss 7.378, Val loss 7.416\n","Epoch 1 (Step 000085): Train loss 7.296, Val loss 7.420\n","Epoch 1 (Step 000090): Train loss 7.440, Val loss 7.360\n","Epoch 1 (Step 000095): Train loss 7.199, Val loss 7.338\n","Epoch 1 (Step 000100): Train loss 7.373, Val loss 7.306\n","Epoch 1 (Step 000105): Train loss 7.348, Val loss 7.273\n","Epoch 1 (Step 000110): Train loss 7.307, Val loss 7.258\n","Epoch 1 (Step 000115): Train loss 7.311, Val loss 7.260\n","Epoch 1 (Step 000120): Train loss 7.299, Val loss 7.253\n","Epoch 1 (Step 000125): Train loss 7.253, Val loss 7.266\n","Epoch 1 (Step 000130): Train loss 7.381, Val loss 7.249\n","Epoch 1 (Step 000135): Train loss 7.272, Val loss 7.248\n","Epoch 1 (Step 000140): Train loss 7.310, Val loss 7.228\n","Epoch 1 (Step 000145): Train loss 7.135, Val loss 7.217\n","Epoch 1 (Step 000150): Train loss 7.310, Val loss 7.207\n","Epoch 1 (Step 000155): Train loss 7.253, Val loss 7.188\n","Epoch 1 (Step 000160): Train loss 7.181, Val loss 7.187\n","Epoch 1 (Step 000165): Train loss 7.245, Val loss 7.168\n","Epoch 1 (Step 000170): Train loss 7.147, Val loss 7.162\n","Epoch 1 (Step 000175): Train loss 7.129, Val loss 7.123\n","Epoch 1 (Step 000180): Train loss 7.098, Val loss 7.109\n","Epoch 1 (Step 000185): Train loss 7.157, Val loss 7.106\n","Epoch 1 (Step 000190): Train loss 7.084, Val loss 7.112\n","Epoch 1 (Step 000195): Train loss 7.093, Val loss 7.082\n","Epoch 1 (Step 000200): Train loss 7.181, Val loss 7.081\n","Epoch 1 (Step 000205): Train loss 7.244, Val loss 7.092\n","Epoch 1 (Step 000210): Train loss 7.159, Val loss 7.086\n","Epoch 1 (Step 000215): Train loss 7.090, Val loss 7.074\n","Epoch 1 (Step 000220): Train loss 7.136, Val loss 7.065\n","Epoch 1 (Step 000225): Train loss 7.072, Val loss 7.050\n","Epoch 1 (Step 000230): Train loss 7.198, Val loss 7.052\n","Epoch 1 (Step 000235): Train loss 7.036, Val loss 7.017\n","Epoch 1 (Step 000240): Train loss 7.069, Val loss 7.002\n","Epoch 1 (Step 000245): Train loss 7.109, Val loss 7.010\n","Epoch 1 (Step 000250): Train loss 7.085, Val loss 7.003\n","Epoch 1 (Step 000255): Train loss 7.072, Val loss 7.005\n","Epoch 1 (Step 000260): Train loss 7.023, Val loss 6.994\n","Epoch 1 (Step 000265): Train loss 7.082, Val loss 6.976\n","Epoch 1 (Step 000270): Train loss 7.041, Val loss 6.988\n","Epoch 1 (Step 000275): Train loss 7.053, Val loss 6.967\n","Epoch 1 (Step 000280): Train loss 6.980, Val loss 6.955\n","Epoch 1 (Step 000285): Train loss 6.958, Val loss 6.950\n","Epoch 1 (Step 000290): Train loss 6.949, Val loss 6.941\n","Epoch 1 (Step 000295): Train loss 7.025, Val loss 6.940\n","Epoch 1 (Step 000300): Train loss 6.950, Val loss 6.952\n","Epoch 1 (Step 000305): Train loss 6.926, Val loss 6.928\n","Epoch 1 (Step 000310): Train loss 6.877, Val loss 6.926\n","Epoch 1 (Step 000315): Train loss 6.970, Val loss 6.924\n","Epoch 1 (Step 000320): Train loss 6.848, Val loss 6.910\n","Epoch 1 (Step 000325): Train loss 6.979, Val loss 6.903\n","Epoch 1 (Step 000330): Train loss 6.899, Val loss 6.910\n","Epoch 1 (Step 000335): Train loss 6.876, Val loss 6.889\n","Epoch 1 (Step 000340): Train loss 6.815, Val loss 6.895\n","Epoch 1 (Step 000345): Train loss 6.929, Val loss 6.878\n","Epoch 1 (Step 000350): Train loss 6.893, Val loss 6.884\n","Epoch 1 (Step 000355): Train loss 6.905, Val loss 6.895\n","Epoch 1 (Step 000360): Train loss 6.906, Val loss 6.876\n","Epoch 1 (Step 000365): Train loss 6.868, Val loss 6.867\n","Epoch 1 (Step 000370): Train loss 7.066, Val loss 6.866\n","Epoch 1 (Step 000375): Train loss 6.910, Val loss 6.889\n","Epoch 1 (Step 000380): Train loss 6.917, Val loss 6.857\n","Epoch 1 (Step 000385): Train loss 6.875, Val loss 6.867\n","Epoch 1 (Step 000390): Train loss 6.917, Val loss 6.859\n","Epoch 1 (Step 000395): Train loss 6.782, Val loss 6.848\n","Epoch 1 (Step 000400): Train loss 6.838, Val loss 6.838\n","Epoch 1 (Step 000405): Train loss 6.844, Val loss 6.840\n","Epoch 1 (Step 000410): Train loss 6.762, Val loss 6.855\n","Epoch 1 (Step 000415): Train loss 6.921, Val loss 6.831\n","Epoch 1 (Step 000420): Train loss 6.826, Val loss 6.835\n","Epoch 1 (Step 000425): Train loss 6.812, Val loss 6.832\n","Epoch 1 (Step 000430): Train loss 6.785, Val loss 6.785\n","Epoch 1 (Step 000435): Train loss 6.815, Val loss 6.796\n","Epoch 1 (Step 000440): Train loss 6.829, Val loss 6.778\n","Epoch 1 (Step 000445): Train loss 6.808, Val loss 6.776\n","Epoch 1 (Step 000450): Train loss 6.715, Val loss 6.794\n","Epoch 1 (Step 000455): Train loss 6.899, Val loss 6.767\n","Epoch 1 (Step 000460): Train loss 6.794, Val loss 6.757\n","Epoch 1 (Step 000465): Train loss 6.727, Val loss 6.761\n","Epoch 1 (Step 000470): Train loss 6.812, Val loss 6.763\n","Epoch 1 (Step 000475): Train loss 6.724, Val loss 6.752\n","Epoch 1 (Step 000480): Train loss 6.786, Val loss 6.751\n","Epoch 1 (Step 000485): Train loss 6.691, Val loss 6.747\n","Epoch 1 (Step 000490): Train loss 6.801, Val loss 6.752\n","Epoch 1 (Step 000495): Train loss 6.894, Val loss 6.756\n","Epoch 1 (Step 000500): Train loss 6.783, Val loss 6.745\n","Epoch 1 (Step 000505): Train loss 6.693, Val loss 6.768\n","Epoch 1 (Step 000510): Train loss 6.846, Val loss 6.746\n","Epoch 1 (Step 000515): Train loss 6.680, Val loss 6.768\n","Epoch 1 (Step 000520): Train loss 6.698, Val loss 6.737\n","Epoch 1 (Step 000525): Train loss 6.762, Val loss 6.739\n","Epoch 1 (Step 000530): Train loss 6.689, Val loss 6.725\n","Epoch 1 (Step 000535): Train loss 6.843, Val loss 6.720\n","Epoch 1 (Step 000540): Train loss 6.660, Val loss 6.711\n","Epoch 1 (Step 000545): Train loss 6.616, Val loss 6.703\n","Epoch 1 (Step 000550): Train loss 6.749, Val loss 6.714\n","Epoch 1 (Step 000555): Train loss 6.783, Val loss 6.721\n","Epoch 1 (Step 000560): Train loss 6.671, Val loss 6.720\n","Epoch 1 (Step 000565): Train loss 6.655, Val loss 6.724\n","Epoch 1 (Step 000570): Train loss 6.726, Val loss 6.720\n","Epoch 1 (Step 000575): Train loss 6.683, Val loss 6.719\n","Epoch 1 (Step 000580): Train loss 6.562, Val loss 6.719\n","Epoch 1 (Step 000585): Train loss 6.645, Val loss 6.719\n","Epoch 1 (Step 000590): Train loss 6.707, Val loss 6.720\n","Epoch 1 (Step 000595): Train loss 6.652, Val loss 6.696\n","Epoch 1 (Step 000600): Train loss 6.615, Val loss 6.675\n","Epoch 1 (Step 000605): Train loss 6.630, Val loss 6.681\n","Epoch 1 (Step 000610): Train loss 6.656, Val loss 6.682\n","Epoch 1 (Step 000615): Train loss 6.468, Val loss 6.670\n","Epoch 1 (Step 000620): Train loss 6.740, Val loss 6.671\n","Epoch 1 (Step 000625): Train loss 6.584, Val loss 6.701\n","Epoch 1 (Step 000630): Train loss 6.577, Val loss 6.697\n","Epoch 1 (Step 000635): Train loss 6.592, Val loss 6.683\n","Epoch 1 (Step 000640): Train loss 6.673, Val loss 6.664\n","Epoch 1 (Step 000645): Train loss 6.604, Val loss 6.659\n","Epoch 1 (Step 000650): Train loss 6.596, Val loss 6.644\n","Epoch 1 (Step 000655): Train loss 6.626, Val loss 6.643\n","Epoch 1 (Step 000660): Train loss 6.544, Val loss 6.660\n","Epoch 1 (Step 000665): Train loss 6.559, Val loss 6.665\n","Epoch 1 (Step 000670): Train loss 6.724, Val loss 6.676\n","Epoch 1 (Step 000675): Train loss 6.618, Val loss 6.641\n","Epoch 1 (Step 000680): Train loss 6.620, Val loss 6.646\n","Epoch 1 (Step 000685): Train loss 6.657, Val loss 6.648\n","Epoch 1 (Step 000690): Train loss 6.661, Val loss 6.653\n","Epoch 1 (Step 000695): Train loss 6.610, Val loss 6.623\n","Epoch 1 (Step 000700): Train loss 6.682, Val loss 6.617\n","Epoch 1 (Step 000705): Train loss 6.507, Val loss 6.615\n","Epoch 1 (Step 000710): Train loss 6.569, Val loss 6.602\n","Epoch 1 (Step 000715): Train loss 6.567, Val loss 6.614\n","Epoch 1 (Step 000720): Train loss 6.626, Val loss 6.619\n","Epoch 1 (Step 000725): Train loss 6.516, Val loss 6.606\n","Epoch 1 (Step 000730): Train loss 6.726, Val loss 6.601\n","Epoch 1 (Step 000735): Train loss 6.548, Val loss 6.594\n","Epoch 1 (Step 000740): Train loss 6.544, Val loss 6.599\n","Epoch 1 (Step 000745): Train loss 6.585, Val loss 6.605\n","Epoch 1 (Step 000750): Train loss 6.590, Val loss 6.612\n","Epoch 1 (Step 000755): Train loss 6.549, Val loss 6.587\n","Epoch 1 (Step 000760): Train loss 6.671, Val loss 6.599\n","Epoch 1 (Step 000765): Train loss 6.612, Val loss 6.617\n","Epoch 1 (Step 000770): Train loss 6.647, Val loss 6.591\n","Epoch 1 (Step 000775): Train loss 6.669, Val loss 6.635\n","Epoch 1 (Step 000780): Train loss 6.586, Val loss 6.583\n","Epoch 1 (Step 000785): Train loss 6.459, Val loss 6.591\n","Epoch 1 (Step 000790): Train loss 6.535, Val loss 6.593\n","Epoch 1 (Step 000795): Train loss 6.520, Val loss 6.599\n","Epoch 1 (Step 000800): Train loss 6.605, Val loss 6.603\n","Epoch 1 (Step 000805): Train loss 6.563, Val loss 6.569\n","Epoch 1 (Step 000810): Train loss 6.505, Val loss 6.575\n","Epoch 1 (Step 000815): Train loss 6.570, Val loss 6.588\n","Epoch 1 (Step 000820): Train loss 6.540, Val loss 6.586\n","Epoch 1 (Step 000825): Train loss 6.511, Val loss 6.570\n","Epoch 1 (Step 000830): Train loss 6.391, Val loss 6.568\n","Epoch 1 (Step 000835): Train loss 6.586, Val loss 6.565\n","Epoch 1 (Step 000840): Train loss 6.540, Val loss 6.557\n","Epoch 1 (Step 000845): Train loss 6.474, Val loss 6.554\n","Epoch 1 (Step 000850): Train loss 6.548, Val loss 6.558\n","Epoch 1 (Step 000855): Train loss 6.492, Val loss 6.561\n","Epoch 1 (Step 000860): Train loss 6.570, Val loss 6.563\n","Epoch 1 (Step 000865): Train loss 6.506, Val loss 6.598\n","Epoch 1 (Step 000870): Train loss 6.422, Val loss 6.575\n","Epoch 1 (Step 000875): Train loss 6.481, Val loss 6.565\n","Epoch 1 (Step 000880): Train loss 6.463, Val loss 6.550\n","Epoch 1 (Step 000885): Train loss 6.526, Val loss 6.545\n","Epoch 1 (Step 000890): Train loss 6.540, Val loss 6.541\n","Epoch 1 (Step 000895): Train loss 6.532, Val loss 6.558\n","Epoch 1 (Step 000900): Train loss 6.319, Val loss 6.528\n","Epoch 1 (Step 000905): Train loss 6.558, Val loss 6.530\n","Epoch 1 (Step 000910): Train loss 6.422, Val loss 6.520\n","Epoch 1 (Step 000915): Train loss 6.402, Val loss 6.527\n","Epoch 1 (Step 000920): Train loss 6.415, Val loss 6.511\n","Epoch 1 (Step 000925): Train loss 6.535, Val loss 6.514\n","Epoch 1 (Step 000930): Train loss 6.558, Val loss 6.514\n","Epoch 1 (Step 000935): Train loss 6.565, Val loss 6.538\n","Epoch 1 (Step 000940): Train loss 6.407, Val loss 6.532\n","Epoch 1 (Step 000945): Train loss 6.573, Val loss 6.550\n","Epoch 1 (Step 000950): Train loss 6.505, Val loss 6.526\n","Epoch 1 (Step 000955): Train loss 6.320, Val loss 6.526\n","Epoch 1 (Step 000960): Train loss 6.480, Val loss 6.523\n","Epoch 1 (Step 000965): Train loss 6.562, Val loss 6.510\n","Epoch 1 (Step 000970): Train loss 6.435, Val loss 6.506\n","Epoch 1 (Step 000975): Train loss 6.461, Val loss 6.493\n","Epoch 1 (Step 000980): Train loss 6.380, Val loss 6.478\n","Epoch 1 (Step 000985): Train loss 6.518, Val loss 6.491\n","Epoch 1 (Step 000990): Train loss 6.486, Val loss 6.455\n","Epoch 1 (Step 000995): Train loss 6.591, Val loss 6.473\n","Epoch 1 (Step 001000): Train loss 6.391, Val loss 6.458\n","Epoch 1 (Step 001005): Train loss 6.445, Val loss 6.455\n","Epoch 1 (Step 001010): Train loss 6.505, Val loss 6.465\n","Epoch 1 (Step 001015): Train loss 6.431, Val loss 6.467\n","Epoch 1 (Step 001020): Train loss 6.486, Val loss 6.480\n","Epoch 1 (Step 001025): Train loss 6.488, Val loss 6.468\n","Epoch 1 (Step 001030): Train loss 6.396, Val loss 6.471\n","Epoch 1 (Step 001035): Train loss 6.342, Val loss 6.482\n","Epoch 1 (Step 001040): Train loss 6.472, Val loss 6.482\n","Epoch 1 (Step 001045): Train loss 6.456, Val loss 6.483\n","Epoch 1 (Step 001050): Train loss 6.401, Val loss 6.487\n","Epoch 1 (Step 001055): Train loss 6.523, Val loss 6.478\n","Epoch 1 (Step 001060): Train loss 6.358, Val loss 6.496\n","Epoch 1 (Step 001065): Train loss 6.391, Val loss 6.469\n","Epoch 1 (Step 001070): Train loss 6.404, Val loss 6.493\n","Epoch 1 (Step 001075): Train loss 6.429, Val loss 6.490\n","Epoch 1 (Step 001080): Train loss 6.332, Val loss 6.469\n","Epoch 1 (Step 001085): Train loss 6.306, Val loss 6.460\n","Epoch 1 (Step 001090): Train loss 6.513, Val loss 6.484\n","Epoch 1 (Step 001095): Train loss 6.434, Val loss 6.466\n","Epoch 1 (Step 001100): Train loss 6.372, Val loss 6.473\n","Epoch 1 (Step 001105): Train loss 6.437, Val loss 6.471\n","Epoch 1 (Step 001110): Train loss 6.490, Val loss 6.457\n","Epoch 1 (Step 001115): Train loss 6.330, Val loss 6.467\n","Epoch 1 (Step 001120): Train loss 6.477, Val loss 6.487\n","Epoch 1 (Step 001125): Train loss 6.530, Val loss 6.475\n","Epoch 1 (Step 001130): Train loss 6.418, Val loss 6.467\n","Epoch 1 (Step 001135): Train loss 6.369, Val loss 6.468\n","Epoch 1 (Step 001140): Train loss 6.354, Val loss 6.463\n","Epoch 1 (Step 001145): Train loss 6.487, Val loss 6.445\n","Epoch 1 (Step 001150): Train loss 6.301, Val loss 6.444\n","Epoch 1 (Step 001155): Train loss 6.293, Val loss 6.450\n","Epoch 1 (Step 001160): Train loss 6.368, Val loss 6.463\n","Epoch 1 (Step 001165): Train loss 6.422, Val loss 6.472\n","Epoch 1 (Step 001170): Train loss 6.232, Val loss 6.464\n","Epoch 1 (Step 001175): Train loss 6.476, Val loss 6.463\n","Epoch 1 (Step 001180): Train loss 6.332, Val loss 6.449\n","Epoch 1 (Step 001185): Train loss 6.390, Val loss 6.428\n","Epoch 1 (Step 001190): Train loss 6.350, Val loss 6.433\n","Epoch 1 (Step 001195): Train loss 6.394, Val loss 6.434\n","Epoch 1 (Step 001200): Train loss 6.307, Val loss 6.450\n","Epoch 1 (Step 001205): Train loss 6.451, Val loss 6.451\n","Epoch 1 (Step 001210): Train loss 6.347, Val loss 6.449\n","Epoch 1 (Step 001215): Train loss 6.494, Val loss 6.428\n","Epoch 1 (Step 001220): Train loss 6.432, Val loss 6.426\n","Epoch 1 (Step 001225): Train loss 6.398, Val loss 6.417\n","Epoch 1 (Step 001230): Train loss 6.430, Val loss 6.445\n","Epoch 1 (Step 001235): Train loss 6.382, Val loss 6.426\n","Epoch 1 (Step 001240): Train loss 6.534, Val loss 6.452\n","Epoch 1 (Step 001245): Train loss 6.441, Val loss 6.441\n","Epoch 1 (Step 001250): Train loss 6.321, Val loss 6.460\n","Epoch 1 (Step 001255): Train loss 6.354, Val loss 6.443\n","Epoch 1 (Step 001260): Train loss 6.374, Val loss 6.422\n","Epoch 1 (Step 001265): Train loss 6.334, Val loss 6.420\n","Epoch 1 (Step 001270): Train loss 6.334, Val loss 6.430\n","Epoch 1 (Step 001275): Train loss 6.296, Val loss 6.426\n","Epoch 1 (Step 001280): Train loss 6.436, Val loss 6.409\n","Epoch 1 (Step 001285): Train loss 6.234, Val loss 6.442\n","Epoch 1 (Step 001290): Train loss 6.344, Val loss 6.439\n","Epoch 1 (Step 001295): Train loss 6.320, Val loss 6.421\n","Epoch 1 (Step 001300): Train loss 6.303, Val loss 6.432\n","Epoch 1 (Step 001305): Train loss 6.359, Val loss 6.427\n","Epoch 1 (Step 001310): Train loss 6.292, Val loss 6.426\n","Epoch 1 (Step 001315): Train loss 6.463, Val loss 6.424\n","Epoch 1 (Step 001320): Train loss 6.212, Val loss 6.414\n","Epoch 1 (Step 001325): Train loss 6.387, Val loss 6.427\n","Epoch 1 (Step 001330): Train loss 6.172, Val loss 6.402\n","Epoch 1 (Step 001335): Train loss 6.262, Val loss 6.411\n","Epoch 1 (Step 001340): Train loss 6.371, Val loss 6.405\n","Epoch 1 (Step 001345): Train loss 6.284, Val loss 6.419\n","Epoch 1 (Step 001350): Train loss 6.322, Val loss 6.399\n","Epoch 1 (Step 001355): Train loss 6.317, Val loss 6.414\n","Epoch 1 (Step 001360): Train loss 6.462, Val loss 6.400\n","Epoch 1 (Step 001365): Train loss 6.301, Val loss 6.407\n","Epoch 1 (Step 001370): Train loss 6.336, Val loss 6.409\n","Epoch 1 (Step 001375): Train loss 6.236, Val loss 6.379\n","Epoch 1 (Step 001380): Train loss 6.313, Val loss 6.413\n","Epoch 1 (Step 001385): Train loss 6.381, Val loss 6.391\n","Epoch 1 (Step 001390): Train loss 6.441, Val loss 6.380\n","Epoch 1 (Step 001395): Train loss 6.294, Val loss 6.382\n","Epoch 1 (Step 001400): Train loss 6.301, Val loss 6.382\n","Epoch 1 (Step 001405): Train loss 6.480, Val loss 6.390\n","Epoch 1 (Step 001410): Train loss 6.324, Val loss 6.381\n","Epoch 1 (Step 001415): Train loss 6.380, Val loss 6.376\n","Epoch 1 (Step 001420): Train loss 6.285, Val loss 6.391\n","Epoch 1 (Step 001425): Train loss 6.368, Val loss 6.385\n","Epoch 1 (Step 001430): Train loss 6.403, Val loss 6.370\n","Epoch 1 (Step 001435): Train loss 6.305, Val loss 6.388\n","Epoch 1 (Step 001440): Train loss 6.487, Val loss 6.366\n","Epoch 1 (Step 001445): Train loss 6.277, Val loss 6.377\n","Epoch 1 (Step 001450): Train loss 6.187, Val loss 6.379\n","Epoch 1 (Step 001455): Train loss 6.373, Val loss 6.365\n","Epoch 1 (Step 001460): Train loss 6.322, Val loss 6.352\n","Epoch 1 (Step 001465): Train loss 6.408, Val loss 6.359\n","Epoch 1 (Step 001470): Train loss 6.348, Val loss 6.354\n","Epoch 1 (Step 001475): Train loss 6.286, Val loss 6.346\n","Epoch 1 (Step 001480): Train loss 6.362, Val loss 6.365\n","Epoch 1 (Step 001485): Train loss 6.202, Val loss 6.375\n","Epoch 1 (Step 001490): Train loss 6.420, Val loss 6.370\n","Epoch 1 (Step 001495): Train loss 6.290, Val loss 6.407\n","Epoch 1 (Step 001500): Train loss 6.393, Val loss 6.369\n","Epoch 1 (Step 001505): Train loss 6.141, Val loss 6.381\n","Epoch 1 (Step 001510): Train loss 6.225, Val loss 6.344\n","Epoch 1 (Step 001515): Train loss 6.313, Val loss 6.345\n","Epoch 1 (Step 001520): Train loss 6.321, Val loss 6.375\n","Epoch 1 (Step 001525): Train loss 6.250, Val loss 6.368\n","Epoch 1 (Step 001530): Train loss 6.252, Val loss 6.357\n","Epoch 1 (Step 001535): Train loss 6.230, Val loss 6.348\n","Epoch 1 (Step 001540): Train loss 6.421, Val loss 6.344\n","Epoch 1 (Step 001545): Train loss 6.235, Val loss 6.360\n","Epoch 1 (Step 001550): Train loss 6.311, Val loss 6.332\n","Epoch 1 (Step 001555): Train loss 6.120, Val loss 6.342\n","Epoch 1 (Step 001560): Train loss 6.228, Val loss 6.335\n","Epoch 1 (Step 001565): Train loss 6.292, Val loss 6.338\n","Epoch 1 (Step 001570): Train loss 6.366, Val loss 6.345\n","Epoch 1 (Step 001575): Train loss 6.232, Val loss 6.345\n","Epoch 1 (Step 001580): Train loss 6.357, Val loss 6.350\n","Epoch 1 (Step 001585): Train loss 6.155, Val loss 6.347\n","Epoch 1 (Step 001590): Train loss 6.229, Val loss 6.348\n","Epoch 1 (Step 001595): Train loss 6.317, Val loss 6.323\n","Epoch 1 (Step 001600): Train loss 6.245, Val loss 6.329\n","Epoch 1 (Step 001605): Train loss 6.289, Val loss 6.323\n","Epoch 1 (Step 001610): Train loss 6.293, Val loss 6.334\n","Epoch 1 (Step 001615): Train loss 6.247, Val loss 6.350\n","Epoch 1 (Step 001620): Train loss 6.230, Val loss 6.340\n","Epoch 1 (Step 001625): Train loss 6.224, Val loss 6.332\n","Epoch 1 (Step 001630): Train loss 6.217, Val loss 6.323\n","Epoch 1 (Step 001635): Train loss 6.133, Val loss 6.322\n","Epoch 1 (Step 001640): Train loss 6.377, Val loss 6.321\n","Epoch 1 (Step 001645): Train loss 6.254, Val loss 6.332\n","Epoch 1 (Step 001650): Train loss 6.173, Val loss 6.319\n","Epoch 1 (Step 001655): Train loss 6.231, Val loss 6.306\n","Epoch 1 (Step 001660): Train loss 6.188, Val loss 6.315\n","Epoch 1 (Step 001665): Train loss 6.207, Val loss 6.328\n","Epoch 1 (Step 001670): Train loss 6.200, Val loss 6.334\n","Epoch 1 (Step 001675): Train loss 6.355, Val loss 6.330\n","Epoch 1 (Step 001680): Train loss 6.194, Val loss 6.333\n","Epoch 1 (Step 001685): Train loss 6.091, Val loss 6.310\n","Epoch 1 (Step 001690): Train loss 6.190, Val loss 6.314\n","Epoch 1 (Step 001695): Train loss 6.257, Val loss 6.326\n","Epoch 1 (Step 001700): Train loss 6.230, Val loss 6.316\n","Epoch 1 (Step 001705): Train loss 6.199, Val loss 6.327\n","Epoch 1 (Step 001710): Train loss 6.224, Val loss 6.340\n","Epoch 1 (Step 001715): Train loss 6.231, Val loss 6.333\n","Epoch 1 (Step 001720): Train loss 6.181, Val loss 6.315\n","Epoch 1 (Step 001725): Train loss 6.159, Val loss 6.315\n","Epoch 1 (Step 001730): Train loss 6.157, Val loss 6.309\n","Epoch 1 (Step 001735): Train loss 6.280, Val loss 6.292\n","Epoch 1 (Step 001740): Train loss 6.234, Val loss 6.303\n","Epoch 1 (Step 001745): Train loss 6.290, Val loss 6.294\n","Epoch 1 (Step 001750): Train loss 6.188, Val loss 6.307\n","Epoch 1 (Step 001755): Train loss 6.205, Val loss 6.320\n","Epoch 1 (Step 001760): Train loss 5.943, Val loss 6.304\n","Epoch 1 (Step 001765): Train loss 6.179, Val loss 6.325\n","Epoch 1 (Step 001770): Train loss 6.224, Val loss 6.329\n","Epoch 1 (Step 001775): Train loss 6.147, Val loss 6.318\n","Epoch 1 (Step 001780): Train loss 6.071, Val loss 6.295\n","Epoch 1 (Step 001785): Train loss 6.251, Val loss 6.311\n","Epoch 1 (Step 001790): Train loss 6.218, Val loss 6.282\n","Epoch 1 (Step 001795): Train loss 6.117, Val loss 6.277\n","Epoch 1 (Step 001800): Train loss 6.276, Val loss 6.272\n","Epoch 1 (Step 001805): Train loss 6.251, Val loss 6.272\n","Epoch 1 (Step 001810): Train loss 6.168, Val loss 6.284\n","Epoch 1 (Step 001815): Train loss 6.017, Val loss 6.274\n","Epoch 1 (Step 001820): Train loss 6.307, Val loss 6.270\n","Epoch 1 (Step 001825): Train loss 6.241, Val loss 6.275\n","Epoch 1 (Step 001830): Train loss 6.168, Val loss 6.285\n","Epoch 1 (Step 001835): Train loss 6.099, Val loss 6.290\n","Epoch 1 (Step 001840): Train loss 6.182, Val loss 6.281\n","Epoch 1 (Step 001845): Train loss 6.263, Val loss 6.294\n","Epoch 1 (Step 001850): Train loss 6.125, Val loss 6.293\n","Epoch 1 (Step 001855): Train loss 6.049, Val loss 6.272\n","Epoch 1 (Step 001860): Train loss 6.099, Val loss 6.304\n","Epoch 1 (Step 001865): Train loss 6.126, Val loss 6.267\n","Epoch 1 (Step 001870): Train loss 6.088, Val loss 6.282\n","Epoch 1 (Step 001875): Train loss 6.106, Val loss 6.285\n","Epoch 1 (Step 001880): Train loss 6.174, Val loss 6.269\n","Epoch 1 (Step 001885): Train loss 6.173, Val loss 6.259\n","Epoch 1 (Step 001890): Train loss 6.009, Val loss 6.273\n","Epoch 1 (Step 001895): Train loss 6.089, Val loss 6.286\n","Epoch 1 (Step 001900): Train loss 6.074, Val loss 6.265\n","Epoch 1 (Step 001905): Train loss 6.145, Val loss 6.257\n","Epoch 1 (Step 001910): Train loss 5.991, Val loss 6.262\n","Epoch 1 (Step 001915): Train loss 6.122, Val loss 6.268\n","Epoch 1 (Step 001920): Train loss 6.153, Val loss 6.259\n","Epoch 1 (Step 001925): Train loss 6.307, Val loss 6.258\n","Epoch 1 (Step 001930): Train loss 6.194, Val loss 6.251\n","Epoch 1 (Step 001935): Train loss 6.194, Val loss 6.238\n","Epoch 1 (Step 001940): Train loss 5.964, Val loss 6.266\n","Epoch 1 (Step 001945): Train loss 6.079, Val loss 6.246\n","Epoch 1 (Step 001950): Train loss 6.180, Val loss 6.236\n","Epoch 1 (Step 001955): Train loss 5.981, Val loss 6.238\n","Epoch 1 (Step 001960): Train loss 6.222, Val loss 6.258\n","Epoch 1 (Step 001965): Train loss 6.013, Val loss 6.224\n","Epoch 1 (Step 001970): Train loss 6.241, Val loss 6.212\n","Epoch 1 (Step 001975): Train loss 6.195, Val loss 6.245\n","Epoch 1 (Step 001980): Train loss 6.142, Val loss 6.222\n","Epoch 1 (Step 001985): Train loss 6.121, Val loss 6.243\n","Epoch 1 (Step 001990): Train loss 6.099, Val loss 6.248\n","Epoch 1 (Step 001995): Train loss 6.172, Val loss 6.259\n","Epoch 1 (Step 002000): Train loss 6.044, Val loss 6.263\n","Epoch 1 (Step 002005): Train loss 6.074, Val loss 6.242\n","Epoch 1 (Step 002010): Train loss 6.172, Val loss 6.246\n","Epoch 1 (Step 002015): Train loss 6.088, Val loss 6.242\n","Epoch 1 (Step 002020): Train loss 6.054, Val loss 6.228\n","Epoch 1 (Step 002025): Train loss 6.109, Val loss 6.259\n","Epoch 1 (Step 002030): Train loss 6.087, Val loss 6.285\n","Epoch 1 (Step 002035): Train loss 6.156, Val loss 6.255\n","Epoch 1 (Step 002040): Train loss 6.049, Val loss 6.243\n","Epoch 1 (Step 002045): Train loss 6.018, Val loss 6.239\n","Epoch 1 (Step 002050): Train loss 6.063, Val loss 6.242\n","Epoch 1 (Step 002055): Train loss 6.160, Val loss 6.230\n","Epoch 1 (Step 002060): Train loss 5.937, Val loss 6.234\n","Epoch 1 (Step 002065): Train loss 6.075, Val loss 6.205\n","Epoch 1 (Step 002070): Train loss 6.152, Val loss 6.221\n","Epoch 1 (Step 002075): Train loss 6.124, Val loss 6.229\n","Epoch 1 (Step 002080): Train loss 6.053, Val loss 6.223\n","Epoch 1 (Step 002085): Train loss 6.104, Val loss 6.228\n","Epoch 1 (Step 002090): Train loss 6.340, Val loss 6.259\n","Epoch 1 (Step 002095): Train loss 6.219, Val loss 6.250\n","Epoch 1 (Step 002100): Train loss 6.195, Val loss 6.243\n","Epoch 1 (Step 002105): Train loss 6.209, Val loss 6.258\n","Epoch 1 (Step 002110): Train loss 6.231, Val loss 6.221\n","Epoch 1 (Step 002115): Train loss 6.045, Val loss 6.226\n","Epoch 1 (Step 002120): Train loss 6.112, Val loss 6.236\n","Epoch 1 (Step 002125): Train loss 6.157, Val loss 6.228\n","Epoch 1 (Step 002130): Train loss 6.031, Val loss 6.255\n","Epoch 1 (Step 002135): Train loss 6.285, Val loss 6.230\n","Epoch 1 (Step 002140): Train loss 6.151, Val loss 6.227\n","Epoch 1 (Step 002145): Train loss 5.995, Val loss 6.241\n","Epoch 1 (Step 002150): Train loss 6.201, Val loss 6.262\n","Epoch 1 (Step 002155): Train loss 6.071, Val loss 6.240\n","Epoch 1 (Step 002160): Train loss 6.181, Val loss 6.231\n","Epoch 1 (Step 002165): Train loss 6.046, Val loss 6.216\n","Epoch 1 (Step 002170): Train loss 6.105, Val loss 6.204\n","Epoch 1 (Step 002175): Train loss 6.142, Val loss 6.226\n","Epoch 1 (Step 002180): Train loss 5.985, Val loss 6.208\n","Epoch 1 (Step 002185): Train loss 6.005, Val loss 6.221\n","Epoch 1 (Step 002190): Train loss 6.085, Val loss 6.227\n","Epoch 1 (Step 002195): Train loss 6.190, Val loss 6.252\n","Epoch 1 (Step 002200): Train loss 6.262, Val loss 6.206\n","Epoch 1 (Step 002205): Train loss 6.070, Val loss 6.198\n","Epoch 1 (Step 002210): Train loss 6.136, Val loss 6.217\n","Epoch 1 (Step 002215): Train loss 6.247, Val loss 6.224\n","Epoch 1 (Step 002220): Train loss 6.111, Val loss 6.209\n","Epoch 1 (Step 002225): Train loss 6.042, Val loss 6.216\n","Epoch 1 (Step 002230): Train loss 6.170, Val loss 6.210\n","Epoch 1 (Step 002235): Train loss 6.116, Val loss 6.255\n","Epoch 1 (Step 002240): Train loss 5.886, Val loss 6.226\n","Epoch 1 (Step 002245): Train loss 5.996, Val loss 6.221\n","Epoch 1 (Step 002250): Train loss 6.203, Val loss 6.238\n","Epoch 1 (Step 002255): Train loss 5.972, Val loss 6.218\n","Epoch 1 (Step 002260): Train loss 6.223, Val loss 6.225\n","Epoch 1 (Step 002265): Train loss 6.189, Val loss 6.231\n","Epoch 1 (Step 002270): Train loss 6.183, Val loss 6.206\n","Epoch 1 (Step 002275): Train loss 5.988, Val loss 6.230\n","Epoch 1 (Step 002280): Train loss 6.046, Val loss 6.209\n","Epoch 1 (Step 002285): Train loss 6.095, Val loss 6.223\n","Epoch 1 (Step 002290): Train loss 6.019, Val loss 6.199\n","Epoch 1 (Step 002295): Train loss 5.996, Val loss 6.212\n","Epoch 1 (Step 002300): Train loss 5.942, Val loss 6.196\n","Epoch 1 (Step 002305): Train loss 6.064, Val loss 6.217\n","Epoch 1 (Step 002310): Train loss 6.244, Val loss 6.208\n","Epoch 1 (Step 002315): Train loss 6.185, Val loss 6.199\n","Epoch 1 (Step 002320): Train loss 6.110, Val loss 6.203\n","Epoch 1 (Step 002325): Train loss 6.093, Val loss 6.209\n","Epoch 1 (Step 002330): Train loss 6.157, Val loss 6.181\n","Epoch 1 (Step 002335): Train loss 6.044, Val loss 6.166\n","Epoch 1 (Step 002340): Train loss 6.015, Val loss 6.192\n","Epoch 1 (Step 002345): Train loss 6.022, Val loss 6.188\n","Epoch 1 (Step 002350): Train loss 6.072, Val loss 6.184\n","Epoch 1 (Step 002355): Train loss 6.124, Val loss 6.195\n","Epoch 1 (Step 002360): Train loss 6.138, Val loss 6.190\n","Epoch 1 (Step 002365): Train loss 6.056, Val loss 6.180\n","Epoch 1 (Step 002370): Train loss 6.152, Val loss 6.225\n","Epoch 1 (Step 002375): Train loss 6.012, Val loss 6.204\n","Epoch 1 (Step 002380): Train loss 6.083, Val loss 6.179\n","Epoch 1 (Step 002385): Train loss 6.047, Val loss 6.210\n","Epoch 1 (Step 002390): Train loss 6.100, Val loss 6.209\n","Epoch 1 (Step 002395): Train loss 5.950, Val loss 6.190\n","Epoch 1 (Step 002400): Train loss 6.022, Val loss 6.206\n","Epoch 1 (Step 002405): Train loss 6.100, Val loss 6.187\n","Epoch 1 (Step 002410): Train loss 5.899, Val loss 6.186\n","Epoch 1 (Step 002415): Train loss 6.075, Val loss 6.196\n","Epoch 1 (Step 002420): Train loss 6.107, Val loss 6.198\n","Epoch 1 (Step 002425): Train loss 5.990, Val loss 6.193\n","Epoch 1 (Step 002430): Train loss 6.074, Val loss 6.218\n","Epoch 1 (Step 002435): Train loss 6.196, Val loss 6.200\n","Epoch 1 (Step 002440): Train loss 6.034, Val loss 6.194\n","Epoch 1 (Step 002445): Train loss 6.111, Val loss 6.188\n","Epoch 1 (Step 002450): Train loss 6.097, Val loss 6.176\n","Epoch 1 (Step 002455): Train loss 6.098, Val loss 6.184\n","Epoch 1 (Step 002460): Train loss 6.032, Val loss 6.172\n","Epoch 1 (Step 002465): Train loss 5.955, Val loss 6.175\n","Epoch 1 (Step 002470): Train loss 6.079, Val loss 6.161\n","Epoch 1 (Step 002475): Train loss 6.044, Val loss 6.181\n","Epoch 1 (Step 002480): Train loss 6.035, Val loss 6.182\n","Epoch 1 (Step 002485): Train loss 5.958, Val loss 6.173\n","Epoch 1 (Step 002490): Train loss 6.000, Val loss 6.178\n","Epoch 1 (Step 002495): Train loss 6.086, Val loss 6.169\n","Epoch 1 (Step 002500): Train loss 6.039, Val loss 6.147\n","Epoch 1 (Step 002505): Train loss 6.140, Val loss 6.149\n","Epoch 1 (Step 002510): Train loss 6.045, Val loss 6.151\n","Epoch 1 (Step 002515): Train loss 6.043, Val loss 6.156\n","Epoch 1 (Step 002520): Train loss 5.992, Val loss 6.163\n","Epoch 1 (Step 002525): Train loss 6.033, Val loss 6.165\n","Epoch 1 (Step 002530): Train loss 6.170, Val loss 6.168\n","Epoch 1 (Step 002535): Train loss 6.045, Val loss 6.155\n","Epoch 1 (Step 002540): Train loss 6.026, Val loss 6.131\n","Epoch 1 (Step 002545): Train loss 6.104, Val loss 6.151\n","Epoch 1 (Step 002550): Train loss 5.959, Val loss 6.157\n","Epoch 1 (Step 002555): Train loss 5.986, Val loss 6.162\n","Epoch 1 (Step 002560): Train loss 6.031, Val loss 6.170\n","Epoch 1 (Step 002565): Train loss 6.068, Val loss 6.152\n","Epoch 1 (Step 002570): Train loss 5.968, Val loss 6.157\n","Epoch 1 (Step 002575): Train loss 5.999, Val loss 6.168\n","Epoch 1 (Step 002580): Train loss 5.895, Val loss 6.151\n","Epoch 1 (Step 002585): Train loss 5.938, Val loss 6.202\n","Epoch 1 (Step 002590): Train loss 6.122, Val loss 6.158\n","Epoch 1 (Step 002595): Train loss 6.036, Val loss 6.147\n","Epoch 1 (Step 002600): Train loss 5.986, Val loss 6.156\n","Epoch 1 (Step 002605): Train loss 6.066, Val loss 6.144\n","Epoch 1 (Step 002610): Train loss 6.024, Val loss 6.136\n","Epoch 1 (Step 002615): Train loss 6.210, Val loss 6.125\n","Epoch 1 (Step 002620): Train loss 6.033, Val loss 6.166\n","Epoch 1 (Step 002625): Train loss 6.013, Val loss 6.133\n","Epoch 1 (Step 002630): Train loss 6.075, Val loss 6.154\n","Epoch 1 (Step 002635): Train loss 6.143, Val loss 6.147\n","Epoch 1 (Step 002640): Train loss 6.012, Val loss 6.131\n","Epoch 1 (Step 002645): Train loss 6.007, Val loss 6.159\n","Epoch 1 (Step 002650): Train loss 6.010, Val loss 6.126\n","Epoch 1 (Step 002655): Train loss 6.010, Val loss 6.131\n","Epoch 1 (Step 002660): Train loss 6.023, Val loss 6.141\n","Epoch 1 (Step 002665): Train loss 5.957, Val loss 6.144\n","Epoch 1 (Step 002670): Train loss 5.989, Val loss 6.125\n","Epoch 1 (Step 002675): Train loss 5.997, Val loss 6.146\n","Epoch 1 (Step 002680): Train loss 5.987, Val loss 6.109\n","Epoch 1 (Step 002685): Train loss 5.809, Val loss 6.116\n","Epoch 1 (Step 002690): Train loss 6.116, Val loss 6.132\n","Epoch 1 (Step 002695): Train loss 6.017, Val loss 6.132\n","Epoch 1 (Step 002700): Train loss 6.000, Val loss 6.130\n","Epoch 1 (Step 002705): Train loss 5.951, Val loss 6.137\n","Epoch 1 (Step 002710): Train loss 5.909, Val loss 6.148\n","Epoch 1 (Step 002715): Train loss 5.856, Val loss 6.166\n","Epoch 1 (Step 002720): Train loss 6.092, Val loss 6.149\n","Epoch 1 (Step 002725): Train loss 5.994, Val loss 6.149\n","Epoch 1 (Step 002730): Train loss 6.013, Val loss 6.140\n","Epoch 1 (Step 002735): Train loss 6.021, Val loss 6.177\n","Epoch 1 (Step 002740): Train loss 6.071, Val loss 6.154\n","Epoch 1 (Step 002745): Train loss 6.067, Val loss 6.171\n","Epoch 1 (Step 002750): Train loss 5.970, Val loss 6.152\n","Epoch 1 (Step 002755): Train loss 6.110, Val loss 6.151\n","Epoch 1 (Step 002760): Train loss 6.025, Val loss 6.172\n","Epoch 1 (Step 002765): Train loss 5.945, Val loss 6.163\n","Epoch 1 (Step 002770): Train loss 6.112, Val loss 6.159\n","Epoch 1 (Step 002775): Train loss 6.033, Val loss 6.166\n","Epoch 1 (Step 002780): Train loss 6.085, Val loss 6.153\n","Epoch 1 (Step 002785): Train loss 6.035, Val loss 6.156\n","Epoch 1 (Step 002790): Train loss 5.906, Val loss 6.181\n","Epoch 1 (Step 002795): Train loss 5.851, Val loss 6.148\n","Epoch 1 (Step 002800): Train loss 6.021, Val loss 6.153\n","Epoch 1 (Step 002805): Train loss 6.217, Val loss 6.150\n","Epoch 1 (Step 002810): Train loss 5.979, Val loss 6.145\n","Epoch 1 (Step 002815): Train loss 6.098, Val loss 6.136\n","Epoch 1 (Step 002820): Train loss 5.986, Val loss 6.151\n","Epoch 1 (Step 002825): Train loss 6.002, Val loss 6.133\n","Epoch 1 (Step 002830): Train loss 5.969, Val loss 6.154\n","Epoch 1 (Step 002835): Train loss 6.035, Val loss 6.147\n","Epoch 1 (Step 002840): Train loss 6.101, Val loss 6.156\n","Epoch 1 (Step 002845): Train loss 5.888, Val loss 6.159\n","Epoch 1 (Step 002850): Train loss 5.971, Val loss 6.169\n","Epoch 1 (Step 002855): Train loss 5.906, Val loss 6.153\n","Epoch 1 (Step 002860): Train loss 6.062, Val loss 6.135\n","Epoch 1 (Step 002865): Train loss 5.957, Val loss 6.148\n","Epoch 1 (Step 002870): Train loss 6.030, Val loss 6.162\n","Epoch 1 (Step 002875): Train loss 6.031, Val loss 6.147\n","Epoch 1 (Step 002880): Train loss 6.011, Val loss 6.145\n","Epoch 1 (Step 002885): Train loss 6.028, Val loss 6.124\n","Epoch 1 (Step 002890): Train loss 6.023, Val loss 6.133\n","Epoch 1 (Step 002895): Train loss 5.868, Val loss 6.159\n","Epoch 1 (Step 002900): Train loss 5.928, Val loss 6.129\n","Epoch 1 (Step 002905): Train loss 6.034, Val loss 6.124\n","Epoch 1 (Step 002910): Train loss 6.131, Val loss 6.161\n","Epoch 1 (Step 002915): Train loss 6.095, Val loss 6.177\n","Epoch 1 (Step 002920): Train loss 6.149, Val loss 6.150\n","Epoch 1 (Step 002925): Train loss 6.056, Val loss 6.176\n","Epoch 1 (Step 002930): Train loss 5.921, Val loss 6.152\n","Epoch 1 (Step 002935): Train loss 5.988, Val loss 6.124\n","Epoch 1 (Step 002940): Train loss 6.075, Val loss 6.138\n","Epoch 1 (Step 002945): Train loss 6.108, Val loss 6.136\n","Epoch 1 (Step 002950): Train loss 5.986, Val loss 6.154\n","Epoch 1 (Step 002955): Train loss 6.104, Val loss 6.127\n","Epoch 1 (Step 002960): Train loss 5.914, Val loss 6.132\n","Epoch 1 (Step 002965): Train loss 6.040, Val loss 6.139\n","Epoch 1 (Step 002970): Train loss 5.977, Val loss 6.133\n","Epoch 1 (Step 002975): Train loss 6.036, Val loss 6.150\n","Epoch 1 (Step 002980): Train loss 5.971, Val loss 6.159\n","Epoch 1 (Step 002985): Train loss 6.059, Val loss 6.152\n","Epoch 1 (Step 002990): Train loss 5.959, Val loss 6.135\n","Epoch 1 (Step 002995): Train loss 5.852, Val loss 6.121\n","Epoch 1 (Step 003000): Train loss 5.978, Val loss 6.137\n","Epoch 1 (Step 003005): Train loss 5.927, Val loss 6.117\n","Epoch 1 (Step 003010): Train loss 5.975, Val loss 6.106\n","Epoch 1 (Step 003015): Train loss 6.008, Val loss 6.134\n","Epoch 1 (Step 003020): Train loss 6.051, Val loss 6.125\n","Epoch 1 (Step 003025): Train loss 5.900, Val loss 6.127\n","Epoch 1 (Step 003030): Train loss 5.844, Val loss 6.132\n","Epoch 1 (Step 003035): Train loss 6.016, Val loss 6.137\n","Epoch 1 (Step 003040): Train loss 5.946, Val loss 6.175\n","Epoch 1 (Step 003045): Train loss 5.962, Val loss 6.150\n","Epoch 1 (Step 003050): Train loss 6.030, Val loss 6.133\n","Epoch 1 (Step 003055): Train loss 6.005, Val loss 6.140\n","Epoch 1 (Step 003060): Train loss 5.825, Val loss 6.118\n","Epoch 1 (Step 003065): Train loss 6.029, Val loss 6.114\n","Epoch 1 (Step 003070): Train loss 5.958, Val loss 6.140\n","Epoch 1 (Step 003075): Train loss 5.982, Val loss 6.133\n","Epoch 1 (Step 003080): Train loss 6.062, Val loss 6.190\n","Epoch 1 (Step 003085): Train loss 6.039, Val loss 6.186\n","Epoch 1 (Step 003090): Train loss 5.895, Val loss 6.192\n","Epoch 1 (Step 003095): Train loss 6.002, Val loss 6.219\n","Epoch 1 (Step 003100): Train loss 5.990, Val loss 6.193\n","Epoch 1 (Step 003105): Train loss 6.052, Val loss 6.193\n","Epoch 1 (Step 003110): Train loss 6.176, Val loss 6.191\n","Epoch 1 (Step 003115): Train loss 6.088, Val loss 6.177\n","Epoch 1 (Step 003120): Train loss 6.008, Val loss 6.182\n","Epoch 1 (Step 003125): Train loss 6.031, Val loss 6.174\n","Epoch 1 (Step 003130): Train loss 5.977, Val loss 6.169\n","Epoch 1 (Step 003135): Train loss 6.119, Val loss 6.152\n","Epoch 1 (Step 003140): Train loss 6.048, Val loss 6.164\n","Epoch 1 (Step 003145): Train loss 6.003, Val loss 6.165\n","Epoch 1 (Step 003150): Train loss 6.016, Val loss 6.170\n","Epoch 1 (Step 003155): Train loss 6.044, Val loss 6.206\n","Epoch 1 (Step 003160): Train loss 5.936, Val loss 6.177\n","Epoch 1 (Step 003165): Train loss 5.993, Val loss 6.199\n","Epoch 1 (Step 003170): Train loss 6.010, Val loss 6.191\n","Epoch 1 (Step 003175): Train loss 6.042, Val loss 6.206\n","Epoch 1 (Step 003180): Train loss 5.973, Val loss 6.178\n","Epoch 1 (Step 003185): Train loss 5.996, Val loss 6.154\n","Epoch 1 (Step 003190): Train loss 5.982, Val loss 6.158\n","Epoch 1 (Step 003195): Train loss 5.933, Val loss 6.171\n","Epoch 1 (Step 003200): Train loss 6.044, Val loss 6.140\n","Epoch 1 (Step 003205): Train loss 6.001, Val loss 6.135\n","Epoch 1 (Step 003210): Train loss 6.191, Val loss 6.175\n","Epoch 1 (Step 003215): Train loss 5.835, Val loss 6.147\n","Epoch 1 (Step 003220): Train loss 6.004, Val loss 6.150\n","Epoch 1 (Step 003225): Train loss 5.894, Val loss 6.152\n","Epoch 1 (Step 003230): Train loss 6.033, Val loss 6.142\n","Epoch 1 (Step 003235): Train loss 5.935, Val loss 6.142\n","Epoch 1 (Step 003240): Train loss 5.972, Val loss 6.128\n","Epoch 1 (Step 003245): Train loss 5.897, Val loss 6.127\n","Epoch 1 (Step 003250): Train loss 5.970, Val loss 6.114\n","Epoch 1 (Step 003255): Train loss 6.035, Val loss 6.135\n","Epoch 1 (Step 003260): Train loss 5.883, Val loss 6.107\n","Epoch 1 (Step 003265): Train loss 5.973, Val loss 6.112\n","Epoch 1 (Step 003270): Train loss 6.097, Val loss 6.117\n","Epoch 1 (Step 003275): Train loss 5.864, Val loss 6.133\n","Epoch 1 (Step 003280): Train loss 5.920, Val loss 6.128\n","Epoch 1 (Step 003285): Train loss 5.996, Val loss 6.122\n","Epoch 1 (Step 003290): Train loss 5.990, Val loss 6.119\n","Epoch 1 (Step 003295): Train loss 5.972, Val loss 6.133\n","Epoch 1 (Step 003300): Train loss 5.954, Val loss 6.104\n","Epoch 1 (Step 003305): Train loss 6.023, Val loss 6.110\n","Epoch 1 (Step 003310): Train loss 5.894, Val loss 6.102\n","Epoch 1 (Step 003315): Train loss 5.878, Val loss 6.107\n","Epoch 1 (Step 003320): Train loss 5.897, Val loss 6.102\n","Epoch 1 (Step 003325): Train loss 5.936, Val loss 6.113\n","Epoch 1 (Step 003330): Train loss 5.928, Val loss 6.105\n","Epoch 1 (Step 003335): Train loss 5.965, Val loss 6.092\n","Epoch 1 (Step 003340): Train loss 5.949, Val loss 6.096\n","Epoch 1 (Step 003345): Train loss 6.026, Val loss 6.095\n","Epoch 1 (Step 003350): Train loss 5.918, Val loss 6.097\n","Epoch 1 (Step 003355): Train loss 5.963, Val loss 6.050\n","Epoch 1 (Step 003360): Train loss 5.995, Val loss 6.046\n","Epoch 1 (Step 003365): Train loss 5.862, Val loss 6.029\n","Epoch 1 (Step 003370): Train loss 5.969, Val loss 6.038\n","Epoch 1 (Step 003375): Train loss 6.064, Val loss 6.035\n","Epoch 1 (Step 003380): Train loss 5.997, Val loss 6.041\n","Epoch 1 (Step 003385): Train loss 6.098, Val loss 6.055\n","Epoch 1 (Step 003390): Train loss 5.883, Val loss 6.043\n","Epoch 1 (Step 003395): Train loss 5.922, Val loss 6.031\n","Epoch 1 (Step 003400): Train loss 5.874, Val loss 6.037\n","Epoch 1 (Step 003405): Train loss 5.776, Val loss 6.034\n","Epoch 1 (Step 003410): Train loss 5.850, Val loss 6.044\n","Epoch 1 (Step 003415): Train loss 5.954, Val loss 6.038\n","Epoch 1 (Step 003420): Train loss 6.031, Val loss 6.048\n","Epoch 1 (Step 003425): Train loss 5.916, Val loss 6.054\n","Epoch 1 (Step 003430): Train loss 5.859, Val loss 6.049\n","Epoch 1 (Step 003435): Train loss 5.802, Val loss 6.045\n","Epoch 1 (Step 003440): Train loss 5.965, Val loss 6.059\n","Epoch 1 (Step 003445): Train loss 5.913, Val loss 6.053\n","Epoch 1 (Step 003450): Train loss 6.005, Val loss 6.047\n","Epoch 1 (Step 003455): Train loss 5.739, Val loss 6.051\n","Epoch 1 (Step 003460): Train loss 5.978, Val loss 6.060\n","Epoch 1 (Step 003465): Train loss 6.089, Val loss 6.064\n","Epoch 1 (Step 003470): Train loss 5.942, Val loss 6.068\n","Epoch 1 (Step 003475): Train loss 5.967, Val loss 6.080\n","Epoch 1 (Step 003480): Train loss 5.855, Val loss 6.056\n","Epoch 1 (Step 003485): Train loss 5.918, Val loss 6.050\n","Epoch 1 (Step 003490): Train loss 5.957, Val loss 6.052\n","Epoch 1 (Step 003495): Train loss 5.968, Val loss 6.064\n","Epoch 1 (Step 003500): Train loss 5.878, Val loss 6.049\n","Epoch 1 (Step 003505): Train loss 5.881, Val loss 6.052\n","Epoch 1 (Step 003510): Train loss 6.000, Val loss 6.067\n","Epoch 1 (Step 003515): Train loss 5.985, Val loss 6.059\n","Epoch 1 (Step 003520): Train loss 5.852, Val loss 6.063\n","Epoch 1 (Step 003525): Train loss 5.974, Val loss 6.053\n","Epoch 1 (Step 003530): Train loss 6.055, Val loss 6.054\n","Epoch 1 (Step 003535): Train loss 5.859, Val loss 6.056\n","Epoch 1 (Step 003540): Train loss 5.927, Val loss 6.057\n","Epoch 1 (Step 003545): Train loss 5.934, Val loss 6.052\n","Epoch 1 (Step 003550): Train loss 5.812, Val loss 6.058\n","Epoch 1 (Step 003555): Train loss 5.947, Val loss 6.047\n","Epoch 1 (Step 003560): Train loss 5.797, Val loss 6.064\n","Epoch 1 (Step 003565): Train loss 5.886, Val loss 6.062\n","Epoch 1 (Step 003570): Train loss 6.005, Val loss 6.036\n","Epoch 1 (Step 003575): Train loss 5.887, Val loss 6.070\n","Epoch 1 (Step 003580): Train loss 6.147, Val loss 6.064\n","Epoch 1 (Step 003585): Train loss 5.916, Val loss 6.048\n","Epoch 1 (Step 003590): Train loss 5.867, Val loss 6.076\n","Epoch 1 (Step 003595): Train loss 5.997, Val loss 6.071\n","Epoch 1 (Step 003600): Train loss 5.935, Val loss 6.072\n","Epoch 1 (Step 003605): Train loss 5.877, Val loss 6.059\n","Epoch 1 (Step 003610): Train loss 6.015, Val loss 6.063\n","Epoch 1 (Step 003615): Train loss 5.759, Val loss 6.067\n","Epoch 1 (Step 003620): Train loss 6.094, Val loss 6.050\n","Epoch 1 (Step 003625): Train loss 5.861, Val loss 6.051\n","Epoch 1 (Step 003630): Train loss 5.993, Val loss 6.042\n","Epoch 1 (Step 003635): Train loss 5.932, Val loss 6.048\n","Epoch 1 (Step 003640): Train loss 5.800, Val loss 6.056\n","Epoch 1 (Step 003645): Train loss 5.867, Val loss 6.061\n","Epoch 1 (Step 003650): Train loss 5.932, Val loss 6.054\n","Epoch 1 (Step 003655): Train loss 5.955, Val loss 6.055\n","Epoch 1 (Step 003660): Train loss 5.800, Val loss 6.036\n","Epoch 1 (Step 003665): Train loss 5.850, Val loss 6.048\n","Epoch 1 (Step 003670): Train loss 5.767, Val loss 6.042\n","Epoch 1 (Step 003675): Train loss 5.910, Val loss 6.046\n","Epoch 1 (Step 003680): Train loss 5.715, Val loss 6.030\n","Epoch 1 (Step 003685): Train loss 5.837, Val loss 6.016\n","Epoch 1 (Step 003690): Train loss 5.782, Val loss 6.027\n","Epoch 1 (Step 003695): Train loss 5.834, Val loss 6.020\n","Epoch 1 (Step 003700): Train loss 5.943, Val loss 6.013\n","Epoch 1 (Step 003705): Train loss 5.844, Val loss 6.004\n","Epoch 1 (Step 003710): Train loss 5.959, Val loss 6.016\n","Epoch 1 (Step 003715): Train loss 5.855, Val loss 6.003\n","Epoch 1 (Step 003720): Train loss 5.882, Val loss 6.001\n","Epoch 1 (Step 003725): Train loss 5.840, Val loss 6.010\n","Epoch 1 (Step 003730): Train loss 5.931, Val loss 6.007\n","Epoch 1 (Step 003735): Train loss 5.827, Val loss 6.025\n","Epoch 1 (Step 003740): Train loss 5.881, Val loss 6.037\n","Epoch 1 (Step 003745): Train loss 5.825, Val loss 6.022\n","Epoch 1 (Step 003750): Train loss 5.928, Val loss 6.022\n","Epoch 1 (Step 003755): Train loss 5.827, Val loss 6.012\n","Epoch 1 (Step 003760): Train loss 5.828, Val loss 6.017\n","Epoch 1 (Step 003765): Train loss 5.711, Val loss 6.028\n","Epoch 1 (Step 003770): Train loss 5.778, Val loss 6.028\n","Epoch 1 (Step 003775): Train loss 5.907, Val loss 6.026\n","Epoch 1 (Step 003780): Train loss 5.954, Val loss 6.027\n","Epoch 1 (Step 003785): Train loss 5.833, Val loss 6.011\n","Epoch 1 (Step 003790): Train loss 5.929, Val loss 6.015\n","Epoch 1 (Step 003795): Train loss 5.830, Val loss 6.021\n","Epoch 1 (Step 003800): Train loss 5.791, Val loss 6.003\n","Epoch 1 (Step 003805): Train loss 5.799, Val loss 6.011\n","Epoch 1 (Step 003810): Train loss 5.810, Val loss 6.021\n","Epoch 1 (Step 003815): Train loss 5.853, Val loss 6.019\n","Epoch 1 (Step 003820): Train loss 5.899, Val loss 6.024\n","Epoch 1 (Step 003825): Train loss 5.847, Val loss 6.019\n","Epoch 1 (Step 003830): Train loss 5.938, Val loss 6.018\n","Epoch 1 (Step 003835): Train loss 5.844, Val loss 6.031\n","Epoch 1 (Step 003840): Train loss 5.907, Val loss 6.046\n","Epoch 1 (Step 003845): Train loss 5.921, Val loss 6.037\n","Epoch 1 (Step 003850): Train loss 5.938, Val loss 6.052\n","Epoch 1 (Step 003855): Train loss 5.823, Val loss 6.054\n","Epoch 1 (Step 003860): Train loss 5.772, Val loss 6.043\n","Epoch 1 (Step 003865): Train loss 5.822, Val loss 6.045\n","Epoch 1 (Step 003870): Train loss 5.782, Val loss 6.075\n","Epoch 1 (Step 003875): Train loss 5.783, Val loss 6.065\n","Epoch 1 (Step 003880): Train loss 5.987, Val loss 6.039\n","Epoch 1 (Step 003885): Train loss 5.951, Val loss 6.029\n","Epoch 1 (Step 003890): Train loss 5.833, Val loss 6.038\n","Epoch 1 (Step 003895): Train loss 5.966, Val loss 6.053\n","Epoch 1 (Step 003900): Train loss 5.844, Val loss 6.034\n","Epoch 1 (Step 003905): Train loss 5.739, Val loss 6.035\n","Epoch 1 (Step 003910): Train loss 5.791, Val loss 6.046\n","Epoch 1 (Step 003915): Train loss 5.805, Val loss 6.042\n","Epoch 1 (Step 003920): Train loss 5.961, Val loss 6.037\n","Epoch 1 (Step 003925): Train loss 5.784, Val loss 6.043\n","Epoch 1 (Step 003930): Train loss 5.761, Val loss 6.021\n","Epoch 1 (Step 003935): Train loss 5.915, Val loss 6.024\n","Epoch 1 (Step 003940): Train loss 5.849, Val loss 6.039\n","Epoch 1 (Step 003945): Train loss 5.833, Val loss 6.031\n","Epoch 1 (Step 003950): Train loss 5.841, Val loss 6.023\n","Epoch 1 (Step 003955): Train loss 5.828, Val loss 6.013\n","Epoch 1 (Step 003960): Train loss 5.854, Val loss 6.012\n","Epoch 1 (Step 003965): Train loss 5.849, Val loss 5.999\n","Epoch 1 (Step 003970): Train loss 5.861, Val loss 6.002\n","Epoch 1 (Step 003975): Train loss 5.752, Val loss 6.011\n","Epoch 1 (Step 003980): Train loss 5.907, Val loss 6.006\n","Epoch 1 (Step 003985): Train loss 5.965, Val loss 5.981\n","Epoch 1 (Step 003990): Train loss 5.963, Val loss 5.991\n","Epoch 1 (Step 003995): Train loss 5.982, Val loss 5.991\n","Epoch 1 (Step 004000): Train loss 5.865, Val loss 5.984\n","Epoch 1 (Step 004005): Train loss 5.693, Val loss 5.984\n","Epoch 1 (Step 004010): Train loss 5.892, Val loss 6.023\n","Epoch 1 (Step 004015): Train loss 5.865, Val loss 5.979\n","Epoch 1 (Step 004020): Train loss 5.826, Val loss 5.991\n","Epoch 1 (Step 004025): Train loss 5.943, Val loss 6.018\n","Epoch 1 (Step 004030): Train loss 5.781, Val loss 6.005\n","Epoch 1 (Step 004035): Train loss 5.821, Val loss 5.998\n","Epoch 1 (Step 004040): Train loss 5.865, Val loss 5.997\n","Epoch 1 (Step 004045): Train loss 5.586, Val loss 5.997\n","Epoch 1 (Step 004050): Train loss 5.866, Val loss 6.004\n","Epoch 1 (Step 004055): Train loss 5.789, Val loss 5.986\n","Epoch 1 (Step 004060): Train loss 5.788, Val loss 5.996\n","Epoch 1 (Step 004065): Train loss 5.732, Val loss 6.020\n","Epoch 1 (Step 004070): Train loss 5.890, Val loss 6.021\n","Epoch 1 (Step 004075): Train loss 5.888, Val loss 6.012\n","Epoch 1 (Step 004080): Train loss 5.747, Val loss 6.020\n","Epoch 1 (Step 004085): Train loss 5.768, Val loss 6.001\n","Epoch 1 (Step 004090): Train loss 5.779, Val loss 6.002\n","Epoch 1 (Step 004095): Train loss 5.853, Val loss 6.012\n","Epoch 1 (Step 004100): Train loss 5.741, Val loss 6.009\n","Epoch 1 (Step 004105): Train loss 5.902, Val loss 5.995\n","Epoch 1 (Step 004110): Train loss 5.668, Val loss 6.003\n","Epoch 1 (Step 004115): Train loss 5.885, Val loss 5.999\n","Epoch 1 (Step 004120): Train loss 5.761, Val loss 5.999\n","Epoch 1 (Step 004125): Train loss 5.813, Val loss 5.986\n","Epoch 1 (Step 004130): Train loss 5.671, Val loss 5.978\n","Epoch 1 (Step 004135): Train loss 5.829, Val loss 5.981\n","Epoch 1 (Step 004140): Train loss 5.738, Val loss 5.978\n","Epoch 1 (Step 004145): Train loss 5.922, Val loss 5.980\n","Epoch 1 (Step 004150): Train loss 5.863, Val loss 5.981\n","Epoch 1 (Step 004155): Train loss 5.729, Val loss 5.996\n","Epoch 1 (Step 004160): Train loss 5.884, Val loss 5.999\n","Epoch 1 (Step 004165): Train loss 5.766, Val loss 5.995\n","Epoch 1 (Step 004170): Train loss 5.851, Val loss 5.983\n","Epoch 1 (Step 004175): Train loss 5.498, Val loss 6.007\n","Epoch 1 (Step 004180): Train loss 5.872, Val loss 6.010\n","Epoch 1 (Step 004185): Train loss 5.748, Val loss 6.004\n","Epoch 1 (Step 004190): Train loss 5.737, Val loss 6.003\n","Epoch 1 (Step 004195): Train loss 5.856, Val loss 5.977\n","Epoch 1 (Step 004200): Train loss 5.636, Val loss 5.982\n","Epoch 1 (Step 004205): Train loss 5.785, Val loss 5.985\n","Epoch 1 (Step 004210): Train loss 5.818, Val loss 5.989\n","Epoch 1 (Step 004215): Train loss 5.835, Val loss 5.986\n","Epoch 1 (Step 004220): Train loss 5.750, Val loss 5.971\n","Epoch 1 (Step 004225): Train loss 5.676, Val loss 5.980\n","Epoch 1 (Step 004230): Train loss 5.775, Val loss 5.967\n","Epoch 1 (Step 004235): Train loss 5.639, Val loss 5.972\n","Epoch 1 (Step 004240): Train loss 5.643, Val loss 5.963\n","Epoch 1 (Step 004245): Train loss 5.786, Val loss 5.968\n","Epoch 1 (Step 004250): Train loss 5.930, Val loss 5.958\n","Epoch 1 (Step 004255): Train loss 5.687, Val loss 5.961\n","Epoch 1 (Step 004260): Train loss 5.806, Val loss 5.987\n","Epoch 1 (Step 004265): Train loss 5.820, Val loss 5.997\n","Epoch 1 (Step 004270): Train loss 5.763, Val loss 5.985\n","Epoch 1 (Step 004275): Train loss 5.852, Val loss 5.975\n","Epoch 1 (Step 004280): Train loss 5.863, Val loss 5.974\n","Epoch 1 (Step 004285): Train loss 5.711, Val loss 5.972\n","Epoch 1 (Step 004290): Train loss 5.660, Val loss 5.974\n","Epoch 1 (Step 004295): Train loss 5.632, Val loss 5.981\n","Epoch 1 (Step 004300): Train loss 5.652, Val loss 5.975\n","Epoch 1 (Step 004305): Train loss 5.812, Val loss 5.974\n","Epoch 1 (Step 004310): Train loss 5.907, Val loss 5.968\n","Epoch 1 (Step 004315): Train loss 5.812, Val loss 5.961\n","Epoch 1 (Step 004320): Train loss 5.852, Val loss 5.956\n","Epoch 1 (Step 004325): Train loss 5.825, Val loss 5.968\n","Epoch 1 (Step 004330): Train loss 5.934, Val loss 5.967\n","Epoch 1 (Step 004335): Train loss 5.842, Val loss 5.957\n","Epoch 1 (Step 004340): Train loss 5.724, Val loss 5.953\n","Epoch 1 (Step 004345): Train loss 5.735, Val loss 5.952\n","Epoch 1 (Step 004350): Train loss 5.760, Val loss 5.945\n","Epoch 1 (Step 004355): Train loss 5.833, Val loss 5.943\n","Epoch 1 (Step 004360): Train loss 5.726, Val loss 5.945\n","Epoch 1 (Step 004365): Train loss 5.713, Val loss 5.936\n","Epoch 1 (Step 004370): Train loss 5.637, Val loss 5.953\n","Epoch 1 (Step 004375): Train loss 5.778, Val loss 5.945\n","Epoch 1 (Step 004380): Train loss 5.824, Val loss 5.940\n","Epoch 1 (Step 004385): Train loss 5.823, Val loss 5.933\n","Epoch 1 (Step 004390): Train loss 5.816, Val loss 5.924\n","Epoch 1 (Step 004395): Train loss 5.742, Val loss 5.933\n","Epoch 1 (Step 004400): Train loss 5.671, Val loss 5.931\n","Epoch 1 (Step 004405): Train loss 5.733, Val loss 5.941\n","Epoch 1 (Step 004410): Train loss 5.627, Val loss 5.932\n","Epoch 1 (Step 004415): Train loss 5.769, Val loss 5.928\n","Epoch 1 (Step 004420): Train loss 5.798, Val loss 5.938\n","Epoch 1 (Step 004425): Train loss 5.717, Val loss 5.935\n","Epoch 1 (Step 004430): Train loss 5.798, Val loss 5.950\n","Epoch 1 (Step 004435): Train loss 5.784, Val loss 5.947\n","Epoch 1 (Step 004440): Train loss 5.702, Val loss 5.941\n","Epoch 1 (Step 004445): Train loss 5.752, Val loss 5.955\n","Epoch 1 (Step 004450): Train loss 5.803, Val loss 5.946\n","Epoch 1 (Step 004455): Train loss 5.850, Val loss 5.965\n","Epoch 1 (Step 004460): Train loss 5.724, Val loss 5.940\n","Epoch 1 (Step 004465): Train loss 5.859, Val loss 5.937\n","Epoch 1 (Step 004470): Train loss 5.739, Val loss 5.944\n","Epoch 1 (Step 004475): Train loss 5.807, Val loss 5.938\n","Epoch 1 (Step 004480): Train loss 5.749, Val loss 5.940\n","Epoch 1 (Step 004485): Train loss 5.793, Val loss 5.934\n","Epoch 1 (Step 004490): Train loss 5.756, Val loss 5.920\n","Epoch 1 (Step 004495): Train loss 5.872, Val loss 5.936\n","Epoch 1 (Step 004500): Train loss 5.731, Val loss 5.942\n","Epoch 1 (Step 004505): Train loss 5.807, Val loss 5.918\n","Epoch 1 (Step 004510): Train loss 5.706, Val loss 5.923\n","Epoch 1 (Step 004515): Train loss 5.795, Val loss 5.915\n","Epoch 1 (Step 004520): Train loss 5.646, Val loss 5.936\n","Epoch 1 (Step 004525): Train loss 5.856, Val loss 5.942\n","Epoch 1 (Step 004530): Train loss 5.649, Val loss 5.939\n","Epoch 1 (Step 004535): Train loss 5.787, Val loss 5.935\n","Epoch 1 (Step 004540): Train loss 5.657, Val loss 5.946\n","Epoch 1 (Step 004545): Train loss 5.731, Val loss 5.939\n","Epoch 1 (Step 004550): Train loss 5.622, Val loss 5.924\n","Epoch 1 (Step 004555): Train loss 5.706, Val loss 5.929\n","Epoch 1 (Step 004560): Train loss 5.720, Val loss 5.939\n","Epoch 1 (Step 004565): Train loss 5.841, Val loss 5.948\n","Epoch 1 (Step 004570): Train loss 5.915, Val loss 5.937\n","Epoch 1 (Step 004575): Train loss 5.691, Val loss 5.942\n","Epoch 1 (Step 004580): Train loss 5.727, Val loss 5.947\n","Epoch 1 (Step 004585): Train loss 5.696, Val loss 5.924\n","Epoch 1 (Step 004590): Train loss 5.844, Val loss 5.920\n","Epoch 1 (Step 004595): Train loss 5.701, Val loss 5.916\n","Epoch 1 (Step 004600): Train loss 5.762, Val loss 5.903\n","Epoch 1 (Step 004605): Train loss 5.634, Val loss 5.912\n","Epoch 1 (Step 004610): Train loss 5.629, Val loss 5.911\n","Epoch 1 (Step 004615): Train loss 5.677, Val loss 5.916\n","Epoch 1 (Step 004620): Train loss 5.642, Val loss 5.940\n","Epoch 1 (Step 004625): Train loss 5.682, Val loss 5.937\n","Epoch 1 (Step 004630): Train loss 5.804, Val loss 5.927\n","Epoch 1 (Step 004635): Train loss 5.740, Val loss 5.936\n","Epoch 1 (Step 004640): Train loss 5.736, Val loss 5.956\n","Epoch 1 (Step 004645): Train loss 5.765, Val loss 5.944\n","Epoch 1 (Step 004650): Train loss 5.828, Val loss 5.937\n","Epoch 1 (Step 004655): Train loss 5.760, Val loss 5.947\n","Epoch 1 (Step 004660): Train loss 5.631, Val loss 5.925\n","Epoch 1 (Step 004665): Train loss 5.749, Val loss 5.925\n","Epoch 1 (Step 004670): Train loss 5.678, Val loss 5.933\n","Epoch 1 (Step 004675): Train loss 5.566, Val loss 5.926\n","Epoch 1 (Step 004680): Train loss 5.781, Val loss 5.927\n","Epoch 1 (Step 004685): Train loss 5.822, Val loss 5.908\n","Epoch 1 (Step 004690): Train loss 5.856, Val loss 5.915\n","Epoch 1 (Step 004695): Train loss 5.599, Val loss 5.908\n","Epoch 1 (Step 004700): Train loss 5.687, Val loss 5.905\n","Epoch 1 (Step 004705): Train loss 5.734, Val loss 5.902\n","Epoch 1 (Step 004710): Train loss 5.672, Val loss 5.897\n","Epoch 1 (Step 004715): Train loss 5.689, Val loss 5.906\n","Epoch 1 (Step 004720): Train loss 5.542, Val loss 5.914\n","Epoch 1 (Step 004725): Train loss 5.750, Val loss 5.893\n","Epoch 1 (Step 004730): Train loss 5.767, Val loss 5.899\n","Epoch 1 (Step 004735): Train loss 5.709, Val loss 5.900\n","Epoch 1 (Step 004740): Train loss 5.702, Val loss 5.891\n","Epoch 1 (Step 004745): Train loss 5.737, Val loss 5.889\n","Epoch 1 (Step 004750): Train loss 5.497, Val loss 5.881\n","Epoch 1 (Step 004755): Train loss 5.689, Val loss 5.888\n","Epoch 1 (Step 004760): Train loss 5.756, Val loss 5.880\n","Epoch 1 (Step 004765): Train loss 5.740, Val loss 5.902\n","Epoch 1 (Step 004770): Train loss 5.770, Val loss 5.903\n","Epoch 1 (Step 004775): Train loss 5.764, Val loss 5.882\n","Epoch 1 (Step 004780): Train loss 5.691, Val loss 5.897\n","Epoch 1 (Step 004785): Train loss 5.763, Val loss 5.888\n","Epoch 1 (Step 004790): Train loss 5.733, Val loss 5.886\n","Epoch 1 (Step 004795): Train loss 5.759, Val loss 5.890\n","Epoch 1 (Step 004800): Train loss 5.786, Val loss 5.877\n","Epoch 1 (Step 004805): Train loss 5.652, Val loss 5.879\n","Epoch 1 (Step 004810): Train loss 5.759, Val loss 5.889\n","Epoch 1 (Step 004815): Train loss 5.747, Val loss 5.893\n","Epoch 1 (Step 004820): Train loss 5.642, Val loss 5.900\n","Epoch 1 (Step 004825): Train loss 5.707, Val loss 5.883\n","Epoch 1 (Step 004830): Train loss 5.703, Val loss 5.883\n","Epoch 1 (Step 004835): Train loss 5.810, Val loss 5.873\n","Epoch 1 (Step 004840): Train loss 5.561, Val loss 5.856\n","Epoch 1 (Step 004845): Train loss 5.679, Val loss 5.854\n","Epoch 1 (Step 004850): Train loss 5.694, Val loss 5.857\n","Epoch 1 (Step 004855): Train loss 5.607, Val loss 5.853\n","Epoch 1 (Step 004860): Train loss 5.652, Val loss 5.868\n","Epoch 1 (Step 004865): Train loss 5.681, Val loss 5.855\n","Epoch 1 (Step 004870): Train loss 5.774, Val loss 5.859\n","Epoch 1 (Step 004875): Train loss 5.638, Val loss 5.873\n","Epoch 1 (Step 004880): Train loss 5.767, Val loss 5.880\n","Epoch 1 (Step 004885): Train loss 5.676, Val loss 5.879\n","Epoch 1 (Step 004890): Train loss 5.707, Val loss 5.864\n","Epoch 1 (Step 004895): Train loss 5.675, Val loss 5.852\n","Epoch 1 (Step 004900): Train loss 5.724, Val loss 5.874\n","Epoch 1 (Step 004905): Train loss 5.616, Val loss 5.867\n","Epoch 1 (Step 004910): Train loss 5.776, Val loss 5.850\n","Epoch 1 (Step 004915): Train loss 5.745, Val loss 5.839\n","Epoch 1 (Step 004920): Train loss 5.649, Val loss 5.827\n","Epoch 1 (Step 004925): Train loss 5.621, Val loss 5.811\n","Epoch 1 (Step 004930): Train loss 5.719, Val loss 5.805\n","Epoch 1 (Step 004935): Train loss 5.693, Val loss 5.805\n","Epoch 1 (Step 004940): Train loss 5.807, Val loss 5.817\n","Epoch 1 (Step 004945): Train loss 5.724, Val loss 5.819\n","Epoch 1 (Step 004950): Train loss 5.759, Val loss 5.825\n","Epoch 1 (Step 004955): Train loss 5.548, Val loss 5.835\n","Epoch 1 (Step 004960): Train loss 5.628, Val loss 5.826\n","Epoch 1 (Step 004965): Train loss 5.641, Val loss 5.829\n","Epoch 1 (Step 004970): Train loss 5.735, Val loss 5.843\n","Epoch 1 (Step 004975): Train loss 5.756, Val loss 5.837\n","Epoch 1 (Step 004980): Train loss 5.530, Val loss 5.840\n","Epoch 1 (Step 004985): Train loss 5.679, Val loss 5.833\n","Epoch 1 (Step 004990): Train loss 5.728, Val loss 5.827\n","Epoch 1 (Step 004995): Train loss 5.607, Val loss 5.807\n","Epoch 1 (Step 005000): Train loss 5.684, Val loss 5.808\n","Epoch 1 (Step 005005): Train loss 5.534, Val loss 5.833\n","Epoch 1 (Step 005010): Train loss 5.370, Val loss 5.812\n","Epoch 1 (Step 005015): Train loss 5.572, Val loss 5.801\n","Epoch 1 (Step 005020): Train loss 5.710, Val loss 5.807\n","Epoch 1 (Step 005025): Train loss 5.539, Val loss 5.822\n","Epoch 1 (Step 005030): Train loss 5.612, Val loss 5.829\n","Epoch 1 (Step 005035): Train loss 5.648, Val loss 5.832\n","Epoch 1 (Step 005040): Train loss 5.784, Val loss 5.811\n","Epoch 1 (Step 005045): Train loss 5.557, Val loss 5.844\n","Epoch 1 (Step 005050): Train loss 5.744, Val loss 5.833\n","Epoch 1 (Step 005055): Train loss 5.534, Val loss 5.822\n","Epoch 1 (Step 005060): Train loss 5.573, Val loss 5.812\n","Epoch 1 (Step 005065): Train loss 5.683, Val loss 5.819\n","Epoch 1 (Step 005070): Train loss 5.669, Val loss 5.827\n","Epoch 1 (Step 005075): Train loss 5.564, Val loss 5.839\n","Epoch 1 (Step 005080): Train loss 5.706, Val loss 5.829\n","Epoch 1 (Step 005085): Train loss 5.746, Val loss 5.826\n","Epoch 1 (Step 005090): Train loss 5.554, Val loss 5.824\n","Epoch 1 (Step 005095): Train loss 5.635, Val loss 5.827\n","Epoch 1 (Step 005100): Train loss 5.511, Val loss 5.829\n","Epoch 1 (Step 005105): Train loss 5.737, Val loss 5.838\n","Epoch 1 (Step 005110): Train loss 5.614, Val loss 5.836\n","Epoch 1 (Step 005115): Train loss 5.710, Val loss 5.831\n","Epoch 1 (Step 005120): Train loss 5.406, Val loss 5.839\n","Epoch 1 (Step 005125): Train loss 5.564, Val loss 5.842\n","Epoch 1 (Step 005130): Train loss 5.632, Val loss 5.825\n","Epoch 1 (Step 005135): Train loss 5.564, Val loss 5.815\n","Epoch 1 (Step 005140): Train loss 5.603, Val loss 5.814\n","Epoch 1 (Step 005145): Train loss 5.616, Val loss 5.832\n","Epoch 1 (Step 005150): Train loss 5.664, Val loss 5.833\n","Epoch 1 (Step 005155): Train loss 5.557, Val loss 5.826\n","Epoch 1 (Step 005160): Train loss 5.688, Val loss 5.812\n","Epoch 1 (Step 005165): Train loss 5.655, Val loss 5.797\n","Epoch 1 (Step 005170): Train loss 5.564, Val loss 5.827\n","Epoch 1 (Step 005175): Train loss 5.622, Val loss 5.821\n","Epoch 1 (Step 005180): Train loss 5.588, Val loss 5.816\n","Epoch 1 (Step 005185): Train loss 5.661, Val loss 5.817\n","Epoch 1 (Step 005190): Train loss 5.758, Val loss 5.814\n","Epoch 1 (Step 005195): Train loss 5.491, Val loss 5.802\n","Epoch 1 (Step 005200): Train loss 5.665, Val loss 5.787\n","Epoch 1 (Step 005205): Train loss 5.605, Val loss 5.811\n","Epoch 1 (Step 005210): Train loss 5.638, Val loss 5.815\n","Epoch 1 (Step 005215): Train loss 5.662, Val loss 5.799\n","Epoch 1 (Step 005220): Train loss 5.497, Val loss 5.792\n","Epoch 1 (Step 005225): Train loss 5.576, Val loss 5.799\n","Epoch 1 (Step 005230): Train loss 5.596, Val loss 5.820\n","Epoch 1 (Step 005235): Train loss 5.670, Val loss 5.817\n","Epoch 1 (Step 005240): Train loss 5.592, Val loss 5.799\n","Epoch 1 (Step 005245): Train loss 5.550, Val loss 5.802\n","Epoch 1 (Step 005250): Train loss 5.596, Val loss 5.819\n","Epoch 1 (Step 005255): Train loss 5.660, Val loss 5.805\n","Epoch 1 (Step 005260): Train loss 5.485, Val loss 5.814\n","Epoch 1 (Step 005265): Train loss 5.688, Val loss 5.824\n","Epoch 1 (Step 005270): Train loss 5.613, Val loss 5.806\n","Epoch 1 (Step 005275): Train loss 5.715, Val loss 5.812\n","Epoch 1 (Step 005280): Train loss 5.601, Val loss 5.791\n","Epoch 1 (Step 005285): Train loss 5.522, Val loss 5.795\n","Epoch 1 (Step 005290): Train loss 5.690, Val loss 5.802\n","Epoch 1 (Step 005295): Train loss 5.614, Val loss 5.795\n","Epoch 1 (Step 005300): Train loss 5.438, Val loss 5.796\n","Epoch 1 (Step 005305): Train loss 5.647, Val loss 5.820\n","Epoch 1 (Step 005310): Train loss 5.546, Val loss 5.811\n","Epoch 1 (Step 005315): Train loss 5.624, Val loss 5.806\n","Epoch 1 (Step 005320): Train loss 5.616, Val loss 5.798\n","Epoch 1 (Step 005325): Train loss 5.612, Val loss 5.808\n","Epoch 1 (Step 005330): Train loss 5.423, Val loss 5.811\n","Epoch 1 (Step 005335): Train loss 5.580, Val loss 5.803\n","Epoch 1 (Step 005340): Train loss 5.447, Val loss 5.818\n","Epoch 1 (Step 005345): Train loss 5.553, Val loss 5.805\n","Epoch 1 (Step 005350): Train loss 5.555, Val loss 5.787\n","Epoch 1 (Step 005355): Train loss 5.455, Val loss 5.786\n","Epoch 1 (Step 005360): Train loss 5.504, Val loss 5.812\n","Epoch 1 (Step 005365): Train loss 5.533, Val loss 5.788\n","Epoch 1 (Step 005370): Train loss 5.503, Val loss 5.783\n","Epoch 1 (Step 005375): Train loss 5.709, Val loss 5.788\n","Epoch 1 (Step 005380): Train loss 5.480, Val loss 5.790\n","Epoch 1 (Step 005385): Train loss 5.578, Val loss 5.793\n","Epoch 1 (Step 005390): Train loss 5.488, Val loss 5.790\n","Epoch 1 (Step 005395): Train loss 5.523, Val loss 5.798\n","Epoch 1 (Step 005400): Train loss 5.628, Val loss 5.791\n","Epoch 1 (Step 005405): Train loss 5.642, Val loss 5.792\n","Epoch 1 (Step 005410): Train loss 5.572, Val loss 5.782\n","Epoch 1 (Step 005415): Train loss 5.483, Val loss 5.791\n","Epoch 1 (Step 005420): Train loss 5.621, Val loss 5.803\n","Epoch 1 (Step 005425): Train loss 5.484, Val loss 5.797\n","Epoch 1 (Step 005430): Train loss 5.581, Val loss 5.787\n","Epoch 1 (Step 005435): Train loss 5.693, Val loss 5.797\n","Epoch 1 (Step 005440): Train loss 5.392, Val loss 5.799\n","Epoch 1 (Step 005445): Train loss 5.467, Val loss 5.799\n","Epoch 1 (Step 005450): Train loss 5.546, Val loss 5.787\n","Epoch 1 (Step 005455): Train loss 5.476, Val loss 5.797\n","Epoch 1 (Step 005460): Train loss 5.576, Val loss 5.800\n","Epoch 1 (Step 005465): Train loss 5.381, Val loss 5.803\n","Epoch 1 (Step 005470): Train loss 5.605, Val loss 5.792\n","Epoch 1 (Step 005475): Train loss 5.462, Val loss 5.791\n","Epoch 1 (Step 005480): Train loss 5.482, Val loss 5.790\n","Epoch 1 (Step 005485): Train loss 5.640, Val loss 5.784\n","Epoch 1 (Step 005490): Train loss 5.613, Val loss 5.787\n","Epoch 1 (Step 005495): Train loss 5.568, Val loss 5.782\n","Epoch 1 (Step 005500): Train loss 5.493, Val loss 5.786\n","Epoch 1 (Step 005505): Train loss 5.520, Val loss 5.846\n","Epoch 1 (Step 005510): Train loss 5.623, Val loss 5.792\n","Epoch 1 (Step 005515): Train loss 5.520, Val loss 5.814\n","Epoch 1 (Step 005520): Train loss 5.472, Val loss 5.803\n","Epoch 1 (Step 005525): Train loss 5.472, Val loss 5.783\n","Epoch 1 (Step 005530): Train loss 5.545, Val loss 5.808\n","Epoch 1 (Step 005535): Train loss 5.646, Val loss 5.790\n","Epoch 1 (Step 005540): Train loss 5.623, Val loss 5.777\n","Epoch 1 (Step 005545): Train loss 5.668, Val loss 5.799\n","Epoch 1 (Step 005550): Train loss 5.569, Val loss 5.785\n","Epoch 1 (Step 005555): Train loss 5.579, Val loss 5.789\n","Epoch 1 (Step 005560): Train loss 5.609, Val loss 5.797\n","Epoch 1 (Step 005565): Train loss 5.547, Val loss 5.770\n","Epoch 1 (Step 005570): Train loss 5.613, Val loss 5.769\n","Epoch 1 (Step 005575): Train loss 5.444, Val loss 5.774\n","Epoch 1 (Step 005580): Train loss 5.640, Val loss 5.767\n","Epoch 1 (Step 005585): Train loss 5.504, Val loss 5.764\n","Epoch 1 (Step 005590): Train loss 5.617, Val loss 5.802\n","Epoch 1 (Step 005595): Train loss 5.631, Val loss 5.783\n","Epoch 1 (Step 005600): Train loss 5.538, Val loss 5.779\n","Epoch 1 (Step 005605): Train loss 5.661, Val loss 5.783\n","Epoch 1 (Step 005610): Train loss 5.511, Val loss 5.770\n","Epoch 1 (Step 005615): Train loss 5.534, Val loss 5.772\n","Epoch 1 (Step 005620): Train loss 5.481, Val loss 5.765\n","Epoch 1 (Step 005625): Train loss 5.596, Val loss 5.762\n","Epoch 1 (Step 005630): Train loss 5.535, Val loss 5.773\n","Epoch 1 (Step 005635): Train loss 5.528, Val loss 5.782\n","Epoch 1 (Step 005640): Train loss 5.533, Val loss 5.768\n","Epoch 1 (Step 005645): Train loss 5.416, Val loss 5.753\n","Epoch 1 (Step 005650): Train loss 5.528, Val loss 5.762\n","Epoch 1 (Step 005655): Train loss 5.523, Val loss 5.779\n","Epoch 1 (Step 005660): Train loss 5.486, Val loss 5.775\n","Epoch 1 (Step 005665): Train loss 5.607, Val loss 5.769\n","Epoch 1 (Step 005670): Train loss 5.602, Val loss 5.758\n","Epoch 1 (Step 005675): Train loss 5.484, Val loss 5.745\n","Epoch 1 (Step 005680): Train loss 5.550, Val loss 5.730\n","Epoch 1 (Step 005685): Train loss 5.546, Val loss 5.721\n","Epoch 1 (Step 005690): Train loss 5.437, Val loss 5.729\n","Epoch 1 (Step 005695): Train loss 5.523, Val loss 5.723\n","Epoch 1 (Step 005700): Train loss 5.439, Val loss 5.729\n","Epoch 1 (Step 005705): Train loss 5.701, Val loss 5.727\n","Epoch 1 (Step 005710): Train loss 5.549, Val loss 5.726\n","Epoch 1 (Step 005715): Train loss 5.447, Val loss 5.712\n","Epoch 1 (Step 005720): Train loss 5.605, Val loss 5.720\n","Epoch 1 (Step 005725): Train loss 5.533, Val loss 5.713\n","Epoch 1 (Step 005730): Train loss 5.481, Val loss 5.683\n","Epoch 1 (Step 005735): Train loss 5.523, Val loss 5.692\n","Epoch 1 (Step 005740): Train loss 5.490, Val loss 5.700\n","Epoch 1 (Step 005745): Train loss 5.372, Val loss 5.701\n","Epoch 1 (Step 005750): Train loss 5.488, Val loss 5.700\n","Epoch 1 (Step 005755): Train loss 5.537, Val loss 5.711\n","Epoch 1 (Step 005760): Train loss 5.455, Val loss 5.696\n","Epoch 1 (Step 005765): Train loss 5.411, Val loss 5.713\n","Every effort moves you have been a new team.  The city has a city's city's city's city's city's city's city's city's city's city's city's city's city's city's city's city's city's city's city's\n","Training completed in 221.45 minutes.\n"]}]},{"cell_type":"code","source":["# Save checkpoint after 1 epoch\n","save_checkpoint(epoch=0, model=model, optimizer=optimizer)"],"metadata":{"id":"ux6Z146osqPe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training and validation set loss"],"metadata":{"id":"ucuO-WnyrNYK"}}]}