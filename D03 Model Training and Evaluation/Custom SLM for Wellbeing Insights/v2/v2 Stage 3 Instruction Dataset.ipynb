{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Creating Dataset"],"metadata":{"id":"JHoYOPBiaG2U"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xdMbVRl5WT-1","executionInfo":{"status":"ok","timestamp":1743833498107,"user_tz":-330,"elapsed":740,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"78e57bdc-cdd7-4f8b-c0db-0ac521dc4946"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n","Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"]}],"source":["!pip install google-generativeai"]},{"cell_type":"code","source":["import os\n","import time\n","import json\n","import pandas as pd\n","import google.generativeai as genai\n","\n","# Configure Gemini API\n","genai.configure(api_key=\"AIzaSyAHX6Zl-x5iNQQnGnWtjLxYJ6VTdkq0Zfo\")\n","generation_config = {\n","    \"temperature\": 1,\n","    \"top_p\": 0.95,\n","    \"top_k\": 40,\n","    \"max_output_tokens\": 8192,\n","    \"response_mime_type\": \"text/plain\",\n","}\n","gemini_model = genai.GenerativeModel(model_name=\"gemini-2.0-flash\", generation_config=generation_config)\n","\n","# Paths\n","csv_path = \"pmh_part_1.csv\"\n","json_path = \"mental_health_dataset17.json\"\n","\n","# Check if JSON file exists and load existing data\n","if os.path.exists(json_path):\n","    with open(json_path, \"r\") as f:\n","        dataset = json.load(f)\n","else:\n","    dataset = []\n","\n","# Read CSV\n","df = pd.read_csv(csv_path)\n","\n","# Process first 10 records for testing\n","# df_sample = df.head(10)\n","df_sample = df[1601:1700]\n","\n","# Change this line to process the entire dataset\n","# df_sample = df\n","\n","def generate_insight(text, issue):\n","    # Construct prompt\n","    prompt = f\"Analyze the following mental health issue: {issue}\\nText: {text}\\nProvide wellbeing insights based on the Ryff Scale of Psychological Wellbeing (Autonomy, Environmental Mastery, Personal Growth, Positive Relations, Purpose in Life, Self-Acceptance). The response should be of 1024 characters or less covering practical advice for all the 6 paramters\"\n","\n","    # Generate response\n","    response = gemini_model.generate_content([prompt])\n","    return response.text.strip()\n","\n","def create_json_dataset():\n","    for index, row in df_sample.iterrows():\n","        text = row.get('text', '')\n","        issue = row.get('mental_health_issue', '')\n","\n","        # Skip if entry already exists in JSON\n","        if any(entry['text'] == text and entry['mental_health_issue'] == issue for entry in dataset):\n","            print(f\"Skipping record {index + 1}: Already present in JSON\")\n","            continue\n","\n","        try:\n","            # Generate wellbeing insight\n","            insight = generate_insight(text, issue)\n","\n","            print(\"----------------------------------\\n\")\n","            print(f\"Processed record {index + 1}:\")\n","            print(f\"Text: {text}\")\n","            print(f\"Mental Issue: {issue}\")\n","            print(f\"Wellbeing Insight: {insight}\")\n","            print(\"----------------------------------\\n\")\n","\n","            # Append to dataset\n","            dataset.append({\n","                \"text\": text,\n","                \"mental_issue\": issue,\n","                \"wellbeing_insight\": insight\n","            })\n","\n","            # Save to JSON\n","            with open(json_path, \"w\") as f:\n","                json.dump(dataset, f, indent=4)\n","\n","            print(f\"✅ Record {index + 1} appended to JSON\")\n","\n","        except Exception as e:\n","            print(f\"❌ Error on record {index + 1}: {e}\")\n","\n","        # Wait for 5 seconds to handle rate limits\n","        print(f\"⏳ Waiting for 10 seconds before processing the next record...\")\n","        time.sleep(10)\n","\n","create_json_dataset()\n"],"metadata":{"id":"-lUN4KDtaAT8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Combining Dataset"],"metadata":{"id":"3-erl7W_aK1k"}},{"cell_type":"code","source":["import os\n","import json\n","from glob import glob\n","\n","# Paths\n","json_folder = \"./\"  # Folder containing JSON files\n","output_path = \"combined_mental_health_dataset.json\"\n","\n","# Collect all JSON file paths\n","json_files = glob(os.path.join(json_folder, \"*.json\"))\n","\n","# Final dataset list\n","combined_dataset = []\n","\n","# Read and combine JSON files\n","for file in json_files:\n","    with open(file, \"r\") as f:\n","        data = json.load(f)\n","        if isinstance(data, list):  # Ensure data is a list\n","            combined_dataset.extend(data)\n","        else:\n","            print(f\"⚠️ Skipping {file} as it does not contain a valid JSON list.\")\n","\n","# Remove duplicate entries based on text and mental_issue\n","unique_dataset = []\n","seen_entries = set()\n","\n","for entry in combined_dataset:\n","    identifier = (entry.get(\"text\", \"\"), entry.get(\"mental_issue\", \"\"))\n","    if identifier not in seen_entries:\n","        seen_entries.add(identifier)\n","        unique_dataset.append(entry)\n","\n","# Save combined JSON\n","with open(output_path, \"w\") as f:\n","    json.dump(unique_dataset, f, indent=4)\n","\n","print(f\"✅ Combined JSON file created: {output_path}\")"],"metadata":{"id":"en08nox0aEKL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Converting it into records with instruction, input output format"],"metadata":{"id":"uoE6b5fVaM5F"}},{"cell_type":"code","source":["import json\n","import re\n","\n","def clean_text(text):\n","    # Keep only English letters, numbers, basic punctuation, and whitespace\n","    cleaned = re.sub(r\"[^a-zA-Z0-9\\s.,?!]\", \"\", text)\n","    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()  # Normalize whitespace\n","    return cleaned\n","\n","# Load the original records\n","with open('combined_mental_health_dataset.json', 'r') as f:\n","    records = json.load(f)\n","\n","instruction_data = []\n","for idx, record in enumerate(records, start=1):\n","    cleaned_text = clean_text(record[\"text\"])\n","\n","    new_record = {\n","        \"instruction\": f\"Provide wellbeing insight for the below text with {record['mental_issue']}.\",\n","        \"input\": cleaned_text,\n","        \"output\": record[\"wellbeing_insight\"]\n","    }\n","    instruction_data.append(new_record)\n","    print(f\"Converted record #{idx}\")\n","\n","# Save the transformed data\n","with open('instruction_data.json', 'w') as f:\n","    json.dump(instruction_data, f, indent=4)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"anEHck69aR-G","executionInfo":{"status":"ok","timestamp":1743831087706,"user_tz":-330,"elapsed":517,"user":{"displayName":"Arkapratim Ghosh","userId":"10495266069218769056"}},"outputId":"96b16f0b-2bef-493c-fc0e-be818ca2ed0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Converted record #1\n","Converted record #2\n","Converted record #3\n","Converted record #4\n","Converted record #5\n","Converted record #6\n","Converted record #7\n","Converted record #8\n","Converted record #9\n","Converted record #10\n","Converted record #11\n","Converted record #12\n","Converted record #13\n","Converted record #14\n","Converted record #15\n","Converted record #16\n","Converted record #17\n","Converted record #18\n","Converted record #19\n","Converted record #20\n","Converted record #21\n","Converted record #22\n","Converted record #23\n","Converted record #24\n","Converted record #25\n","Converted record #26\n","Converted record #27\n","Converted record #28\n","Converted record #29\n","Converted record #30\n","Converted record #31\n","Converted record #32\n","Converted record #33\n","Converted record #34\n","Converted record #35\n","Converted record #36\n","Converted record #37\n","Converted record #38\n","Converted record #39\n","Converted record #40\n","Converted record #41\n","Converted record #42\n","Converted record #43\n","Converted record #44\n","Converted record #45\n","Converted record #46\n","Converted record #47\n","Converted record #48\n","Converted record #49\n","Converted record #50\n","Converted record #51\n","Converted record #52\n","Converted record #53\n","Converted record #54\n","Converted record #55\n","Converted record #56\n","Converted record #57\n","Converted record #58\n","Converted record #59\n","Converted record #60\n","Converted record #61\n","Converted record #62\n","Converted record #63\n","Converted record #64\n","Converted record #65\n","Converted record #66\n","Converted record #67\n","Converted record #68\n","Converted record #69\n","Converted record #70\n","Converted record #71\n","Converted record #72\n","Converted record #73\n","Converted record #74\n","Converted record #75\n","Converted record #76\n","Converted record #77\n","Converted record #78\n","Converted record #79\n","Converted record #80\n","Converted record #81\n","Converted record #82\n","Converted record #83\n","Converted record #84\n","Converted record #85\n","Converted record #86\n","Converted record #87\n","Converted record #88\n","Converted record #89\n","Converted record #90\n","Converted record #91\n","Converted record #92\n","Converted record #93\n","Converted record #94\n","Converted record #95\n","Converted record #96\n","Converted record #97\n","Converted record #98\n","Converted record #99\n","Converted record #100\n","Converted record #101\n","Converted record #102\n","Converted record #103\n","Converted record #104\n","Converted record #105\n","Converted record #106\n","Converted record #107\n","Converted record #108\n","Converted record #109\n","Converted record #110\n","Converted record #111\n","Converted record #112\n","Converted record #113\n","Converted record #114\n","Converted record #115\n","Converted record #116\n","Converted record #117\n","Converted record #118\n","Converted record #119\n","Converted record #120\n","Converted record #121\n","Converted record #122\n","Converted record #123\n","Converted record #124\n","Converted record #125\n","Converted record #126\n","Converted record #127\n","Converted record #128\n","Converted record #129\n","Converted record #130\n","Converted record #131\n","Converted record #132\n","Converted record #133\n","Converted record #134\n","Converted record #135\n","Converted record #136\n","Converted record #137\n","Converted record #138\n","Converted record #139\n","Converted record #140\n","Converted record #141\n","Converted record #142\n","Converted record #143\n","Converted record #144\n","Converted record #145\n","Converted record #146\n","Converted record #147\n","Converted record #148\n","Converted record #149\n","Converted record #150\n","Converted record #151\n","Converted record #152\n","Converted record #153\n","Converted record #154\n","Converted record #155\n","Converted record #156\n","Converted record #157\n","Converted record #158\n","Converted record #159\n","Converted record #160\n","Converted record #161\n","Converted record #162\n","Converted record #163\n","Converted record #164\n","Converted record #165\n","Converted record #166\n","Converted record #167\n","Converted record #168\n","Converted record #169\n","Converted record #170\n","Converted record #171\n","Converted record #172\n","Converted record #173\n","Converted record #174\n","Converted record #175\n","Converted record #176\n","Converted record #177\n","Converted record #178\n","Converted record #179\n","Converted record #180\n","Converted record #181\n","Converted record #182\n","Converted record #183\n","Converted record #184\n","Converted record #185\n","Converted record #186\n","Converted record #187\n","Converted record #188\n","Converted record #189\n","Converted record #190\n","Converted record #191\n","Converted record #192\n","Converted record #193\n","Converted record #194\n","Converted record #195\n","Converted record #196\n","Converted record #197\n","Converted record #198\n","Converted record #199\n","Converted record #200\n","Converted record #201\n","Converted record #202\n","Converted record #203\n","Converted record #204\n","Converted record #205\n","Converted record #206\n","Converted record #207\n","Converted record #208\n","Converted record #209\n","Converted record #210\n","Converted record #211\n","Converted record #212\n","Converted record #213\n","Converted record #214\n","Converted record #215\n","Converted record #216\n","Converted record #217\n","Converted record #218\n","Converted record #219\n","Converted record #220\n","Converted record #221\n","Converted record #222\n","Converted record #223\n","Converted record #224\n","Converted record #225\n","Converted record #226\n","Converted record #227\n","Converted record #228\n","Converted record #229\n","Converted record #230\n","Converted record #231\n","Converted record #232\n","Converted record #233\n","Converted record #234\n","Converted record #235\n","Converted record #236\n","Converted record #237\n","Converted record #238\n","Converted record #239\n","Converted record #240\n","Converted record #241\n","Converted record #242\n","Converted record #243\n","Converted record #244\n","Converted record #245\n","Converted record #246\n","Converted record #247\n","Converted record #248\n","Converted record #249\n","Converted record #250\n","Converted record #251\n","Converted record #252\n","Converted record #253\n","Converted record #254\n","Converted record #255\n","Converted record #256\n","Converted record #257\n","Converted record #258\n","Converted record #259\n","Converted record #260\n","Converted record #261\n","Converted record #262\n","Converted record #263\n","Converted record #264\n","Converted record #265\n","Converted record #266\n","Converted record #267\n","Converted record #268\n","Converted record #269\n","Converted record #270\n","Converted record #271\n","Converted record #272\n","Converted record #273\n","Converted record #274\n","Converted record #275\n","Converted record #276\n","Converted record #277\n","Converted record #278\n","Converted record #279\n","Converted record #280\n","Converted record #281\n","Converted record #282\n","Converted record #283\n","Converted record #284\n","Converted record #285\n","Converted record #286\n","Converted record #287\n","Converted record #288\n","Converted record #289\n","Converted record #290\n","Converted record #291\n","Converted record #292\n","Converted record #293\n","Converted record #294\n","Converted record #295\n","Converted record #296\n","Converted record #297\n","Converted record #298\n","Converted record #299\n","Converted record #300\n","Converted record #301\n","Converted record #302\n","Converted record #303\n","Converted record #304\n","Converted record #305\n","Converted record #306\n","Converted record #307\n","Converted record #308\n","Converted record #309\n","Converted record #310\n","Converted record #311\n","Converted record #312\n","Converted record #313\n","Converted record #314\n","Converted record #315\n","Converted record #316\n","Converted record #317\n","Converted record #318\n","Converted record #319\n","Converted record #320\n","Converted record #321\n","Converted record #322\n","Converted record #323\n","Converted record #324\n","Converted record #325\n","Converted record #326\n","Converted record #327\n","Converted record #328\n","Converted record #329\n","Converted record #330\n","Converted record #331\n","Converted record #332\n","Converted record #333\n","Converted record #334\n","Converted record #335\n","Converted record #336\n","Converted record #337\n","Converted record #338\n","Converted record #339\n","Converted record #340\n","Converted record #341\n","Converted record #342\n","Converted record #343\n","Converted record #344\n","Converted record #345\n","Converted record #346\n","Converted record #347\n","Converted record #348\n","Converted record #349\n","Converted record #350\n","Converted record #351\n","Converted record #352\n","Converted record #353\n","Converted record #354\n","Converted record #355\n","Converted record #356\n","Converted record #357\n","Converted record #358\n","Converted record #359\n","Converted record #360\n","Converted record #361\n","Converted record #362\n","Converted record #363\n","Converted record #364\n","Converted record #365\n","Converted record #366\n","Converted record #367\n","Converted record #368\n","Converted record #369\n","Converted record #370\n","Converted record #371\n","Converted record #372\n","Converted record #373\n","Converted record #374\n","Converted record #375\n","Converted record #376\n","Converted record #377\n","Converted record #378\n","Converted record #379\n","Converted record #380\n","Converted record #381\n","Converted record #382\n","Converted record #383\n","Converted record #384\n","Converted record #385\n","Converted record #386\n","Converted record #387\n","Converted record #388\n","Converted record #389\n","Converted record #390\n","Converted record #391\n","Converted record #392\n","Converted record #393\n","Converted record #394\n","Converted record #395\n","Converted record #396\n","Converted record #397\n","Converted record #398\n","Converted record #399\n","Converted record #400\n","Converted record #401\n","Converted record #402\n","Converted record #403\n","Converted record #404\n","Converted record #405\n","Converted record #406\n","Converted record #407\n","Converted record #408\n","Converted record #409\n","Converted record #410\n","Converted record #411\n","Converted record #412\n","Converted record #413\n","Converted record #414\n","Converted record #415\n","Converted record #416\n","Converted record #417\n","Converted record #418\n","Converted record #419\n","Converted record #420\n","Converted record #421\n","Converted record #422\n","Converted record #423\n","Converted record #424\n","Converted record #425\n","Converted record #426\n","Converted record #427\n","Converted record #428\n","Converted record #429\n","Converted record #430\n","Converted record #431\n","Converted record #432\n","Converted record #433\n","Converted record #434\n","Converted record #435\n","Converted record #436\n","Converted record #437\n","Converted record #438\n","Converted record #439\n","Converted record #440\n","Converted record #441\n","Converted record #442\n","Converted record #443\n","Converted record #444\n","Converted record #445\n","Converted record #446\n","Converted record #447\n","Converted record #448\n","Converted record #449\n","Converted record #450\n","Converted record #451\n","Converted record #452\n","Converted record #453\n","Converted record #454\n","Converted record #455\n","Converted record #456\n","Converted record #457\n","Converted record #458\n","Converted record #459\n","Converted record #460\n","Converted record #461\n","Converted record #462\n","Converted record #463\n","Converted record #464\n","Converted record #465\n","Converted record #466\n","Converted record #467\n","Converted record #468\n","Converted record #469\n","Converted record #470\n","Converted record #471\n","Converted record #472\n","Converted record #473\n","Converted record #474\n","Converted record #475\n","Converted record #476\n","Converted record #477\n","Converted record #478\n","Converted record #479\n","Converted record #480\n","Converted record #481\n","Converted record #482\n","Converted record #483\n","Converted record #484\n","Converted record #485\n","Converted record #486\n","Converted record #487\n","Converted record #488\n","Converted record #489\n","Converted record #490\n","Converted record #491\n","Converted record #492\n","Converted record #493\n","Converted record #494\n","Converted record #495\n","Converted record #496\n","Converted record #497\n","Converted record #498\n","Converted record #499\n","Converted record #500\n","Converted record #501\n","Converted record #502\n","Converted record #503\n","Converted record #504\n","Converted record #505\n","Converted record #506\n","Converted record #507\n","Converted record #508\n","Converted record #509\n","Converted record #510\n","Converted record #511\n","Converted record #512\n","Converted record #513\n","Converted record #514\n","Converted record #515\n","Converted record #516\n","Converted record #517\n","Converted record #518\n","Converted record #519\n","Converted record #520\n","Converted record #521\n","Converted record #522\n","Converted record #523\n","Converted record #524\n","Converted record #525\n","Converted record #526\n","Converted record #527\n","Converted record #528\n","Converted record #529\n","Converted record #530\n","Converted record #531\n","Converted record #532\n","Converted record #533\n","Converted record #534\n","Converted record #535\n","Converted record #536\n","Converted record #537\n","Converted record #538\n","Converted record #539\n","Converted record #540\n","Converted record #541\n","Converted record #542\n","Converted record #543\n","Converted record #544\n","Converted record #545\n","Converted record #546\n","Converted record #547\n","Converted record #548\n","Converted record #549\n","Converted record #550\n","Converted record #551\n","Converted record #552\n","Converted record #553\n","Converted record #554\n","Converted record #555\n","Converted record #556\n","Converted record #557\n","Converted record #558\n","Converted record #559\n","Converted record #560\n","Converted record #561\n","Converted record #562\n","Converted record #563\n","Converted record #564\n","Converted record #565\n","Converted record #566\n","Converted record #567\n","Converted record #568\n","Converted record #569\n","Converted record #570\n","Converted record #571\n","Converted record #572\n","Converted record #573\n","Converted record #574\n","Converted record #575\n","Converted record #576\n","Converted record #577\n","Converted record #578\n","Converted record #579\n","Converted record #580\n","Converted record #581\n","Converted record #582\n","Converted record #583\n","Converted record #584\n","Converted record #585\n","Converted record #586\n","Converted record #587\n","Converted record #588\n","Converted record #589\n","Converted record #590\n","Converted record #591\n","Converted record #592\n","Converted record #593\n","Converted record #594\n","Converted record #595\n","Converted record #596\n","Converted record #597\n","Converted record #598\n","Converted record #599\n","Converted record #600\n","Converted record #601\n","Converted record #602\n","Converted record #603\n","Converted record #604\n","Converted record #605\n","Converted record #606\n","Converted record #607\n","Converted record #608\n","Converted record #609\n","Converted record #610\n","Converted record #611\n","Converted record #612\n","Converted record #613\n","Converted record #614\n","Converted record #615\n","Converted record #616\n","Converted record #617\n","Converted record #618\n","Converted record #619\n","Converted record #620\n","Converted record #621\n","Converted record #622\n","Converted record #623\n","Converted record #624\n","Converted record #625\n","Converted record #626\n","Converted record #627\n","Converted record #628\n","Converted record #629\n","Converted record #630\n","Converted record #631\n","Converted record #632\n","Converted record #633\n","Converted record #634\n","Converted record #635\n","Converted record #636\n","Converted record #637\n","Converted record #638\n","Converted record #639\n","Converted record #640\n","Converted record #641\n","Converted record #642\n","Converted record #643\n","Converted record #644\n","Converted record #645\n","Converted record #646\n","Converted record #647\n","Converted record #648\n","Converted record #649\n","Converted record #650\n","Converted record #651\n","Converted record #652\n","Converted record #653\n","Converted record #654\n","Converted record #655\n","Converted record #656\n","Converted record #657\n","Converted record #658\n","Converted record #659\n","Converted record #660\n","Converted record #661\n","Converted record #662\n","Converted record #663\n","Converted record #664\n","Converted record #665\n","Converted record #666\n","Converted record #667\n","Converted record #668\n","Converted record #669\n","Converted record #670\n","Converted record #671\n","Converted record #672\n","Converted record #673\n","Converted record #674\n","Converted record #675\n","Converted record #676\n","Converted record #677\n","Converted record #678\n","Converted record #679\n","Converted record #680\n","Converted record #681\n","Converted record #682\n","Converted record #683\n","Converted record #684\n","Converted record #685\n","Converted record #686\n","Converted record #687\n","Converted record #688\n","Converted record #689\n","Converted record #690\n","Converted record #691\n","Converted record #692\n","Converted record #693\n","Converted record #694\n","Converted record #695\n","Converted record #696\n","Converted record #697\n","Converted record #698\n","Converted record #699\n","Converted record #700\n","Converted record #701\n","Converted record #702\n","Converted record #703\n","Converted record #704\n","Converted record #705\n","Converted record #706\n","Converted record #707\n","Converted record #708\n","Converted record #709\n","Converted record #710\n","Converted record #711\n","Converted record #712\n","Converted record #713\n","Converted record #714\n","Converted record #715\n","Converted record #716\n","Converted record #717\n","Converted record #718\n","Converted record #719\n","Converted record #720\n","Converted record #721\n","Converted record #722\n","Converted record #723\n","Converted record #724\n","Converted record #725\n","Converted record #726\n","Converted record #727\n","Converted record #728\n","Converted record #729\n","Converted record #730\n","Converted record #731\n","Converted record #732\n","Converted record #733\n","Converted record #734\n","Converted record #735\n","Converted record #736\n","Converted record #737\n","Converted record #738\n","Converted record #739\n","Converted record #740\n","Converted record #741\n","Converted record #742\n","Converted record #743\n","Converted record #744\n","Converted record #745\n","Converted record #746\n","Converted record #747\n","Converted record #748\n","Converted record #749\n","Converted record #750\n","Converted record #751\n","Converted record #752\n","Converted record #753\n","Converted record #754\n","Converted record #755\n","Converted record #756\n","Converted record #757\n","Converted record #758\n","Converted record #759\n","Converted record #760\n","Converted record #761\n","Converted record #762\n","Converted record #763\n","Converted record #764\n","Converted record #765\n","Converted record #766\n","Converted record #767\n","Converted record #768\n","Converted record #769\n","Converted record #770\n","Converted record #771\n","Converted record #772\n","Converted record #773\n","Converted record #774\n","Converted record #775\n","Converted record #776\n","Converted record #777\n","Converted record #778\n","Converted record #779\n","Converted record #780\n","Converted record #781\n","Converted record #782\n","Converted record #783\n","Converted record #784\n","Converted record #785\n","Converted record #786\n","Converted record #787\n","Converted record #788\n","Converted record #789\n","Converted record #790\n","Converted record #791\n","Converted record #792\n","Converted record #793\n","Converted record #794\n","Converted record #795\n","Converted record #796\n","Converted record #797\n","Converted record #798\n","Converted record #799\n","Converted record #800\n","Converted record #801\n","Converted record #802\n","Converted record #803\n","Converted record #804\n","Converted record #805\n","Converted record #806\n","Converted record #807\n","Converted record #808\n","Converted record #809\n","Converted record #810\n","Converted record #811\n","Converted record #812\n","Converted record #813\n","Converted record #814\n","Converted record #815\n","Converted record #816\n","Converted record #817\n","Converted record #818\n","Converted record #819\n","Converted record #820\n","Converted record #821\n","Converted record #822\n","Converted record #823\n","Converted record #824\n","Converted record #825\n","Converted record #826\n","Converted record #827\n","Converted record #828\n","Converted record #829\n","Converted record #830\n","Converted record #831\n","Converted record #832\n","Converted record #833\n","Converted record #834\n","Converted record #835\n","Converted record #836\n","Converted record #837\n","Converted record #838\n","Converted record #839\n","Converted record #840\n","Converted record #841\n","Converted record #842\n","Converted record #843\n","Converted record #844\n","Converted record #845\n","Converted record #846\n","Converted record #847\n","Converted record #848\n","Converted record #849\n","Converted record #850\n","Converted record #851\n","Converted record #852\n","Converted record #853\n","Converted record #854\n","Converted record #855\n","Converted record #856\n","Converted record #857\n","Converted record #858\n","Converted record #859\n","Converted record #860\n","Converted record #861\n","Converted record #862\n","Converted record #863\n","Converted record #864\n","Converted record #865\n","Converted record #866\n","Converted record #867\n","Converted record #868\n","Converted record #869\n","Converted record #870\n","Converted record #871\n","Converted record #872\n","Converted record #873\n","Converted record #874\n","Converted record #875\n","Converted record #876\n","Converted record #877\n","Converted record #878\n","Converted record #879\n","Converted record #880\n","Converted record #881\n","Converted record #882\n","Converted record #883\n","Converted record #884\n","Converted record #885\n","Converted record #886\n","Converted record #887\n","Converted record #888\n","Converted record #889\n","Converted record #890\n","Converted record #891\n","Converted record #892\n","Converted record #893\n","Converted record #894\n","Converted record #895\n","Converted record #896\n","Converted record #897\n","Converted record #898\n","Converted record #899\n","Converted record #900\n","Converted record #901\n","Converted record #902\n","Converted record #903\n","Converted record #904\n","Converted record #905\n","Converted record #906\n","Converted record #907\n","Converted record #908\n","Converted record #909\n","Converted record #910\n","Converted record #911\n","Converted record #912\n","Converted record #913\n","Converted record #914\n","Converted record #915\n","Converted record #916\n","Converted record #917\n","Converted record #918\n","Converted record #919\n","Converted record #920\n","Converted record #921\n","Converted record #922\n","Converted record #923\n","Converted record #924\n","Converted record #925\n","Converted record #926\n","Converted record #927\n","Converted record #928\n","Converted record #929\n","Converted record #930\n","Converted record #931\n","Converted record #932\n","Converted record #933\n","Converted record #934\n","Converted record #935\n","Converted record #936\n","Converted record #937\n","Converted record #938\n","Converted record #939\n","Converted record #940\n","Converted record #941\n","Converted record #942\n","Converted record #943\n","Converted record #944\n","Converted record #945\n","Converted record #946\n","Converted record #947\n","Converted record #948\n","Converted record #949\n","Converted record #950\n","Converted record #951\n","Converted record #952\n","Converted record #953\n","Converted record #954\n","Converted record #955\n","Converted record #956\n","Converted record #957\n","Converted record #958\n","Converted record #959\n","Converted record #960\n","Converted record #961\n","Converted record #962\n","Converted record #963\n","Converted record #964\n","Converted record #965\n","Converted record #966\n","Converted record #967\n","Converted record #968\n","Converted record #969\n","Converted record #970\n","Converted record #971\n","Converted record #972\n","Converted record #973\n","Converted record #974\n","Converted record #975\n","Converted record #976\n","Converted record #977\n","Converted record #978\n","Converted record #979\n","Converted record #980\n","Converted record #981\n","Converted record #982\n","Converted record #983\n","Converted record #984\n","Converted record #985\n","Converted record #986\n","Converted record #987\n","Converted record #988\n","Converted record #989\n","Converted record #990\n","Converted record #991\n","Converted record #992\n","Converted record #993\n","Converted record #994\n","Converted record #995\n","Converted record #996\n","Converted record #997\n","Converted record #998\n","Converted record #999\n","Converted record #1000\n","Converted record #1001\n","Converted record #1002\n","Converted record #1003\n","Converted record #1004\n","Converted record #1005\n","Converted record #1006\n","Converted record #1007\n","Converted record #1008\n","Converted record #1009\n","Converted record #1010\n","Converted record #1011\n","Converted record #1012\n","Converted record #1013\n","Converted record #1014\n","Converted record #1015\n","Converted record #1016\n","Converted record #1017\n","Converted record #1018\n","Converted record #1019\n","Converted record #1020\n","Converted record #1021\n","Converted record #1022\n","Converted record #1023\n","Converted record #1024\n","Converted record #1025\n","Converted record #1026\n","Converted record #1027\n","Converted record #1028\n","Converted record #1029\n","Converted record #1030\n","Converted record #1031\n","Converted record #1032\n","Converted record #1033\n","Converted record #1034\n","Converted record #1035\n","Converted record #1036\n","Converted record #1037\n","Converted record #1038\n","Converted record #1039\n","Converted record #1040\n","Converted record #1041\n","Converted record #1042\n","Converted record #1043\n","Converted record #1044\n","Converted record #1045\n","Converted record #1046\n","Converted record #1047\n","Converted record #1048\n","Converted record #1049\n","Converted record #1050\n","Converted record #1051\n","Converted record #1052\n","Converted record #1053\n","Converted record #1054\n","Converted record #1055\n","Converted record #1056\n","Converted record #1057\n","Converted record #1058\n","Converted record #1059\n","Converted record #1060\n","Converted record #1061\n","Converted record #1062\n","Converted record #1063\n","Converted record #1064\n","Converted record #1065\n","Converted record #1066\n","Converted record #1067\n","Converted record #1068\n","Converted record #1069\n","Converted record #1070\n","Converted record #1071\n","Converted record #1072\n","Converted record #1073\n","Converted record #1074\n","Converted record #1075\n","Converted record #1076\n","Converted record #1077\n","Converted record #1078\n","Converted record #1079\n","Converted record #1080\n","Converted record #1081\n","Converted record #1082\n","Converted record #1083\n","Converted record #1084\n","Converted record #1085\n","Converted record #1086\n","Converted record #1087\n","Converted record #1088\n","Converted record #1089\n","Converted record #1090\n","Converted record #1091\n","Converted record #1092\n","Converted record #1093\n","Converted record #1094\n","Converted record #1095\n","Converted record #1096\n","Converted record #1097\n","Converted record #1098\n","Converted record #1099\n","Converted record #1100\n","Converted record #1101\n","Converted record #1102\n","Converted record #1103\n","Converted record #1104\n","Converted record #1105\n","Converted record #1106\n","Converted record #1107\n","Converted record #1108\n","Converted record #1109\n","Converted record #1110\n","Converted record #1111\n","Converted record #1112\n","Converted record #1113\n","Converted record #1114\n","Converted record #1115\n","Converted record #1116\n","Converted record #1117\n","Converted record #1118\n","Converted record #1119\n","Converted record #1120\n","Converted record #1121\n","Converted record #1122\n","Converted record #1123\n","Converted record #1124\n","Converted record #1125\n","Converted record #1126\n","Converted record #1127\n","Converted record #1128\n","Converted record #1129\n","Converted record #1130\n","Converted record #1131\n","Converted record #1132\n","Converted record #1133\n","Converted record #1134\n","Converted record #1135\n","Converted record #1136\n","Converted record #1137\n","Converted record #1138\n","Converted record #1139\n","Converted record #1140\n","Converted record #1141\n","Converted record #1142\n","Converted record #1143\n","Converted record #1144\n","Converted record #1145\n","Converted record #1146\n","Converted record #1147\n","Converted record #1148\n","Converted record #1149\n","Converted record #1150\n","Converted record #1151\n","Converted record #1152\n","Converted record #1153\n","Converted record #1154\n","Converted record #1155\n","Converted record #1156\n","Converted record #1157\n","Converted record #1158\n","Converted record #1159\n","Converted record #1160\n","Converted record #1161\n","Converted record #1162\n","Converted record #1163\n","Converted record #1164\n","Converted record #1165\n","Converted record #1166\n","Converted record #1167\n","Converted record #1168\n","Converted record #1169\n","Converted record #1170\n","Converted record #1171\n","Converted record #1172\n","Converted record #1173\n","Converted record #1174\n","Converted record #1175\n","Converted record #1176\n","Converted record #1177\n","Converted record #1178\n","Converted record #1179\n","Converted record #1180\n","Converted record #1181\n","Converted record #1182\n","Converted record #1183\n","Converted record #1184\n","Converted record #1185\n","Converted record #1186\n","Converted record #1187\n","Converted record #1188\n","Converted record #1189\n","Converted record #1190\n","Converted record #1191\n","Converted record #1192\n","Converted record #1193\n","Converted record #1194\n","Converted record #1195\n","Converted record #1196\n","Converted record #1197\n","Converted record #1198\n","Converted record #1199\n","Converted record #1200\n","Converted record #1201\n","Converted record #1202\n","Converted record #1203\n","Converted record #1204\n","Converted record #1205\n","Converted record #1206\n","Converted record #1207\n","Converted record #1208\n","Converted record #1209\n","Converted record #1210\n","Converted record #1211\n","Converted record #1212\n","Converted record #1213\n","Converted record #1214\n","Converted record #1215\n","Converted record #1216\n","Converted record #1217\n","Converted record #1218\n","Converted record #1219\n","Converted record #1220\n","Converted record #1221\n","Converted record #1222\n","Converted record #1223\n","Converted record #1224\n","Converted record #1225\n","Converted record #1226\n","Converted record #1227\n","Converted record #1228\n","Converted record #1229\n","Converted record #1230\n","Converted record #1231\n","Converted record #1232\n","Converted record #1233\n","Converted record #1234\n","Converted record #1235\n","Converted record #1236\n","Converted record #1237\n","Converted record #1238\n","Converted record #1239\n","Converted record #1240\n","Converted record #1241\n","Converted record #1242\n","Converted record #1243\n","Converted record #1244\n","Converted record #1245\n","Converted record #1246\n","Converted record #1247\n","Converted record #1248\n","Converted record #1249\n","Converted record #1250\n","Converted record #1251\n","Converted record #1252\n","Converted record #1253\n","Converted record #1254\n","Converted record #1255\n","Converted record #1256\n","Converted record #1257\n","Converted record #1258\n","Converted record #1259\n","Converted record #1260\n","Converted record #1261\n","Converted record #1262\n","Converted record #1263\n","Converted record #1264\n","Converted record #1265\n","Converted record #1266\n","Converted record #1267\n","Converted record #1268\n","Converted record #1269\n","Converted record #1270\n","Converted record #1271\n","Converted record #1272\n","Converted record #1273\n","Converted record #1274\n","Converted record #1275\n","Converted record #1276\n","Converted record #1277\n","Converted record #1278\n","Converted record #1279\n","Converted record #1280\n","Converted record #1281\n","Converted record #1282\n","Converted record #1283\n","Converted record #1284\n","Converted record #1285\n","Converted record #1286\n","Converted record #1287\n","Converted record #1288\n","Converted record #1289\n","Converted record #1290\n","Converted record #1291\n","Converted record #1292\n","Converted record #1293\n","Converted record #1294\n","Converted record #1295\n","Converted record #1296\n","Converted record #1297\n","Converted record #1298\n","Converted record #1299\n","Converted record #1300\n","Converted record #1301\n","Converted record #1302\n","Converted record #1303\n","Converted record #1304\n","Converted record #1305\n","Converted record #1306\n","Converted record #1307\n","Converted record #1308\n","Converted record #1309\n","Converted record #1310\n","Converted record #1311\n","Converted record #1312\n","Converted record #1313\n","Converted record #1314\n","Converted record #1315\n","Converted record #1316\n","Converted record #1317\n","Converted record #1318\n","Converted record #1319\n","Converted record #1320\n","Converted record #1321\n","Converted record #1322\n","Converted record #1323\n","Converted record #1324\n","Converted record #1325\n","Converted record #1326\n","Converted record #1327\n","Converted record #1328\n","Converted record #1329\n","Converted record #1330\n","Converted record #1331\n","Converted record #1332\n","Converted record #1333\n","Converted record #1334\n","Converted record #1335\n","Converted record #1336\n","Converted record #1337\n","Converted record #1338\n","Converted record #1339\n","Converted record #1340\n","Converted record #1341\n","Converted record #1342\n","Converted record #1343\n","Converted record #1344\n","Converted record #1345\n","Converted record #1346\n","Converted record #1347\n","Converted record #1348\n","Converted record #1349\n","Converted record #1350\n","Converted record #1351\n","Converted record #1352\n","Converted record #1353\n","Converted record #1354\n","Converted record #1355\n","Converted record #1356\n","Converted record #1357\n","Converted record #1358\n","Converted record #1359\n","Converted record #1360\n","Converted record #1361\n","Converted record #1362\n","Converted record #1363\n","Converted record #1364\n","Converted record #1365\n","Converted record #1366\n","Converted record #1367\n","Converted record #1368\n","Converted record #1369\n","Converted record #1370\n","Converted record #1371\n","Converted record #1372\n","Converted record #1373\n","Converted record #1374\n","Converted record #1375\n","Converted record #1376\n","Converted record #1377\n","Converted record #1378\n","Converted record #1379\n","Converted record #1380\n","Converted record #1381\n","Converted record #1382\n","Converted record #1383\n","Converted record #1384\n","Converted record #1385\n","Converted record #1386\n","Converted record #1387\n","Converted record #1388\n","Converted record #1389\n","Converted record #1390\n","Converted record #1391\n","Converted record #1392\n","Converted record #1393\n","Converted record #1394\n","Converted record #1395\n","Converted record #1396\n","Converted record #1397\n","Converted record #1398\n","Converted record #1399\n","Converted record #1400\n","Converted record #1401\n","Converted record #1402\n","Converted record #1403\n","Converted record #1404\n","Converted record #1405\n","Converted record #1406\n","Converted record #1407\n","Converted record #1408\n","Converted record #1409\n","Converted record #1410\n","Converted record #1411\n","Converted record #1412\n","Converted record #1413\n","Converted record #1414\n","Converted record #1415\n","Converted record #1416\n","Converted record #1417\n","Converted record #1418\n","Converted record #1419\n","Converted record #1420\n","Converted record #1421\n","Converted record #1422\n","Converted record #1423\n","Converted record #1424\n","Converted record #1425\n","Converted record #1426\n","Converted record #1427\n","Converted record #1428\n","Converted record #1429\n","Converted record #1430\n","Converted record #1431\n","Converted record #1432\n","Converted record #1433\n","Converted record #1434\n","Converted record #1435\n","Converted record #1436\n","Converted record #1437\n","Converted record #1438\n","Converted record #1439\n","Converted record #1440\n","Converted record #1441\n","Converted record #1442\n","Converted record #1443\n","Converted record #1444\n","Converted record #1445\n","Converted record #1446\n","Converted record #1447\n","Converted record #1448\n","Converted record #1449\n","Converted record #1450\n","Converted record #1451\n","Converted record #1452\n","Converted record #1453\n","Converted record #1454\n","Converted record #1455\n","Converted record #1456\n","Converted record #1457\n","Converted record #1458\n","Converted record #1459\n","Converted record #1460\n","Converted record #1461\n","Converted record #1462\n","Converted record #1463\n","Converted record #1464\n","Converted record #1465\n","Converted record #1466\n","Converted record #1467\n","Converted record #1468\n","Converted record #1469\n","Converted record #1470\n","Converted record #1471\n","Converted record #1472\n","Converted record #1473\n","Converted record #1474\n","Converted record #1475\n","Converted record #1476\n","Converted record #1477\n","Converted record #1478\n","Converted record #1479\n","Converted record #1480\n","Converted record #1481\n","Converted record #1482\n","Converted record #1483\n","Converted record #1484\n","Converted record #1485\n","Converted record #1486\n","Converted record #1487\n","Converted record #1488\n","Converted record #1489\n","Converted record #1490\n","Converted record #1491\n","Converted record #1492\n","Converted record #1493\n","Converted record #1494\n","Converted record #1495\n","Converted record #1496\n","Converted record #1497\n","Converted record #1498\n","Converted record #1499\n","Converted record #1500\n","Converted record #1501\n","Converted record #1502\n","Converted record #1503\n","Converted record #1504\n","Converted record #1505\n","Converted record #1506\n","Converted record #1507\n","Converted record #1508\n","Converted record #1509\n","Converted record #1510\n","Converted record #1511\n","Converted record #1512\n","Converted record #1513\n","Converted record #1514\n","Converted record #1515\n","Converted record #1516\n","Converted record #1517\n","Converted record #1518\n","Converted record #1519\n","Converted record #1520\n","Converted record #1521\n","Converted record #1522\n","Converted record #1523\n","Converted record #1524\n","Converted record #1525\n","Converted record #1526\n","Converted record #1527\n","Converted record #1528\n","Converted record #1529\n","Converted record #1530\n","Converted record #1531\n","Converted record #1532\n","Converted record #1533\n","Converted record #1534\n","Converted record #1535\n","Converted record #1536\n","Converted record #1537\n","Converted record #1538\n","Converted record #1539\n","Converted record #1540\n","Converted record #1541\n","Converted record #1542\n","Converted record #1543\n","Converted record #1544\n","Converted record #1545\n","Converted record #1546\n","Converted record #1547\n","Converted record #1548\n","Converted record #1549\n","Converted record #1550\n","Converted record #1551\n","Converted record #1552\n","Converted record #1553\n","Converted record #1554\n","Converted record #1555\n","Converted record #1556\n","Converted record #1557\n","Converted record #1558\n","Converted record #1559\n","Converted record #1560\n","Converted record #1561\n","Converted record #1562\n","Converted record #1563\n","Converted record #1564\n","Converted record #1565\n","Converted record #1566\n","Converted record #1567\n","Converted record #1568\n","Converted record #1569\n","Converted record #1570\n","Converted record #1571\n","Converted record #1572\n","Converted record #1573\n","Converted record #1574\n","Converted record #1575\n","Converted record #1576\n","Converted record #1577\n","Converted record #1578\n","Converted record #1579\n","Converted record #1580\n","Converted record #1581\n","Converted record #1582\n","Converted record #1583\n","Converted record #1584\n","Converted record #1585\n","Converted record #1586\n","Converted record #1587\n","Converted record #1588\n","Converted record #1589\n","Converted record #1590\n","Converted record #1591\n","Converted record #1592\n","Converted record #1593\n","Converted record #1594\n","Converted record #1595\n","Converted record #1596\n","Converted record #1597\n","Converted record #1598\n","Converted record #1599\n","Converted record #1600\n","Converted record #1601\n","Converted record #1602\n","Converted record #1603\n","Converted record #1604\n","Converted record #1605\n","Converted record #1606\n","Converted record #1607\n","Converted record #1608\n","Converted record #1609\n","Converted record #1610\n","Converted record #1611\n","Converted record #1612\n","Converted record #1613\n","Converted record #1614\n","Converted record #1615\n","Converted record #1616\n","Converted record #1617\n","Converted record #1618\n","Converted record #1619\n","Converted record #1620\n","Converted record #1621\n","Converted record #1622\n","Converted record #1623\n","Converted record #1624\n","Converted record #1625\n","Converted record #1626\n","Converted record #1627\n","Converted record #1628\n","Converted record #1629\n","Converted record #1630\n","Converted record #1631\n","Converted record #1632\n","Converted record #1633\n","Converted record #1634\n","Converted record #1635\n","Converted record #1636\n","Converted record #1637\n","Converted record #1638\n","Converted record #1639\n","Converted record #1640\n","Converted record #1641\n","Converted record #1642\n","Converted record #1643\n","Converted record #1644\n","Converted record #1645\n","Converted record #1646\n","Converted record #1647\n","Converted record #1648\n","Converted record #1649\n","Converted record #1650\n","Converted record #1651\n","Converted record #1652\n","Converted record #1653\n","Converted record #1654\n","Converted record #1655\n","Converted record #1656\n","Converted record #1657\n","Converted record #1658\n","Converted record #1659\n","Converted record #1660\n","Converted record #1661\n","Converted record #1662\n","Converted record #1663\n","Converted record #1664\n","Converted record #1665\n","Converted record #1666\n","Converted record #1667\n","Converted record #1668\n","Converted record #1669\n","Converted record #1670\n","Converted record #1671\n","Converted record #1672\n","Converted record #1673\n","Converted record #1674\n","Converted record #1675\n","Converted record #1676\n","Converted record #1677\n","Converted record #1678\n","Converted record #1679\n","Converted record #1680\n","Converted record #1681\n","Converted record #1682\n","Converted record #1683\n","Converted record #1684\n","Converted record #1685\n","Converted record #1686\n","Converted record #1687\n","Converted record #1688\n","Converted record #1689\n","Converted record #1690\n","Converted record #1691\n","Converted record #1692\n","Converted record #1693\n","Converted record #1694\n","Converted record #1695\n","Converted record #1696\n","Converted record #1697\n","Converted record #1698\n","Converted record #1699\n","Converted record #1700\n","Converted record #1701\n","Converted record #1702\n","Converted record #1703\n","Converted record #1704\n","Converted record #1705\n","Converted record #1706\n","Converted record #1707\n","Converted record #1708\n","Converted record #1709\n","Converted record #1710\n","Converted record #1711\n","Converted record #1712\n","Converted record #1713\n","Converted record #1714\n","Converted record #1715\n","Converted record #1716\n","Converted record #1717\n","Converted record #1718\n","Converted record #1719\n","Converted record #1720\n","Converted record #1721\n","Converted record #1722\n","Converted record #1723\n","Converted record #1724\n","Converted record #1725\n","Converted record #1726\n","Converted record #1727\n","Converted record #1728\n","Converted record #1729\n","Converted record #1730\n","Converted record #1731\n","Converted record #1732\n","Converted record #1733\n","Converted record #1734\n","Converted record #1735\n","Converted record #1736\n","Converted record #1737\n","Converted record #1738\n","Converted record #1739\n","Converted record #1740\n","Converted record #1741\n","Converted record #1742\n","Converted record #1743\n","Converted record #1744\n","Converted record #1745\n","Converted record #1746\n","Converted record #1747\n","Converted record #1748\n","Converted record #1749\n","Converted record #1750\n","Converted record #1751\n","Converted record #1752\n","Converted record #1753\n","Converted record #1754\n","Converted record #1755\n","Converted record #1756\n","Converted record #1757\n","Converted record #1758\n","Converted record #1759\n","Converted record #1760\n","Converted record #1761\n","Converted record #1762\n","Converted record #1763\n","Converted record #1764\n","Converted record #1765\n","Converted record #1766\n","Converted record #1767\n","Converted record #1768\n","Converted record #1769\n","Converted record #1770\n","Converted record #1771\n","Converted record #1772\n","Converted record #1773\n","Converted record #1774\n","Converted record #1775\n","Converted record #1776\n","Converted record #1777\n","Converted record #1778\n","Converted record #1779\n","Converted record #1780\n","Converted record #1781\n","Converted record #1782\n","Converted record #1783\n","Converted record #1784\n","Converted record #1785\n","Converted record #1786\n","Converted record #1787\n","Converted record #1788\n","Converted record #1789\n","Converted record #1790\n","Converted record #1791\n","Converted record #1792\n","Converted record #1793\n","Converted record #1794\n","Converted record #1795\n","Converted record #1796\n","Converted record #1797\n","Converted record #1798\n","Converted record #1799\n","Converted record #1800\n","Converted record #1801\n","Converted record #1802\n","Converted record #1803\n","Converted record #1804\n","Converted record #1805\n","Converted record #1806\n","Converted record #1807\n","Converted record #1808\n","Converted record #1809\n","Converted record #1810\n","Converted record #1811\n","Converted record #1812\n","Converted record #1813\n","Converted record #1814\n","Converted record #1815\n","Converted record #1816\n","Converted record #1817\n","Converted record #1818\n","Converted record #1819\n","Converted record #1820\n","Converted record #1821\n","Converted record #1822\n","Converted record #1823\n","Converted record #1824\n","Converted record #1825\n","Converted record #1826\n","Converted record #1827\n","Converted record #1828\n","Converted record #1829\n","Converted record #1830\n","Converted record #1831\n","Converted record #1832\n","Converted record #1833\n","Converted record #1834\n","Converted record #1835\n","Converted record #1836\n","Converted record #1837\n","Converted record #1838\n","Converted record #1839\n","Converted record #1840\n","Converted record #1841\n","Converted record #1842\n","Converted record #1843\n"]}]},{"cell_type":"code","source":["import json\n","\n","# Load the transformed instructions\n","with open('instruction_data.json', 'r') as f:\n","    data = json.load(f)\n","\n","print(len(data))\n","print(\"Example entry : \", data[50])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YfoMBVPpbYQJ","executionInfo":{"status":"ok","timestamp":1743833618399,"user_tz":-330,"elapsed":76,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"d495cd00-a2c1-4a26-9b4c-7dd3f529edf6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["1843\n","Example entry :  {'instruction': 'Provide wellbeing insight for the below text with normal.', 'input': 'In a first since 1938, Des Moines, Iowa, kids will trickortreat on Halloween', 'output': \"The text describes a return to normalcy, indicating potentially positive impacts on psychological wellbeing.\\n\\n**Ryff Scale Insights:**\\n\\n*   **Autonomy:** Resuming activities fosters independence. Encourage decision-making in Halloween plans.\\n*   **Environmental Mastery:** Reclaiming traditions builds competence. Help kids navigate trick-or-treating logistics.\\n*   **Personal Growth:** New experiences aid development. Encourage exploring different costumes/neighborhoods.\\n*   **Positive Relations:** Social events strengthen bonds. Facilitate interaction with friends/neighbors.\\n*   **Purpose in Life:** Participating in community rituals provides meaning. Discuss Halloween's cultural significance.\\n*   **Self-Acceptance:** Normalcy can boost confidence. Celebrate enjoyment of the tradition itself.\\n\\nThis event, while seemingly small, can contribute to a sense of stability and overall psychological health by reinforcing positive social engagement and personal fulfillment.\"}\n"]}]},{"cell_type":"markdown","source":["# Converting Instructions into Alpaca format"],"metadata":{"id":"YhHavVBKcKaG"}},{"cell_type":"code","source":["def format_input(entry):\n","    instruction_text = (\n","        f\"Below is an instruction that describes a task. \"\n","        f\"Write a response that appropriately completes the request.\"\n","        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n","    )\n","\n","    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n","\n","    return instruction_text + input_text"],"metadata":{"id":"84yTU3c3cNym","executionInfo":{"status":"ok","timestamp":1743833620477,"user_tz":-330,"elapsed":32,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["model_input = format_input(data[50])\n","desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n","\n","print(model_input + desired_response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhYQEwRgcRt2","executionInfo":{"status":"ok","timestamp":1743833622074,"user_tz":-330,"elapsed":31,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"ee6ef36f-4656-4ee9-9c1d-5fae95da3183"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Provide wellbeing insight for the below text with normal.\n","\n","### Input:\n","In a first since 1938, Des Moines, Iowa, kids will trickortreat on Halloween\n","\n","### Response:\n","The text describes a return to normalcy, indicating potentially positive impacts on psychological wellbeing.\n","\n","**Ryff Scale Insights:**\n","\n","*   **Autonomy:** Resuming activities fosters independence. Encourage decision-making in Halloween plans.\n","*   **Environmental Mastery:** Reclaiming traditions builds competence. Help kids navigate trick-or-treating logistics.\n","*   **Personal Growth:** New experiences aid development. Encourage exploring different costumes/neighborhoods.\n","*   **Positive Relations:** Social events strengthen bonds. Facilitate interaction with friends/neighbors.\n","*   **Purpose in Life:** Participating in community rituals provides meaning. Discuss Halloween's cultural significance.\n","*   **Self-Acceptance:** Normalcy can boost confidence. Celebrate enjoyment of the tradition itself.\n","\n","This event, while seemingly small, can contribute to a sense of stability and overall psychological health by reinforcing positive social engagement and personal fulfillment.\n"]}]},{"cell_type":"code","source":["model_input = format_input(data[999])\n","desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n","\n","print(model_input + desired_response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qyy1REQLc-B-","executionInfo":{"status":"ok","timestamp":1743833624099,"user_tz":-330,"elapsed":10,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"def26599-196a-43cc-fe0d-db60d12cd25e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Provide wellbeing insight for the below text with normal.\n","\n","### Input:\n","Harvey Weinstein sentenced to 23 years in prison for sex assault in case that sparked MeToo movement\n","\n","### Response:\n","This news may evoke strong emotions. Here's how to maintain wellbeing:\n","\n","*   **Autonomy:** Acknowledge your feelings without letting them dictate your actions. Focus on what you *can* control.\n","*   **Env. Mastery:** Channel frustration into constructive action, like supporting related causes.\n","*   **Personal Growth:** Reflect on your values and how this news impacts them. Use it as a catalyst for growth.\n","*   **Positive Relations:** Connect with others, share your feelings, and offer support.\n","*   **Purpose in Life:** Reaffirm your values & find meaning in fighting injustice.\n","*   **Self-Acceptance:** Validate your emotional response; accept it's okay to feel upset.\n"]}]},{"cell_type":"markdown","source":["# Splitting dataset into train test validation"],"metadata":{"id":"X8Da7F1edAZV"}},{"cell_type":"code","source":["train_portion = int(len(data) * 0.85)  # 85% for training\n","test_portion = int(len(data) * 0.1)    # 10% for testing\n","val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n","\n","train_data = data[:train_portion]\n","test_data = data[train_portion:train_portion + test_portion]\n","val_data = data[train_portion + test_portion:]"],"metadata":{"id":"FpeK0KBpdEbO","executionInfo":{"status":"ok","timestamp":1743833626160,"user_tz":-330,"elapsed":14,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(\"Training set length:\", len(train_data))\n","print(\"Validation set length:\", len(val_data))\n","print(\"Test set length:\", len(test_data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTDGgJTZdJyX","executionInfo":{"status":"ok","timestamp":1743833627750,"user_tz":-330,"elapsed":20,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"c6e01bde-c307-4e7c-de40-fcfdad8fddb0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set length: 1566\n","Validation set length: 93\n","Test set length: 184\n"]}]},{"cell_type":"code","source":["print(train_data[0])\n","print(test_data[0])\n","print(val_data[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5NK7cv6jMD8","executionInfo":{"status":"ok","timestamp":1743833628959,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"149ceead-21ab-45c8-c046-05e54c9d6e2e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{'instruction': 'Provide wellbeing insight for the below text with normal.', 'input': 'Boeing, union reach sweetened contract offer in bid to end strike, vote scheduled for Monday', 'output': \"The text suggests a return to normalcy at Boeing, potentially reducing employee stress. Here's wellbeing advice:\\n\\n*   **Autonomy:** Exercise choice in daily tasks.\\n*   **Environmental Mastery:** Tackle a small, achievable home project.\\n*   **Personal Growth:** Learn a new skill, even a simple one.\\n*   **Positive Relations:** Connect with a coworker or friend.\\n*   **Purpose in Life:** Reflect on your work's contribution.\\n*   **Self-Acceptance:** Acknowledge your strengths in resolving the situation.\"}\n","{'instruction': 'Provide wellbeing insight for the below text with normal.', 'input': 'The US debt is now projected to be larger than the US economy', 'output': \"The text evokes anxiety about economic stability. Here's wellbeing advice:\\n\\n*   **Autonomy:** Research diverse economic viewpoints to form your own opinion.\\n*   **Environmental Mastery:** Focus on personal finances: budget, save, invest wisely.\\n*   **Personal Growth:** Learn about economics; take free online courses.\\n*   **Positive Relations:** Discuss concerns with trusted friends/family; avoid echo chambers.\\n*   **Purpose in Life:** Volunteer to help those economically vulnerable; find meaning in action.\\n*   **Self-Acceptance:** Acknowledge economic anxieties are normal; practice mindfulness to manage stress.\"}\n","{'instruction': 'Provide wellbeing insight for the below text with anxiety.', 'input': 'Normal? Ive been stuck in a panic attack cycle for about 7 months now. Constant. Once it stops it immediately picks back up again. Today, I prayed to God and asked him to help me, to take this pain away, and since then my internal shaking has stopped and I havent had any health anxiety thoughts. I havent felt like this is months and Im freaked out to have panic attacks and freaked out once I dont have them anymore. Help', 'output': 'The text describes a prolonged anxiety and panic cycle. The sudden cessation, while welcome, creates further anxiety.\\n\\n**Ryff Scale Insights:**\\n\\n*   **Autonomy:** Cultivate independence. Practice making small decisions confidently.\\n*   **Env. Mastery:** Set realistic goals, break them down. Focus on what you CAN control.\\n*   **Personal Growth:** Explore new hobbies/skills. Journal to track positive changes.\\n*   **Positive Relations:** Connect with supportive friends/family. Seek a therapist.\\n*   **Purpose in Life:** Reflect on values. Volunteer or engage in meaningful activities.\\n*   **Self-Acceptance:** Practice self-compassion. Acknowledge imperfections without judgment.'}\n"]}]},{"cell_type":"markdown","source":["# Organising data into training batches"],"metadata":{"id":"0RtYMpbjdOmn"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","\n","\n","class InstructionDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","\n","        # Pre-tokenize texts\n","        self.encoded_texts = []\n","        for entry in data:\n","            instruction_plus_input = format_input(entry)\n","            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n","            full_text = instruction_plus_input + response_text\n","            self.encoded_texts.append(\n","                tokenizer.encode(full_text)\n","            )\n","\n","    def __getitem__(self, index):\n","        return self.encoded_texts[index]\n","\n","    def __len__(self):\n","        return len(self.data)"],"metadata":{"id":"-kJOqHrxdRfQ","executionInfo":{"status":"ok","timestamp":1743833635377,"user_tz":-330,"elapsed":4496,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITwdxV3bdZC4","executionInfo":{"status":"ok","timestamp":1743833639633,"user_tz":-330,"elapsed":2635,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"aa967c98-c895-4083-a670-fc2f87960eb1"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.9.0\n"]}]},{"cell_type":"code","source":["import tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eYKh7GDqdVWY","executionInfo":{"status":"ok","timestamp":1743833644042,"user_tz":-330,"elapsed":3444,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"d57665a0-4029-45dc-c506-b85d19be5d89"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[50256]\n"]}]},{"cell_type":"code","source":["def custom_collate_draft_1(\n","    batch,\n","    pad_token_id=50256,\n","    device=\"cpu\"\n","):\n","    # Find the longest sequence in the batch\n","    # and increase the max length by +1, which will add one extra\n","    # padding token below\n","    batch_max_length = max(len(item)+1 for item in batch)\n","\n","    # Pad and prepare inputs\n","    inputs_lst = []\n","\n","    for item in batch:\n","        new_item = item.copy()\n","        # Add an <|endoftext|> token\n","        new_item += [pad_token_id]\n","        # Pad sequences to batch_max_length\n","        padded = (\n","            new_item + [pad_token_id] *\n","            (batch_max_length - len(new_item))\n","        )\n","        # Via padded[:-1], we remove the extra padded token\n","        # that has been added via the +1 setting in batch_max_length\n","        # (the extra padding token will be relevant in later codes)\n","        inputs = torch.tensor(padded[:-1])\n","        inputs_lst.append(inputs)\n","\n","    # Convert list of inputs to tensor and transfer to target device\n","    inputs_tensor = torch.stack(inputs_lst).to(device)\n","    return inputs_tensor"],"metadata":{"id":"r3qrcWKqdddg","executionInfo":{"status":"ok","timestamp":1743833647799,"user_tz":-330,"elapsed":22,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["inputs_1 = [0, 1, 2, 3, 4]\n","inputs_2 = [5, 6]\n","inputs_3 = [7, 8, 9]\n","\n","batch = (\n","    inputs_1,\n","    inputs_2,\n","    inputs_3\n",")\n","\n","print(custom_collate_draft_1(batch))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VV3sRH7_dfla","executionInfo":{"status":"ok","timestamp":1743833650196,"user_tz":-330,"elapsed":134,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"6deb7e0d-51f2-42c4-c009-ea7a279c02c6"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[    0,     1,     2,     3,     4],\n","        [    5,     6, 50256, 50256, 50256],\n","        [    7,     8,     9, 50256, 50256]])\n"]}]},{"cell_type":"markdown","source":["# Creating target token ids for training"],"metadata":{"id":"IYiMz2Dfdhgy"}},{"cell_type":"code","source":["def custom_collate_draft_2(\n","    batch,\n","    pad_token_id=50256,\n","    device=\"cpu\"\n","):\n","    # Find the longest sequence in the batch\n","    batch_max_length = max(len(item)+1 for item in batch)\n","\n","    # Pad and prepare inputs\n","    inputs_lst, targets_lst = [], []\n","\n","    for item in batch:\n","        new_item = item.copy()\n","        # Add an <|endoftext|> token\n","        new_item += [pad_token_id]\n","        # Pad sequences to max_length\n","        padded = (\n","            new_item + [pad_token_id] *\n","            (batch_max_length - len(new_item))\n","        )\n","        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n","        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n","        inputs_lst.append(inputs)\n","        targets_lst.append(targets)\n","\n","    # Convert list of inputs to tensor and transfer to target device\n","    inputs_tensor = torch.stack(inputs_lst).to(device)\n","    targets_tensor = torch.stack(targets_lst).to(device)\n","    return inputs_tensor, targets_tensor"],"metadata":{"id":"WQgdVpEBdkMC","executionInfo":{"status":"ok","timestamp":1743833652214,"user_tz":-330,"elapsed":22,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["inputs_1 = [0, 1, 2, 3, 4]\n","inputs_2 = [5, 6]\n","inputs_3 = [7, 8, 9]\n","\n","batch = (\n","    inputs_1,\n","    inputs_2,\n","    inputs_3\n",")\n","\n","inputs, targets = custom_collate_draft_2(batch)\n","print(inputs)\n","print(targets)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FrNz9JGhdnUS","executionInfo":{"status":"ok","timestamp":1743833654484,"user_tz":-330,"elapsed":27,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"71a89216-4aaf-4afb-d489-2a5c42897e56"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[    0,     1,     2,     3,     4],\n","        [    5,     6, 50256, 50256, 50256],\n","        [    7,     8,     9, 50256, 50256]])\n","tensor([[    1,     2,     3,     4, 50256],\n","        [    6, 50256, 50256, 50256, 50256],\n","        [    8,     9, 50256, 50256, 50256]])\n"]}]},{"cell_type":"code","source":["def custom_collate_fn(\n","    batch,\n","    pad_token_id=50256,\n","    ignore_index=-100,\n","    allowed_max_length=None,\n","    device=\"cpu\"\n","):\n","    # Find the longest sequence in the batch\n","    batch_max_length = max(len(item)+1 for item in batch)\n","\n","    # Pad and prepare inputs and targets\n","    inputs_lst, targets_lst = [], []\n","\n","    for item in batch:\n","        new_item = item.copy()\n","        # Add an <|endoftext|> token\n","        new_item += [pad_token_id]\n","        # Pad sequences to max_length\n","        padded = (\n","            new_item + [pad_token_id] *\n","            (batch_max_length - len(new_item))\n","        )\n","        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n","        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n","\n","        # New: Replace all but the first padding tokens in targets by ignore_index\n","        mask = targets == pad_token_id\n","        indices = torch.nonzero(mask).squeeze()\n","        if indices.numel() > 1:\n","            targets[indices[1:]] = ignore_index\n","\n","        # New: Optionally truncate to maximum sequence length\n","        if allowed_max_length is not None:\n","            inputs = inputs[:allowed_max_length]\n","            targets = targets[:allowed_max_length]\n","\n","        inputs_lst.append(inputs)\n","        targets_lst.append(targets)\n","\n","    # Convert list of inputs and targets to tensors and transfer to target device\n","    inputs_tensor = torch.stack(inputs_lst).to(device)\n","    targets_tensor = torch.stack(targets_lst).to(device)\n","\n","    return inputs_tensor, targets_tensor"],"metadata":{"id":"-YP_IY4RdpSC","executionInfo":{"status":"ok","timestamp":1743833656180,"user_tz":-330,"elapsed":24,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["inputs_1 = [0, 1, 2, 3, 4]\n","inputs_2 = [5, 6]\n","inputs_3 = [7, 8, 9]\n","\n","batch = (\n","    inputs_1,\n","    inputs_2,\n","    inputs_3\n",")\n","\n","inputs, targets = custom_collate_fn(batch)\n","print(inputs)\n","print(targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QExGBh_XdrIi","executionInfo":{"status":"ok","timestamp":1743833658433,"user_tz":-330,"elapsed":43,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"ece88b13-a697-4fb7-8477-8339f3f5068f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[    0,     1,     2,     3,     4],\n","        [    5,     6, 50256, 50256, 50256],\n","        [    7,     8,     9, 50256, 50256]])\n","tensor([[    1,     2,     3,     4, 50256],\n","        [    6, 50256,  -100,  -100,  -100],\n","        [    8,     9, 50256,  -100,  -100]])\n"]}]},{"cell_type":"code","source":["logits_1 = torch.tensor(\n","    [[-1.0, 1.0],  # 1st training example\n","     [-0.5, 1.5]]  # 2nd training example\n",")\n","targets_1 = torch.tensor([0, 1])\n","\n","\n","loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n","print(loss_1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cQ4r3FCJdtAr","executionInfo":{"status":"ok","timestamp":1743833660868,"user_tz":-330,"elapsed":14,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"c0ec122c-c273-4f19-a2b5-d786ded4c29e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.1269)\n"]}]},{"cell_type":"code","source":["logits_2 = torch.tensor(\n","    [[-1.0, 1.0],\n","     [-0.5, 1.5],\n","     [-0.5, 1.5]]  # New 3rd training example\n",")\n","targets_2 = torch.tensor([0, 1, 1])\n","\n","loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n","print(loss_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uMqA9j4Zdul7","executionInfo":{"status":"ok","timestamp":1743833662533,"user_tz":-330,"elapsed":26,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"1102f599-9f4c-4ec2-9db3-9fad86c1abab"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.7936)\n"]}]},{"cell_type":"code","source":["logits_2 = torch.tensor(\n","    [[-1.0, 1.0],\n","     [-0.5, 1.5],\n","     [-0.5, 1.5]]  # New 3rd training example\n",")\n","\n","targets_3 = torch.tensor([0, 1, -100])\n","\n","loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n","print(loss_3)\n","print(\"loss_1 == loss_3:\", loss_1 == loss_3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uff7j-i9dwKj","executionInfo":{"status":"ok","timestamp":1743833664054,"user_tz":-330,"elapsed":12,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"d46ea99b-7c9e-41db-fbd5-5bb18282507f"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.1269)\n","loss_1 == loss_3: tensor(True)\n"]}]},{"cell_type":"markdown","source":["# Creating dataloaders for instruction dataset"],"metadata":{"id":"0ypv3nFQd1Bs"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Note:\n","# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n","# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n","# However, the resulting loss values may be slightly different.\n","\n","#if torch.cuda.is_available():\n","#    device = torch.device(\"cuda\")\n","#elif torch.backends.mps.is_available():\n","#    device = torch.device(\"mps\")\n","#else:\n","#    device = torch.device(\"cpu\")\n","\n","print(\"Device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdG7jMG3d4I9","executionInfo":{"status":"ok","timestamp":1743833665689,"user_tz":-330,"elapsed":27,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"13e4d6fb-1b3c-48c5-85fc-2176297f1954"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","source":["from functools import partial\n","customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"],"metadata":{"id":"45V4zILjd77d","executionInfo":{"status":"ok","timestamp":1743833667448,"user_tz":-330,"elapsed":70,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","\n","num_workers = 0\n","batch_size = 8\n","\n","torch.manual_seed(123)\n","\n","train_dataset = InstructionDataset(train_data, tokenizer)\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    collate_fn=customized_collate_fn,\n","    shuffle=True,\n","    drop_last=True,\n","    num_workers=num_workers\n",")\n","\n","val_dataset = InstructionDataset(val_data, tokenizer)\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","    collate_fn=customized_collate_fn,\n","    shuffle=False,\n","    drop_last=False,\n","    num_workers=num_workers\n",")\n","\n","test_dataset = InstructionDataset(test_data, tokenizer)\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=batch_size,\n","    collate_fn=customized_collate_fn,\n","    shuffle=False,\n","    drop_last=False,\n","    num_workers=num_workers\n",")"],"metadata":{"id":"Slafd1_9d93e","executionInfo":{"status":"ok","timestamp":1743833671599,"user_tz":-330,"elapsed":342,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["print(\"Train loader:\")\n","for inputs, targets in train_loader:\n","    print(inputs.shape, targets.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMuWfcwqd__m","executionInfo":{"status":"ok","timestamp":1743833674862,"user_tz":-330,"elapsed":831,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"850244bc-de22-42b8-8631-66ae80568470"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader:\n","torch.Size([8, 584]) torch.Size([8, 584])\n","torch.Size([8, 560]) torch.Size([8, 560])\n","torch.Size([8, 539]) torch.Size([8, 539])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 820]) torch.Size([8, 820])\n","torch.Size([8, 424]) torch.Size([8, 424])\n","torch.Size([8, 713]) torch.Size([8, 713])\n","torch.Size([8, 293]) torch.Size([8, 293])\n","torch.Size([8, 553]) torch.Size([8, 553])\n","torch.Size([8, 448]) torch.Size([8, 448])\n","torch.Size([8, 530]) torch.Size([8, 530])\n","torch.Size([8, 530]) torch.Size([8, 530])\n","torch.Size([8, 875]) torch.Size([8, 875])\n","torch.Size([8, 309]) torch.Size([8, 309])\n","torch.Size([8, 448]) torch.Size([8, 448])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 447]) torch.Size([8, 447])\n","torch.Size([8, 764]) torch.Size([8, 764])\n","torch.Size([8, 352]) torch.Size([8, 352])\n","torch.Size([8, 605]) torch.Size([8, 605])\n","torch.Size([8, 473]) torch.Size([8, 473])\n","torch.Size([8, 417]) torch.Size([8, 417])\n","torch.Size([8, 803]) torch.Size([8, 803])\n","torch.Size([8, 611]) torch.Size([8, 611])\n","torch.Size([8, 442]) torch.Size([8, 442])\n","torch.Size([8, 416]) torch.Size([8, 416])\n","torch.Size([8, 585]) torch.Size([8, 585])\n","torch.Size([8, 389]) torch.Size([8, 389])\n","torch.Size([8, 723]) torch.Size([8, 723])\n","torch.Size([8, 594]) torch.Size([8, 594])\n","torch.Size([8, 485]) torch.Size([8, 485])\n","torch.Size([8, 765]) torch.Size([8, 765])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 429]) torch.Size([8, 429])\n","torch.Size([8, 319]) torch.Size([8, 319])\n","torch.Size([8, 436]) torch.Size([8, 436])\n","torch.Size([8, 726]) torch.Size([8, 726])\n","torch.Size([8, 472]) torch.Size([8, 472])\n","torch.Size([8, 810]) torch.Size([8, 810])\n","torch.Size([8, 447]) torch.Size([8, 447])\n","torch.Size([8, 482]) torch.Size([8, 482])\n","torch.Size([8, 861]) torch.Size([8, 861])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 537]) torch.Size([8, 537])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 399]) torch.Size([8, 399])\n","torch.Size([8, 408]) torch.Size([8, 408])\n","torch.Size([8, 633]) torch.Size([8, 633])\n","torch.Size([8, 438]) torch.Size([8, 438])\n","torch.Size([8, 441]) torch.Size([8, 441])\n","torch.Size([8, 1023]) torch.Size([8, 1023])\n","torch.Size([8, 581]) torch.Size([8, 581])\n","torch.Size([8, 324]) torch.Size([8, 324])\n","torch.Size([8, 713]) torch.Size([8, 713])\n","torch.Size([8, 919]) torch.Size([8, 919])\n","torch.Size([8, 450]) torch.Size([8, 450])\n","torch.Size([8, 552]) torch.Size([8, 552])\n","torch.Size([8, 359]) torch.Size([8, 359])\n","torch.Size([8, 765]) torch.Size([8, 765])\n","torch.Size([8, 607]) torch.Size([8, 607])\n","torch.Size([8, 734]) torch.Size([8, 734])\n","torch.Size([8, 604]) torch.Size([8, 604])\n","torch.Size([8, 402]) torch.Size([8, 402])\n","torch.Size([8, 755]) torch.Size([8, 755])\n","torch.Size([8, 671]) torch.Size([8, 671])\n","torch.Size([8, 798]) torch.Size([8, 798])\n","torch.Size([8, 696]) torch.Size([8, 696])\n","torch.Size([8, 395]) torch.Size([8, 395])\n","torch.Size([8, 579]) torch.Size([8, 579])\n","torch.Size([8, 436]) torch.Size([8, 436])\n","torch.Size([8, 787]) torch.Size([8, 787])\n","torch.Size([8, 587]) torch.Size([8, 587])\n","torch.Size([8, 814]) torch.Size([8, 814])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 534]) torch.Size([8, 534])\n","torch.Size([8, 444]) torch.Size([8, 444])\n","torch.Size([8, 1022]) torch.Size([8, 1022])\n","torch.Size([8, 671]) torch.Size([8, 671])\n","torch.Size([8, 410]) torch.Size([8, 410])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 538]) torch.Size([8, 538])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 512]) torch.Size([8, 512])\n","torch.Size([8, 448]) torch.Size([8, 448])\n","torch.Size([8, 583]) torch.Size([8, 583])\n","torch.Size([8, 477]) torch.Size([8, 477])\n","torch.Size([8, 498]) torch.Size([8, 498])\n","torch.Size([8, 479]) torch.Size([8, 479])\n","torch.Size([8, 608]) torch.Size([8, 608])\n","torch.Size([8, 570]) torch.Size([8, 570])\n","torch.Size([8, 821]) torch.Size([8, 821])\n","torch.Size([8, 390]) torch.Size([8, 390])\n","torch.Size([8, 906]) torch.Size([8, 906])\n","torch.Size([8, 933]) torch.Size([8, 933])\n","torch.Size([8, 417]) torch.Size([8, 417])\n","torch.Size([8, 459]) torch.Size([8, 459])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 486]) torch.Size([8, 486])\n","torch.Size([8, 571]) torch.Size([8, 571])\n","torch.Size([8, 509]) torch.Size([8, 509])\n","torch.Size([8, 375]) torch.Size([8, 375])\n","torch.Size([8, 521]) torch.Size([8, 521])\n","torch.Size([8, 621]) torch.Size([8, 621])\n","torch.Size([8, 575]) torch.Size([8, 575])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 542]) torch.Size([8, 542])\n","torch.Size([8, 489]) torch.Size([8, 489])\n","torch.Size([8, 598]) torch.Size([8, 598])\n","torch.Size([8, 471]) torch.Size([8, 471])\n","torch.Size([8, 322]) torch.Size([8, 322])\n","torch.Size([8, 471]) torch.Size([8, 471])\n","torch.Size([8, 597]) torch.Size([8, 597])\n","torch.Size([8, 470]) torch.Size([8, 470])\n","torch.Size([8, 409]) torch.Size([8, 409])\n","torch.Size([8, 319]) torch.Size([8, 319])\n","torch.Size([8, 826]) torch.Size([8, 826])\n","torch.Size([8, 917]) torch.Size([8, 917])\n","torch.Size([8, 377]) torch.Size([8, 377])\n","torch.Size([8, 442]) torch.Size([8, 442])\n","torch.Size([8, 669]) torch.Size([8, 669])\n","torch.Size([8, 636]) torch.Size([8, 636])\n","torch.Size([8, 571]) torch.Size([8, 571])\n","torch.Size([8, 486]) torch.Size([8, 486])\n","torch.Size([8, 471]) torch.Size([8, 471])\n","torch.Size([8, 613]) torch.Size([8, 613])\n","torch.Size([8, 690]) torch.Size([8, 690])\n","torch.Size([8, 455]) torch.Size([8, 455])\n","torch.Size([8, 317]) torch.Size([8, 317])\n","torch.Size([8, 374]) torch.Size([8, 374])\n","torch.Size([8, 710]) torch.Size([8, 710])\n","torch.Size([8, 751]) torch.Size([8, 751])\n","torch.Size([8, 616]) torch.Size([8, 616])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 553]) torch.Size([8, 553])\n","torch.Size([8, 482]) torch.Size([8, 482])\n","torch.Size([8, 863]) torch.Size([8, 863])\n","torch.Size([8, 788]) torch.Size([8, 788])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 485]) torch.Size([8, 485])\n","torch.Size([8, 610]) torch.Size([8, 610])\n","torch.Size([8, 451]) torch.Size([8, 451])\n","torch.Size([8, 282]) torch.Size([8, 282])\n","torch.Size([8, 681]) torch.Size([8, 681])\n","torch.Size([8, 539]) torch.Size([8, 539])\n","torch.Size([8, 599]) torch.Size([8, 599])\n","torch.Size([8, 867]) torch.Size([8, 867])\n","torch.Size([8, 975]) torch.Size([8, 975])\n","torch.Size([8, 723]) torch.Size([8, 723])\n","torch.Size([8, 869]) torch.Size([8, 869])\n","torch.Size([8, 526]) torch.Size([8, 526])\n","torch.Size([8, 426]) torch.Size([8, 426])\n","torch.Size([8, 553]) torch.Size([8, 553])\n","torch.Size([8, 535]) torch.Size([8, 535])\n","torch.Size([8, 373]) torch.Size([8, 373])\n","torch.Size([8, 556]) torch.Size([8, 556])\n","torch.Size([8, 916]) torch.Size([8, 916])\n","torch.Size([8, 493]) torch.Size([8, 493])\n","torch.Size([8, 605]) torch.Size([8, 605])\n","torch.Size([8, 853]) torch.Size([8, 853])\n","torch.Size([8, 743]) torch.Size([8, 743])\n","torch.Size([8, 633]) torch.Size([8, 633])\n","torch.Size([8, 381]) torch.Size([8, 381])\n","torch.Size([8, 754]) torch.Size([8, 754])\n","torch.Size([8, 422]) torch.Size([8, 422])\n","torch.Size([8, 512]) torch.Size([8, 512])\n","torch.Size([8, 533]) torch.Size([8, 533])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 950]) torch.Size([8, 950])\n","torch.Size([8, 460]) torch.Size([8, 460])\n","torch.Size([8, 614]) torch.Size([8, 614])\n","torch.Size([8, 556]) torch.Size([8, 556])\n","torch.Size([8, 511]) torch.Size([8, 511])\n","torch.Size([8, 732]) torch.Size([8, 732])\n","torch.Size([8, 343]) torch.Size([8, 343])\n","torch.Size([8, 346]) torch.Size([8, 346])\n","torch.Size([8, 685]) torch.Size([8, 685])\n","torch.Size([8, 410]) torch.Size([8, 410])\n","torch.Size([8, 496]) torch.Size([8, 496])\n","torch.Size([8, 422]) torch.Size([8, 422])\n","torch.Size([8, 336]) torch.Size([8, 336])\n","torch.Size([8, 724]) torch.Size([8, 724])\n","torch.Size([8, 439]) torch.Size([8, 439])\n","torch.Size([8, 890]) torch.Size([8, 890])\n","torch.Size([8, 441]) torch.Size([8, 441])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 723]) torch.Size([8, 723])\n","torch.Size([8, 524]) torch.Size([8, 524])\n","torch.Size([8, 416]) torch.Size([8, 416])\n","torch.Size([8, 1024]) torch.Size([8, 1024])\n","torch.Size([8, 803]) torch.Size([8, 803])\n","torch.Size([8, 766]) torch.Size([8, 766])\n","torch.Size([8, 876]) torch.Size([8, 876])\n","torch.Size([8, 631]) torch.Size([8, 631])\n"]}]},{"cell_type":"markdown","source":["# Loading a pretrained SLM"],"metadata":{"id":"1LxKH0O7eK6k"}},{"cell_type":"code","source":["\n","# Import Libraries\n","import tiktoken\n","import torch\n","import torch.nn as nn\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": True       # Query-Key-Value bias\n","}\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n","\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n","            GELU(), ## Activation\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        # 2*4*768\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x\n","        # 2*4*768\n","\n","class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","def generate_text_simple(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:]\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","        logits = logits[:, -1, :]\n","\n","        # New: Filter logits with top_k sampling\n","        if top_k is not None:\n","            # Keep only top_k values\n","            top_logits, _ = torch.topk(logits, top_k)\n","            min_val = top_logits[:, -1]\n","            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n","\n","        # New: Apply temperature scaling\n","        if temperature > 0.0:\n","            logits = logits / temperature\n","\n","            # Apply softmax to get probabilities\n","            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n","\n","            # Sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n","\n","        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n","        else:\n","            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n","\n","        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n","            break\n","\n","        # Same as before: append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n","\n","    return idx\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","file_path = \"filtered_articles.txt\"\n","\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    text_data = file.read()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","total_characters = len(text_data)\n","total_tokens = len(tokenizer.encode(text_data))\n","\n","print(\"Characters:\", total_characters)\n","print(\"Tokens:\", total_tokens)\n","\n","class GPTDatasetV1(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Tokenize the entire text\n","        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","\n","        # Use a sliding window to chunk the book into overlapping sequences of max_length\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n","\n","def create_dataloader_v1(txt, batch_size=16, max_length=1024,\n","                         stride=512, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader\n","\n","# Train/validation ratio\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","train_data = text_data[:split_idx]\n","val_data = text_data[split_idx:]\n","\n","\n","torch.manual_seed(123)\n","\n","train_loader = create_dataloader_v1(\n","    train_data,\n","    batch_size=4,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    val_data,\n","    batch_size=4,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval();\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","    model.eval()\n","    with torch.no_grad():\n","        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n","        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n","    model.train()\n","    return train_loss, val_loss\n","\n","def generate_and_print_sample(model, tokenizer, device, start_context):\n","    model.eval()\n","    context_size = model.pos_emb.weight.shape[0]\n","    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n","    with torch.no_grad():\n","        token_ids = generate_text_simple(\n","            model=model, idx=encoded,\n","            max_new_tokens=50, context_size=context_size\n","        )\n","    decoded_text = token_ids_to_text(token_ids, tokenizer)\n","    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n","    model.train()\n","\n","def save_checkpoint(epoch, model, optimizer, save_path=\"model_checkpoint1.pth\"):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, save_path)\n","    print(f\"Checkpoint saved at epoch {epoch + 1}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_AzSuy1eNg4","executionInfo":{"status":"ok","timestamp":1743834977340,"user_tz":-330,"elapsed":41350,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"3c40f083-7c39-4dfe-d9a1-408be2413662"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Characters: 121892397\n","Tokens: 26316250\n"]}]},{"cell_type":"code","source":["model = GPTModel(GPT_CONFIG_124M)\n","model.load_state_dict(torch.load(\"foundation_model_v2.pth\", map_location=\"cpu\"))\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JUMwnl7ne3Mu","executionInfo":{"status":"ok","timestamp":1743834348337,"user_tz":-330,"elapsed":2167,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"05bc4e17-2dfc-4eb0-9967-20ceaa21f7f0"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPTModel(\n","  (tok_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(1024, 768)\n","  (drop_emb): Dropout(p=0.1, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (1): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (2): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (3): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (4): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (5): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (6): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (7): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (8): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (9): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (10): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (11): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["train_portion = int(len(data) * 0.85)  # 85% for training\n","test_portion = int(len(data) * 0.1)    # 10% for testing\n","val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n","\n","train_data = data[:train_portion]\n","test_data = data[train_portion:train_portion + test_portion]\n","val_data = data[train_portion + test_portion:]"],"metadata":{"id":"dx-ww-9Fozi9","executionInfo":{"status":"ok","timestamp":1743835032354,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","input_text = format_input(val_data[0]) # Changed val_data to data\n","print(input_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"js_y6TijiN7r","executionInfo":{"status":"ok","timestamp":1743834532390,"user_tz":-330,"elapsed":22,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"01ad1e47-932f-4c5a-f143-ed80ecc9e467"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Provide wellbeing insight for the below text with anxiety.\n","\n","### Input:\n","Normal? Ive been stuck in a panic attack cycle for about 7 months now. Constant. Once it stops it immediately picks back up again. Today, I prayed to God and asked him to help me, to take this pain away, and since then my internal shaking has stopped and I havent had any health anxiety thoughts. I havent felt like this is months and Im freaked out to have panic attacks and freaked out once I dont have them anymore. Help\n"]}]},{"cell_type":"code","source":["token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(input_text, tokenizer),\n","    max_new_tokens=35,\n","    context_size=GPT_CONFIG_124M[\"context_length\"],\n","    eos_id=50256,\n",")\n","generated_text = token_ids_to_text(token_ids, tokenizer)"],"metadata":{"id":"hh_JDf1tjqGQ","executionInfo":{"status":"ok","timestamp":1743834555599,"user_tz":-330,"elapsed":20385,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["response_text = generated_text[len(input_text):].strip()\n","print(response_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qFrtYtXj9bL","executionInfo":{"status":"ok","timestamp":1743834561504,"user_tz":-330,"elapsed":47,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}},"outputId":"8cb3185b-317c-4ee6-99a4-23d136af0f6e"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["me.\n","\n","### Response:\n","\n","Ive been stuck in a panic attack cycle for about 7 months now. Constant. Once it stops it immediately picks back up again\n"]}]},{"cell_type":"markdown","source":["# Finetuning the LLM"],"metadata":{"id":"neaenqetkDiU"}},{"cell_type":"code","source":["def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # Reduce the number of batches to match the total number of batches in the data loader\n","        # if num_batches exceeds the number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","        else:\n","            break\n","    return total_loss / num_batches\n","\n","def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","        # Print a sample text after each epoch\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context\n","        )\n","\n","    return train_losses, val_losses, track_tokens_seen\n"],"metadata":{"id":"aBNi85-ZkFc1","executionInfo":{"status":"ok","timestamp":1743835012506,"user_tz":-330,"elapsed":52,"user":{"displayName":"Dab Kumar Ghosh","userId":"09561287152521955261"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["model.to(device)\n","\n","torch.manual_seed(123)\n","\n","with torch.no_grad():\n","    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n","    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n","\n","print(\"Training loss:\", train_loss)\n","print(\"Validation loss:\", val_loss)"],"metadata":{"id":"3lNcaK4nkU3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","start_time = time.time()\n","\n","torch.manual_seed(123)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n","\n","num_epochs = 1\n","\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",")\n","\n","end_time = time.time()\n","execution_time_minutes = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wiVeslrrkVVQ","outputId":"2b2f8957-905e-42d4-a2b3-9eee71b8b9d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 10.737, Val loss 10.753\n","Ep 1 (Step 000005): Train loss 9.778, Val loss 9.767\n","Ep 1 (Step 000010): Train loss 9.415, Val loss 9.405\n","Ep 1 (Step 000015): Train loss 9.302, Val loss 9.197\n","Ep 1 (Step 000020): Train loss 9.010, Val loss 9.015\n","Ep 1 (Step 000025): Train loss 8.851, Val loss 8.849\n","Ep 1 (Step 000030): Train loss 8.751, Val loss 8.697\n","Ep 1 (Step 000035): Train loss 8.588, Val loss 8.556\n","Ep 1 (Step 000040): Train loss 8.436, Val loss 8.423\n","Ep 1 (Step 000045): Train loss 8.367, Val loss 8.312\n","Ep 1 (Step 000050): Train loss 8.273, Val loss 8.216\n","Ep 1 (Step 000055): Train loss 8.182, Val loss 8.120\n","Ep 1 (Step 000060): Train loss 8.084, Val loss 8.036\n","Ep 1 (Step 000065): Train loss 7.997, Val loss 7.969\n","Ep 1 (Step 000070): Train loss 7.887, Val loss 7.902\n","Ep 1 (Step 000075): Train loss 7.893, Val loss 7.848\n","Ep 1 (Step 000080): Train loss 7.817, Val loss 7.801\n","Ep 1 (Step 000085): Train loss 7.772, Val loss 7.763\n","Ep 1 (Step 000090): Train loss 7.597, Val loss 7.728\n","Ep 1 (Step 000095): Train loss 7.763, Val loss 7.696\n","Ep 1 (Step 000100): Train loss 7.695, Val loss 7.672\n","Ep 1 (Step 000105): Train loss 7.661, Val loss 7.655\n","Ep 1 (Step 000110): Train loss 7.711, Val loss 7.635\n","Ep 1 (Step 000115): Train loss 7.603, Val loss 7.618\n","Ep 1 (Step 000120): Train loss 7.728, Val loss 7.601\n","Ep 1 (Step 000125): Train loss 7.664, Val loss 7.586\n","Ep 1 (Step 000130): Train loss 7.637, Val loss 7.572\n","Ep 1 (Step 000135): Train loss 7.632, Val loss 7.563\n","Ep 1 (Step 000140): Train loss 7.499, Val loss 7.555\n","Ep 1 (Step 000145): Train loss 7.600, Val loss 7.539\n","Ep 1 (Step 000150): Train loss 7.450, Val loss 7.531\n","Ep 1 (Step 000155): Train loss 7.488, Val loss 7.516\n","Ep 1 (Step 000160): Train loss 7.464, Val loss 7.502\n","Ep 1 (Step 000165): Train loss 7.487, Val loss 7.496\n","Ep 1 (Step 000170): Train loss 7.526, Val loss 7.477\n","Ep 1 (Step 000175): Train loss 7.415, Val loss 7.468\n","Ep 1 (Step 000180): Train loss 7.446, Val loss 7.459\n","Ep 1 (Step 000185): Train loss 7.429, Val loss 7.452\n","Ep 1 (Step 000190): Train loss 7.459, Val loss 7.445\n","Ep 1 (Step 000195): Train loss 7.377, Val loss 7.432\n","Ep 1 (Step 000200): Train loss 7.417, Val loss 7.422\n","Ep 1 (Step 000205): Train loss 7.467, Val loss 7.412\n","Ep 1 (Step 000210): Train loss 7.246, Val loss 7.402\n","Ep 1 (Step 000215): Train loss 7.424, Val loss 7.399\n","Ep 1 (Step 000220): Train loss 7.376, Val loss 7.389\n","Ep 1 (Step 000225): Train loss 7.422, Val loss 7.374\n","Ep 1 (Step 000230): Train loss 7.418, Val loss 7.364\n","Ep 1 (Step 000235): Train loss 7.373, Val loss 7.355\n","Ep 1 (Step 000240): Train loss 7.375, Val loss 7.345\n","Ep 1 (Step 000245): Train loss 7.408, Val loss 7.335\n","Ep 1 (Step 000250): Train loss 7.330, Val loss 7.326\n","Ep 1 (Step 000255): Train loss 7.364, Val loss 7.320\n","Ep 1 (Step 000260): Train loss 7.393, Val loss 7.319\n","Ep 1 (Step 000265): Train loss 7.310, Val loss 7.311\n","Ep 1 (Step 000270): Train loss 7.353, Val loss 7.304\n","Ep 1 (Step 000275): Train loss 7.245, Val loss 7.297\n","Ep 1 (Step 000280): Train loss 7.301, Val loss 7.290\n","Ep 1 (Step 000285): Train loss 7.269, Val loss 7.283\n","Ep 1 (Step 000290): Train loss 7.243, Val loss 7.285\n","Ep 1 (Step 000295): Train loss 7.350, Val loss 7.273\n","Ep 1 (Step 000300): Train loss 7.398, Val loss 7.275\n","Ep 1 (Step 000305): Train loss 7.245, Val loss 7.262\n","Ep 1 (Step 000310): Train loss 7.293, Val loss 7.255\n","Ep 1 (Step 000315): Train loss 7.252, Val loss 7.250\n","Ep 1 (Step 000320): Train loss 7.381, Val loss 7.246\n","Ep 1 (Step 000325): Train loss 7.266, Val loss 7.248\n","Ep 1 (Step 000330): Train loss 7.226, Val loss 7.245\n","Ep 1 (Step 000335): Train loss 7.132, Val loss 7.247\n","Ep 1 (Step 000340): Train loss 7.233, Val loss 7.239\n","Ep 1 (Step 000345): Train loss 7.161, Val loss 7.234\n","Ep 1 (Step 000350): Train loss 7.146, Val loss 7.226\n","Ep 1 (Step 000355): Train loss 7.184, Val loss 7.225\n","Ep 1 (Step 000360): Train loss 7.202, Val loss 7.223\n","Ep 1 (Step 000365): Train loss 7.159, Val loss 7.204\n","Ep 1 (Step 000370): Train loss 7.304, Val loss 7.198\n","Ep 1 (Step 000375): Train loss 7.337, Val loss 7.188\n","Ep 1 (Step 000380): Train loss 7.235, Val loss 7.178\n","Ep 1 (Step 000385): Train loss 7.224, Val loss 7.168\n","Ep 1 (Step 000390): Train loss 7.205, Val loss 7.164\n","Ep 1 (Step 000395): Train loss 7.123, Val loss 7.167\n","Ep 1 (Step 000400): Train loss 7.236, Val loss 7.159\n","Ep 1 (Step 000405): Train loss 7.215, Val loss 7.155\n","Ep 1 (Step 000410): Train loss 7.184, Val loss 7.143\n","Ep 1 (Step 000415): Train loss 7.176, Val loss 7.142\n","Ep 1 (Step 000420): Train loss 7.225, Val loss 7.128\n","Ep 1 (Step 000425): Train loss 7.075, Val loss 7.129\n","Ep 1 (Step 000430): Train loss 7.102, Val loss 7.120\n","Ep 1 (Step 000435): Train loss 7.085, Val loss 7.120\n","Ep 1 (Step 000440): Train loss 7.050, Val loss 7.111\n","Ep 1 (Step 000445): Train loss 7.188, Val loss 7.111\n","Ep 1 (Step 000450): Train loss 7.157, Val loss 7.104\n","Ep 1 (Step 000455): Train loss 7.041, Val loss 7.101\n","Ep 1 (Step 000460): Train loss 7.213, Val loss 7.094\n","Ep 1 (Step 000465): Train loss 7.102, Val loss 7.087\n","Ep 1 (Step 000470): Train loss 7.129, Val loss 7.089\n","Ep 1 (Step 000475): Train loss 7.117, Val loss 7.088\n","Ep 1 (Step 000480): Train loss 7.122, Val loss 7.081\n","Ep 1 (Step 000485): Train loss 7.138, Val loss 7.075\n","Ep 1 (Step 000490): Train loss 7.030, Val loss 7.081\n","Ep 1 (Step 000495): Train loss 7.171, Val loss 7.072\n","Ep 1 (Step 000500): Train loss 7.100, Val loss 7.064\n","Ep 1 (Step 000505): Train loss 7.113, Val loss 7.061\n","Ep 1 (Step 000510): Train loss 7.135, Val loss 7.059\n","Ep 1 (Step 000515): Train loss 7.042, Val loss 7.058\n","Ep 1 (Step 000520): Train loss 7.012, Val loss 7.056\n","Ep 1 (Step 000525): Train loss 7.030, Val loss 7.060\n","Ep 1 (Step 000530): Train loss 7.013, Val loss 7.055\n","Ep 1 (Step 000535): Train loss 6.999, Val loss 7.047\n","Ep 1 (Step 000540): Train loss 7.089, Val loss 7.050\n","Ep 1 (Step 000545): Train loss 6.985, Val loss 7.042\n","Ep 1 (Step 000550): Train loss 6.973, Val loss 7.040\n","Ep 1 (Step 000555): Train loss 7.117, Val loss 7.039\n","Ep 1 (Step 000560): Train loss 7.238, Val loss 7.034\n","Ep 1 (Step 000565): Train loss 7.040, Val loss 7.030\n","Ep 1 (Step 000570): Train loss 7.100, Val loss 7.025\n","Ep 1 (Step 000575): Train loss 7.063, Val loss 7.029\n","Ep 1 (Step 000580): Train loss 7.094, Val loss 7.031\n","Ep 1 (Step 000585): Train loss 6.967, Val loss 7.021\n","Ep 1 (Step 000590): Train loss 7.016, Val loss 7.026\n","Ep 1 (Step 000595): Train loss 6.959, Val loss 7.021\n","Ep 1 (Step 000600): Train loss 6.984, Val loss 7.017\n","Ep 1 (Step 000605): Train loss 6.959, Val loss 7.016\n","Ep 1 (Step 000610): Train loss 7.042, Val loss 7.018\n","Ep 1 (Step 000615): Train loss 7.081, Val loss 7.019\n","Ep 1 (Step 000620): Train loss 7.036, Val loss 7.018\n","Ep 1 (Step 000625): Train loss 7.033, Val loss 7.001\n","Ep 1 (Step 000630): Train loss 6.984, Val loss 6.993\n","Ep 1 (Step 000635): Train loss 6.974, Val loss 6.992\n","Ep 1 (Step 000640): Train loss 6.875, Val loss 6.991\n","Ep 1 (Step 000645): Train loss 7.094, Val loss 6.993\n","Ep 1 (Step 000650): Train loss 6.987, Val loss 6.986\n","Ep 1 (Step 000655): Train loss 7.013, Val loss 6.991\n","Ep 1 (Step 000660): Train loss 7.034, Val loss 6.985\n","Ep 1 (Step 000665): Train loss 6.977, Val loss 6.977\n","Ep 1 (Step 000670): Train loss 6.965, Val loss 6.974\n","Ep 1 (Step 000675): Train loss 6.969, Val loss 6.970\n","Ep 1 (Step 000680): Train loss 6.923, Val loss 6.966\n","Ep 1 (Step 000685): Train loss 6.953, Val loss 6.967\n","Ep 1 (Step 000690): Train loss 7.071, Val loss 6.956\n","Ep 1 (Step 000695): Train loss 6.903, Val loss 6.950\n","Ep 1 (Step 000700): Train loss 7.013, Val loss 6.949\n","Ep 1 (Step 000705): Train loss 7.091, Val loss 6.946\n","Ep 1 (Step 000710): Train loss 7.002, Val loss 6.949\n","Ep 1 (Step 000715): Train loss 6.884, Val loss 6.946\n","Ep 1 (Step 000720): Train loss 6.983, Val loss 6.947\n","Ep 1 (Step 000725): Train loss 6.948, Val loss 6.939\n","Ep 1 (Step 000730): Train loss 6.990, Val loss 6.947\n","Ep 1 (Step 000735): Train loss 6.978, Val loss 6.929\n","Ep 1 (Step 000740): Train loss 7.073, Val loss 6.933\n","Ep 1 (Step 000745): Train loss 6.925, Val loss 6.929\n","Ep 1 (Step 000750): Train loss 6.937, Val loss 6.921\n","Ep 1 (Step 000755): Train loss 7.010, Val loss 6.923\n","Ep 1 (Step 000760): Train loss 6.956, Val loss 6.917\n","Ep 1 (Step 000765): Train loss 7.045, Val loss 6.933\n","Ep 1 (Step 000770): Train loss 6.931, Val loss 6.923\n","Ep 1 (Step 000775): Train loss 6.902, Val loss 6.920\n","Ep 1 (Step 000780): Train loss 6.954, Val loss 6.911\n","Ep 1 (Step 000785): Train loss 7.003, Val loss 6.904\n","Ep 1 (Step 000790): Train loss 6.888, Val loss 6.914\n","Ep 1 (Step 000795): Train loss 6.892, Val loss 6.909\n","Ep 1 (Step 000800): Train loss 6.936, Val loss 6.917\n","Ep 1 (Step 000805): Train loss 6.979, Val loss 6.908\n","Ep 1 (Step 000810): Train loss 6.899, Val loss 6.912\n","Ep 1 (Step 000815): Train loss 6.906, Val loss 6.904\n","Ep 1 (Step 000820): Train loss 6.913, Val loss 6.905\n","Ep 1 (Step 000825): Train loss 6.801, Val loss 6.889\n","Ep 1 (Step 000830): Train loss 6.930, Val loss 6.888\n","Ep 1 (Step 000835): Train loss 6.897, Val loss 6.893\n","Ep 1 (Step 000840): Train loss 6.829, Val loss 6.879\n","Ep 1 (Step 000845): Train loss 6.917, Val loss 6.875\n","Ep 1 (Step 000850): Train loss 6.879, Val loss 6.865\n","Ep 1 (Step 000855): Train loss 6.814, Val loss 6.865\n","Ep 1 (Step 000860): Train loss 6.917, Val loss 6.867\n","Ep 1 (Step 000865): Train loss 6.953, Val loss 6.872\n","Ep 1 (Step 000870): Train loss 6.847, Val loss 6.865\n","Ep 1 (Step 000875): Train loss 6.950, Val loss 6.862\n","Ep 1 (Step 000880): Train loss 6.842, Val loss 6.864\n","Ep 1 (Step 000885): Train loss 6.919, Val loss 6.855\n","Ep 1 (Step 000890): Train loss 6.896, Val loss 6.869\n","Ep 1 (Step 000895): Train loss 6.910, Val loss 6.848\n","Ep 1 (Step 000900): Train loss 7.023, Val loss 6.853\n","Ep 1 (Step 000905): Train loss 6.817, Val loss 6.845\n","Ep 1 (Step 000910): Train loss 6.887, Val loss 6.851\n","Ep 1 (Step 000915): Train loss 6.838, Val loss 6.844\n","Ep 1 (Step 000920): Train loss 6.859, Val loss 6.857\n","Ep 1 (Step 000925): Train loss 6.957, Val loss 6.838\n","Ep 1 (Step 000930): Train loss 6.861, Val loss 6.836\n","Ep 1 (Step 000935): Train loss 6.826, Val loss 6.832\n","Ep 1 (Step 000940): Train loss 6.889, Val loss 6.831\n","Ep 1 (Step 000945): Train loss 6.751, Val loss 6.827\n","Ep 1 (Step 000950): Train loss 6.892, Val loss 6.827\n","Ep 1 (Step 000955): Train loss 6.825, Val loss 6.826\n","Ep 1 (Step 000960): Train loss 6.849, Val loss 6.815\n","Ep 1 (Step 000965): Train loss 6.870, Val loss 6.815\n","Ep 1 (Step 000970): Train loss 7.026, Val loss 6.807\n","Ep 1 (Step 000975): Train loss 6.819, Val loss 6.804\n","Ep 1 (Step 000980): Train loss 6.892, Val loss 6.796\n","Ep 1 (Step 000985): Train loss 6.859, Val loss 6.806\n","Ep 1 (Step 000990): Train loss 6.752, Val loss 6.803\n","Ep 1 (Step 000995): Train loss 6.745, Val loss 6.801\n","Ep 1 (Step 001000): Train loss 6.806, Val loss 6.799\n","Ep 1 (Step 001005): Train loss 6.737, Val loss 6.805\n","Ep 1 (Step 001010): Train loss 6.853, Val loss 6.794\n","Ep 1 (Step 001015): Train loss 6.799, Val loss 6.787\n","Ep 1 (Step 001020): Train loss 6.781, Val loss 6.781\n","Ep 1 (Step 001025): Train loss 6.844, Val loss 6.792\n","Ep 1 (Step 001030): Train loss 6.787, Val loss 6.781\n","Ep 1 (Step 001035): Train loss 6.716, Val loss 6.777\n","Ep 1 (Step 001040): Train loss 6.786, Val loss 6.788\n","Ep 1 (Step 001045): Train loss 6.782, Val loss 6.767\n","Ep 1 (Step 001050): Train loss 6.840, Val loss 6.770\n","Ep 1 (Step 001055): Train loss 6.884, Val loss 6.772\n","Ep 1 (Step 001060): Train loss 6.763, Val loss 6.771\n","Ep 1 (Step 001065): Train loss 6.817, Val loss 6.775\n","Ep 1 (Step 001070): Train loss 6.812, Val loss 6.770\n","Ep 1 (Step 001075): Train loss 6.680, Val loss 6.766\n","Ep 1 (Step 001080): Train loss 6.786, Val loss 6.772\n","Ep 1 (Step 001085): Train loss 6.910, Val loss 6.768\n","Ep 1 (Step 001090): Train loss 6.924, Val loss 6.771\n","Ep 1 (Step 001095): Train loss 6.899, Val loss 6.756\n","Ep 1 (Step 001100): Train loss 6.779, Val loss 6.760\n","Ep 1 (Step 001105): Train loss 6.643, Val loss 6.750\n","Ep 1 (Step 001110): Train loss 6.755, Val loss 6.747\n","Ep 1 (Step 001115): Train loss 6.836, Val loss 6.741\n","Ep 1 (Step 001120): Train loss 6.673, Val loss 6.744\n","Ep 1 (Step 001125): Train loss 6.619, Val loss 6.737\n","Ep 1 (Step 001130): Train loss 6.709, Val loss 6.741\n","Ep 1 (Step 001135): Train loss 6.701, Val loss 6.739\n","Ep 1 (Step 001140): Train loss 6.837, Val loss 6.733\n","Ep 1 (Step 001145): Train loss 6.720, Val loss 6.744\n","Ep 1 (Step 001150): Train loss 6.839, Val loss 6.736\n","Ep 1 (Step 001155): Train loss 6.717, Val loss 6.734\n","Ep 1 (Step 001160): Train loss 6.790, Val loss 6.729\n","Ep 1 (Step 001165): Train loss 6.651, Val loss 6.731\n","Ep 1 (Step 001170): Train loss 6.813, Val loss 6.734\n","Ep 1 (Step 001175): Train loss 6.723, Val loss 6.735\n","Ep 1 (Step 001180): Train loss 6.727, Val loss 6.729\n","Ep 1 (Step 001185): Train loss 6.716, Val loss 6.736\n","Ep 1 (Step 001190): Train loss 6.689, Val loss 6.723\n","Ep 1 (Step 001195): Train loss 6.703, Val loss 6.726\n","Ep 1 (Step 001200): Train loss 6.779, Val loss 6.731\n","Ep 1 (Step 001205): Train loss 6.720, Val loss 6.722\n","Ep 1 (Step 001210): Train loss 6.817, Val loss 6.722\n","Ep 1 (Step 001215): Train loss 6.638, Val loss 6.715\n","Ep 1 (Step 001220): Train loss 6.713, Val loss 6.720\n","Ep 1 (Step 001225): Train loss 6.634, Val loss 6.711\n","Ep 1 (Step 001230): Train loss 6.838, Val loss 6.743\n","Ep 1 (Step 001235): Train loss 6.567, Val loss 6.712\n","Ep 1 (Step 001240): Train loss 6.613, Val loss 6.718\n","Ep 1 (Step 001245): Train loss 6.638, Val loss 6.706\n","Ep 1 (Step 001250): Train loss 6.749, Val loss 6.702\n","Ep 1 (Step 001255): Train loss 6.829, Val loss 6.705\n","Ep 1 (Step 001260): Train loss 6.760, Val loss 6.702\n","Ep 1 (Step 001265): Train loss 6.741, Val loss 6.699\n","Ep 1 (Step 001270): Train loss 6.774, Val loss 6.697\n","Ep 1 (Step 001275): Train loss 6.723, Val loss 6.700\n","Ep 1 (Step 001280): Train loss 6.776, Val loss 6.699\n","Ep 1 (Step 001285): Train loss 6.704, Val loss 6.695\n","Ep 1 (Step 001290): Train loss 6.725, Val loss 6.687\n","Ep 1 (Step 001295): Train loss 6.689, Val loss 6.689\n","Ep 1 (Step 001300): Train loss 6.670, Val loss 6.686\n","Ep 1 (Step 001305): Train loss 6.828, Val loss 6.678\n","Ep 1 (Step 001310): Train loss 6.696, Val loss 6.684\n","Ep 1 (Step 001315): Train loss 6.821, Val loss 6.677\n","Ep 1 (Step 001320): Train loss 6.651, Val loss 6.684\n","Ep 1 (Step 001325): Train loss 6.675, Val loss 6.678\n","Ep 1 (Step 001330): Train loss 6.649, Val loss 6.678\n","Ep 1 (Step 001335): Train loss 6.642, Val loss 6.675\n","Ep 1 (Step 001340): Train loss 6.639, Val loss 6.670\n","Ep 1 (Step 001345): Train loss 6.788, Val loss 6.679\n","Ep 1 (Step 001350): Train loss 6.627, Val loss 6.669\n","Ep 1 (Step 001355): Train loss 6.745, Val loss 6.672\n","Ep 1 (Step 001360): Train loss 6.746, Val loss 6.663\n","Ep 1 (Step 001365): Train loss 6.713, Val loss 6.667\n","Ep 1 (Step 001370): Train loss 6.863, Val loss 6.665\n","Ep 1 (Step 001375): Train loss 6.718, Val loss 6.658\n","Ep 1 (Step 001380): Train loss 6.703, Val loss 6.653\n","Ep 1 (Step 001385): Train loss 6.716, Val loss 6.657\n","Ep 1 (Step 001390): Train loss 6.555, Val loss 6.652\n","Ep 1 (Step 001395): Train loss 6.724, Val loss 6.648\n","Ep 1 (Step 001400): Train loss 6.709, Val loss 6.649\n","Ep 1 (Step 001405): Train loss 6.630, Val loss 6.648\n","Ep 1 (Step 001410): Train loss 6.661, Val loss 6.646\n","Ep 1 (Step 001415): Train loss 6.622, Val loss 6.641\n","Ep 1 (Step 001420): Train loss 6.627, Val loss 6.642\n","Ep 1 (Step 001425): Train loss 6.638, Val loss 6.635\n","Ep 1 (Step 001430): Train loss 6.626, Val loss 6.641\n","Ep 1 (Step 001435): Train loss 6.512, Val loss 6.646\n","Ep 1 (Step 001440): Train loss 6.583, Val loss 6.644\n","Ep 1 (Step 001445): Train loss 6.574, Val loss 6.640\n","Ep 1 (Step 001450): Train loss 6.575, Val loss 6.649\n","Ep 1 (Step 001455): Train loss 6.663, Val loss 6.638\n","Ep 1 (Step 001460): Train loss 6.652, Val loss 6.641\n","Ep 1 (Step 001465): Train loss 6.531, Val loss 6.644\n","Ep 1 (Step 001470): Train loss 6.705, Val loss 6.638\n","Ep 1 (Step 001475): Train loss 6.652, Val loss 6.634\n","Ep 1 (Step 001480): Train loss 6.635, Val loss 6.640\n","Ep 1 (Step 001485): Train loss 6.665, Val loss 6.635\n","Ep 1 (Step 001490): Train loss 6.702, Val loss 6.639\n","Ep 1 (Step 001495): Train loss 6.601, Val loss 6.638\n","Ep 1 (Step 001500): Train loss 6.601, Val loss 6.632\n","Ep 1 (Step 001505): Train loss 6.752, Val loss 6.639\n","Ep 1 (Step 001510): Train loss 6.733, Val loss 6.638\n","Ep 1 (Step 001515): Train loss 6.619, Val loss 6.631\n","Ep 1 (Step 001520): Train loss 6.609, Val loss 6.627\n","Ep 1 (Step 001525): Train loss 6.637, Val loss 6.627\n","Ep 1 (Step 001530): Train loss 6.699, Val loss 6.629\n","Ep 1 (Step 001535): Train loss 6.654, Val loss 6.629\n","Ep 1 (Step 001540): Train loss 6.516, Val loss 6.626\n","Ep 1 (Step 001545): Train loss 6.534, Val loss 6.614\n","Ep 1 (Step 001550): Train loss 6.728, Val loss 6.614\n","Ep 1 (Step 001555): Train loss 6.683, Val loss 6.611\n","Ep 1 (Step 001560): Train loss 6.629, Val loss 6.612\n","Ep 1 (Step 001565): Train loss 6.617, Val loss 6.603\n","Ep 1 (Step 001570): Train loss 6.540, Val loss 6.604\n","Ep 1 (Step 001575): Train loss 6.662, Val loss 6.602\n","Ep 1 (Step 001580): Train loss 6.630, Val loss 6.611\n","Ep 1 (Step 001585): Train loss 6.683, Val loss 6.620\n","Ep 1 (Step 001590): Train loss 6.651, Val loss 6.618\n","Ep 1 (Step 001595): Train loss 6.575, Val loss 6.620\n","Ep 1 (Step 001600): Train loss 6.760, Val loss 6.603\n","Ep 1 (Step 001605): Train loss 6.596, Val loss 6.616\n","Ep 1 (Step 001610): Train loss 6.478, Val loss 6.599\n","Ep 1 (Step 001615): Train loss 6.589, Val loss 6.601\n","Ep 1 (Step 001620): Train loss 6.561, Val loss 6.607\n","Ep 1 (Step 001625): Train loss 6.656, Val loss 6.599\n","Ep 1 (Step 001630): Train loss 6.669, Val loss 6.599\n","Ep 1 (Step 001635): Train loss 6.562, Val loss 6.600\n","Ep 1 (Step 001640): Train loss 6.625, Val loss 6.612\n","Ep 1 (Step 001645): Train loss 6.656, Val loss 6.604\n","Ep 1 (Step 001650): Train loss 6.556, Val loss 6.601\n","Ep 1 (Step 001655): Train loss 6.503, Val loss 6.595\n","Ep 1 (Step 001660): Train loss 6.723, Val loss 6.594\n","Ep 1 (Step 001665): Train loss 6.531, Val loss 6.596\n","Ep 1 (Step 001670): Train loss 6.574, Val loss 6.595\n","Ep 1 (Step 001675): Train loss 6.581, Val loss 6.595\n","Ep 1 (Step 001680): Train loss 6.593, Val loss 6.588\n","Ep 1 (Step 001685): Train loss 6.528, Val loss 6.612\n","Ep 1 (Step 001690): Train loss 6.544, Val loss 6.585\n","Ep 1 (Step 001695): Train loss 6.494, Val loss 6.583\n","Ep 1 (Step 001700): Train loss 6.600, Val loss 6.589\n","Ep 1 (Step 001705): Train loss 6.487, Val loss 6.595\n","Ep 1 (Step 001710): Train loss 6.455, Val loss 6.592\n","Ep 1 (Step 001715): Train loss 6.580, Val loss 6.586\n","Ep 1 (Step 001720): Train loss 6.497, Val loss 6.583\n","Ep 1 (Step 001725): Train loss 6.508, Val loss 6.585\n","Ep 1 (Step 001730): Train loss 6.533, Val loss 6.580\n","Ep 1 (Step 001735): Train loss 6.593, Val loss 6.573\n","Ep 1 (Step 001740): Train loss 6.498, Val loss 6.584\n","Ep 1 (Step 001745): Train loss 6.494, Val loss 6.584\n","Ep 1 (Step 001750): Train loss 6.571, Val loss 6.579\n","Ep 1 (Step 001755): Train loss 6.620, Val loss 6.584\n","Ep 1 (Step 001760): Train loss 6.612, Val loss 6.590\n","Ep 1 (Step 001765): Train loss 6.578, Val loss 6.584\n","Ep 1 (Step 001770): Train loss 6.499, Val loss 6.585\n","Ep 1 (Step 001775): Train loss 6.694, Val loss 6.586\n","Ep 1 (Step 001780): Train loss 6.554, Val loss 6.582\n","Ep 1 (Step 001785): Train loss 6.498, Val loss 6.585\n","Ep 1 (Step 001790): Train loss 6.500, Val loss 6.579\n","Ep 1 (Step 001795): Train loss 6.450, Val loss 6.571\n","Ep 1 (Step 001800): Train loss 6.465, Val loss 6.570\n","Ep 1 (Step 001805): Train loss 6.548, Val loss 6.564\n","Ep 1 (Step 001810): Train loss 6.516, Val loss 6.562\n","Ep 1 (Step 001815): Train loss 6.504, Val loss 6.566\n","Ep 1 (Step 001820): Train loss 6.519, Val loss 6.570\n","Ep 1 (Step 001825): Train loss 6.525, Val loss 6.569\n","Ep 1 (Step 001830): Train loss 6.345, Val loss 6.575\n","Ep 1 (Step 001835): Train loss 6.565, Val loss 6.561\n","Ep 1 (Step 001840): Train loss 6.421, Val loss 6.554\n","Ep 1 (Step 001845): Train loss 6.406, Val loss 6.548\n","Ep 1 (Step 001850): Train loss 6.510, Val loss 6.555\n","Ep 1 (Step 001855): Train loss 6.429, Val loss 6.545\n","Ep 1 (Step 001860): Train loss 6.576, Val loss 6.561\n","Ep 1 (Step 001865): Train loss 6.584, Val loss 6.550\n","Ep 1 (Step 001870): Train loss 6.425, Val loss 6.549\n","Ep 1 (Step 001875): Train loss 6.376, Val loss 6.560\n","Ep 1 (Step 001880): Train loss 6.717, Val loss 6.545\n","Ep 1 (Step 001885): Train loss 6.423, Val loss 6.547\n","Ep 1 (Step 001890): Train loss 6.516, Val loss 6.544\n","Ep 1 (Step 001895): Train loss 6.510, Val loss 6.533\n","Ep 1 (Step 001900): Train loss 6.567, Val loss 6.534\n","Ep 1 (Step 001905): Train loss 6.409, Val loss 6.537\n","Ep 1 (Step 001910): Train loss 6.608, Val loss 6.532\n","Ep 1 (Step 001915): Train loss 6.583, Val loss 6.529\n","Ep 1 (Step 001920): Train loss 6.436, Val loss 6.533\n","Ep 1 (Step 001925): Train loss 6.416, Val loss 6.526\n","Ep 1 (Step 001930): Train loss 6.455, Val loss 6.526\n","Ep 1 (Step 001935): Train loss 6.437, Val loss 6.522\n","Ep 1 (Step 001940): Train loss 6.495, Val loss 6.522\n","Ep 1 (Step 001945): Train loss 6.570, Val loss 6.522\n","Ep 1 (Step 001950): Train loss 6.604, Val loss 6.529\n","Ep 1 (Step 001955): Train loss 6.534, Val loss 6.523\n","Ep 1 (Step 001960): Train loss 6.530, Val loss 6.524\n","Ep 1 (Step 001965): Train loss 6.434, Val loss 6.521\n","Ep 1 (Step 001970): Train loss 6.510, Val loss 6.529\n","Ep 1 (Step 001975): Train loss 6.378, Val loss 6.520\n","Ep 1 (Step 001980): Train loss 6.553, Val loss 6.523\n","Ep 1 (Step 001985): Train loss 6.455, Val loss 6.512\n","Ep 1 (Step 001990): Train loss 6.554, Val loss 6.512\n","Ep 1 (Step 001995): Train loss 6.495, Val loss 6.520\n","Ep 1 (Step 002000): Train loss 6.540, Val loss 6.511\n","Ep 1 (Step 002005): Train loss 6.461, Val loss 6.514\n","Ep 1 (Step 002010): Train loss 6.508, Val loss 6.510\n","Ep 1 (Step 002015): Train loss 6.539, Val loss 6.507\n","Ep 1 (Step 002020): Train loss 6.550, Val loss 6.515\n","Ep 1 (Step 002025): Train loss 6.460, Val loss 6.501\n","Ep 1 (Step 002030): Train loss 6.447, Val loss 6.505\n","Ep 1 (Step 002035): Train loss 6.483, Val loss 6.501\n","Ep 1 (Step 002040): Train loss 6.579, Val loss 6.501\n","Ep 1 (Step 002045): Train loss 6.512, Val loss 6.500\n","Ep 1 (Step 002050): Train loss 6.553, Val loss 6.496\n","Ep 1 (Step 002055): Train loss 6.487, Val loss 6.499\n","Ep 1 (Step 002060): Train loss 6.512, Val loss 6.509\n","Ep 1 (Step 002065): Train loss 6.398, Val loss 6.505\n","Ep 1 (Step 002070): Train loss 6.374, Val loss 6.501\n","Ep 1 (Step 002075): Train loss 6.495, Val loss 6.509\n","Ep 1 (Step 002080): Train loss 6.458, Val loss 6.514\n","Ep 1 (Step 002085): Train loss 6.522, Val loss 6.510\n","Ep 1 (Step 002090): Train loss 6.441, Val loss 6.516\n","Ep 1 (Step 002095): Train loss 6.499, Val loss 6.502\n","Ep 1 (Step 002100): Train loss 6.464, Val loss 6.499\n","Ep 1 (Step 002105): Train loss 6.477, Val loss 6.498\n","Ep 1 (Step 002110): Train loss 6.501, Val loss 6.497\n","Ep 1 (Step 002115): Train loss 6.537, Val loss 6.493\n","Ep 1 (Step 002120): Train loss 6.457, Val loss 6.498\n","Ep 1 (Step 002125): Train loss 6.406, Val loss 6.493\n","Ep 1 (Step 002130): Train loss 6.237, Val loss 6.493\n","Ep 1 (Step 002135): Train loss 6.401, Val loss 6.490\n","Ep 1 (Step 002140): Train loss 6.378, Val loss 6.491\n","Ep 1 (Step 002145): Train loss 6.455, Val loss 6.488\n","Ep 1 (Step 002150): Train loss 6.333, Val loss 6.489\n","Ep 1 (Step 002155): Train loss 6.579, Val loss 6.495\n","Ep 1 (Step 002160): Train loss 6.518, Val loss 6.489\n","Ep 1 (Step 002165): Train loss 6.448, Val loss 6.489\n","Ep 1 (Step 002170): Train loss 6.331, Val loss 6.500\n","Ep 1 (Step 002175): Train loss 6.334, Val loss 6.491\n","Ep 1 (Step 002180): Train loss 6.451, Val loss 6.489\n","Ep 1 (Step 002185): Train loss 6.344, Val loss 6.485\n","Ep 1 (Step 002190): Train loss 6.482, Val loss 6.474\n","Ep 1 (Step 002195): Train loss 6.469, Val loss 6.484\n","Ep 1 (Step 002200): Train loss 6.416, Val loss 6.482\n","Ep 1 (Step 002205): Train loss 6.365, Val loss 6.488\n","Ep 1 (Step 002210): Train loss 6.391, Val loss 6.488\n","Ep 1 (Step 002215): Train loss 6.402, Val loss 6.494\n","Ep 1 (Step 002220): Train loss 6.394, Val loss 6.484\n","Ep 1 (Step 002225): Train loss 6.476, Val loss 6.487\n","Ep 1 (Step 002230): Train loss 6.432, Val loss 6.486\n","Ep 1 (Step 002235): Train loss 6.421, Val loss 6.479\n","Ep 1 (Step 002240): Train loss 6.443, Val loss 6.485\n","Ep 1 (Step 002245): Train loss 6.449, Val loss 6.478\n","Ep 1 (Step 002250): Train loss 6.346, Val loss 6.477\n","Ep 1 (Step 002255): Train loss 6.515, Val loss 6.477\n","Ep 1 (Step 002260): Train loss 6.345, Val loss 6.474\n","Ep 1 (Step 002265): Train loss 6.321, Val loss 6.474\n","Ep 1 (Step 002270): Train loss 6.490, Val loss 6.467\n","Ep 1 (Step 002275): Train loss 6.417, Val loss 6.464\n","Ep 1 (Step 002280): Train loss 6.363, Val loss 6.466\n","Ep 1 (Step 002285): Train loss 6.351, Val loss 6.466\n","Ep 1 (Step 002290): Train loss 6.382, Val loss 6.467\n","Ep 1 (Step 002295): Train loss 6.455, Val loss 6.468\n","Ep 1 (Step 002300): Train loss 6.559, Val loss 6.473\n","Ep 1 (Step 002305): Train loss 6.434, Val loss 6.475\n","Ep 1 (Step 002310): Train loss 6.434, Val loss 6.463\n","Ep 1 (Step 002315): Train loss 6.329, Val loss 6.481\n","Ep 1 (Step 002320): Train loss 6.422, Val loss 6.468\n","Ep 1 (Step 002325): Train loss 6.340, Val loss 6.471\n","Ep 1 (Step 002330): Train loss 6.388, Val loss 6.464\n","Ep 1 (Step 002335): Train loss 6.402, Val loss 6.471\n","Ep 1 (Step 002340): Train loss 6.499, Val loss 6.461\n","Ep 1 (Step 002345): Train loss 6.449, Val loss 6.466\n","Ep 1 (Step 002350): Train loss 6.456, Val loss 6.455\n","Ep 1 (Step 002355): Train loss 6.416, Val loss 6.455\n","Ep 1 (Step 002360): Train loss 6.397, Val loss 6.450\n","Ep 1 (Step 002365): Train loss 6.360, Val loss 6.446\n","Ep 1 (Step 002370): Train loss 6.440, Val loss 6.441\n","Ep 1 (Step 002375): Train loss 6.429, Val loss 6.444\n","Ep 1 (Step 002380): Train loss 6.351, Val loss 6.442\n","Ep 1 (Step 002385): Train loss 6.367, Val loss 6.450\n","Ep 1 (Step 002390): Train loss 6.407, Val loss 6.438\n","Ep 1 (Step 002395): Train loss 6.211, Val loss 6.436\n","Ep 1 (Step 002400): Train loss 6.317, Val loss 6.441\n","Ep 1 (Step 002405): Train loss 6.355, Val loss 6.436\n","Ep 1 (Step 002410): Train loss 6.412, Val loss 6.444\n","Ep 1 (Step 002415): Train loss 6.353, Val loss 6.445\n","Ep 1 (Step 002420): Train loss 6.396, Val loss 6.437\n","Ep 1 (Step 002425): Train loss 6.400, Val loss 6.438\n","Ep 1 (Step 002430): Train loss 6.271, Val loss 6.442\n","Ep 1 (Step 002435): Train loss 6.362, Val loss 6.447\n","Ep 1 (Step 002440): Train loss 6.290, Val loss 6.435\n","Ep 1 (Step 002445): Train loss 6.226, Val loss 6.432\n","Ep 1 (Step 002450): Train loss 6.435, Val loss 6.439\n","Ep 1 (Step 002455): Train loss 6.370, Val loss 6.432\n","Ep 1 (Step 002460): Train loss 6.429, Val loss 6.425\n","Ep 1 (Step 002465): Train loss 6.395, Val loss 6.446\n","Ep 1 (Step 002470): Train loss 6.390, Val loss 6.429\n","Ep 1 (Step 002475): Train loss 6.443, Val loss 6.426\n","Ep 1 (Step 002480): Train loss 6.331, Val loss 6.426\n","Ep 1 (Step 002485): Train loss 6.469, Val loss 6.425\n","Ep 1 (Step 002490): Train loss 6.383, Val loss 6.429\n","Ep 1 (Step 002495): Train loss 6.358, Val loss 6.429\n","Ep 1 (Step 002500): Train loss 6.377, Val loss 6.427\n","Ep 1 (Step 002505): Train loss 6.388, Val loss 6.437\n","Ep 1 (Step 002510): Train loss 6.354, Val loss 6.431\n","Ep 1 (Step 002515): Train loss 6.313, Val loss 6.426\n","Ep 1 (Step 002520): Train loss 6.318, Val loss 6.432\n","Ep 1 (Step 002525): Train loss 6.363, Val loss 6.430\n","Ep 1 (Step 002530): Train loss 6.286, Val loss 6.438\n","Ep 1 (Step 002535): Train loss 6.390, Val loss 6.449\n","Ep 1 (Step 002540): Train loss 6.344, Val loss 6.441\n","Ep 1 (Step 002545): Train loss 6.430, Val loss 6.431\n","Ep 1 (Step 002550): Train loss 6.291, Val loss 6.429\n","Ep 1 (Step 002555): Train loss 6.356, Val loss 6.431\n","Ep 1 (Step 002560): Train loss 6.410, Val loss 6.417\n","Ep 1 (Step 002565): Train loss 6.359, Val loss 6.424\n","Ep 1 (Step 002570): Train loss 6.315, Val loss 6.421\n","Ep 1 (Step 002575): Train loss 6.207, Val loss 6.424\n","Ep 1 (Step 002580): Train loss 6.248, Val loss 6.425\n","Ep 1 (Step 002585): Train loss 6.303, Val loss 6.417\n","Ep 1 (Step 002590): Train loss 6.349, Val loss 6.416\n","Ep 1 (Step 002595): Train loss 6.225, Val loss 6.427\n","Ep 1 (Step 002600): Train loss 6.335, Val loss 6.423\n","Ep 1 (Step 002605): Train loss 6.289, Val loss 6.424\n","Ep 1 (Step 002610): Train loss 6.411, Val loss 6.419\n","Ep 1 (Step 002615): Train loss 6.269, Val loss 6.419\n","Ep 1 (Step 002620): Train loss 6.348, Val loss 6.415\n","Ep 1 (Step 002625): Train loss 6.376, Val loss 6.418\n","Ep 1 (Step 002630): Train loss 6.379, Val loss 6.412\n","Ep 1 (Step 002635): Train loss 6.360, Val loss 6.411\n","Ep 1 (Step 002640): Train loss 6.438, Val loss 6.409\n","Ep 1 (Step 002645): Train loss 6.325, Val loss 6.406\n","Ep 1 (Step 002650): Train loss 6.381, Val loss 6.419\n","Ep 1 (Step 002655): Train loss 6.229, Val loss 6.405\n","Ep 1 (Step 002660): Train loss 6.356, Val loss 6.407\n","Ep 1 (Step 002665): Train loss 6.313, Val loss 6.409\n","Ep 1 (Step 002670): Train loss 6.356, Val loss 6.411\n","Ep 1 (Step 002675): Train loss 6.378, Val loss 6.407\n","Ep 1 (Step 002680): Train loss 6.332, Val loss 6.398\n","Ep 1 (Step 002685): Train loss 6.254, Val loss 6.393\n","Ep 1 (Step 002690): Train loss 6.456, Val loss 6.406\n","Ep 1 (Step 002695): Train loss 6.459, Val loss 6.396\n","Ep 1 (Step 002700): Train loss 6.259, Val loss 6.401\n","Ep 1 (Step 002705): Train loss 6.380, Val loss 6.399\n","Ep 1 (Step 002710): Train loss 6.301, Val loss 6.391\n","Ep 1 (Step 002715): Train loss 6.300, Val loss 6.392\n","Ep 1 (Step 002720): Train loss 6.426, Val loss 6.387\n","Ep 1 (Step 002725): Train loss 6.249, Val loss 6.386\n","Ep 1 (Step 002730): Train loss 6.330, Val loss 6.392\n","Ep 1 (Step 002735): Train loss 6.331, Val loss 6.381\n","Ep 1 (Step 002740): Train loss 6.288, Val loss 6.376\n","Ep 1 (Step 002745): Train loss 6.329, Val loss 6.389\n","Ep 1 (Step 002750): Train loss 6.285, Val loss 6.383\n","Ep 1 (Step 002755): Train loss 6.294, Val loss 6.383\n","Ep 1 (Step 002760): Train loss 6.293, Val loss 6.381\n","Ep 1 (Step 002765): Train loss 6.423, Val loss 6.383\n","Ep 1 (Step 002770): Train loss 6.276, Val loss 6.385\n","Ep 1 (Step 002775): Train loss 6.354, Val loss 6.387\n","Ep 1 (Step 002780): Train loss 6.356, Val loss 6.388\n","Ep 1 (Step 002785): Train loss 6.380, Val loss 6.383\n","Ep 1 (Step 002790): Train loss 6.291, Val loss 6.394\n","Ep 1 (Step 002795): Train loss 6.362, Val loss 6.384\n","Ep 1 (Step 002800): Train loss 6.238, Val loss 6.380\n","Ep 1 (Step 002805): Train loss 6.266, Val loss 6.374\n","Ep 1 (Step 002810): Train loss 6.307, Val loss 6.379\n","Ep 1 (Step 002815): Train loss 6.292, Val loss 6.376\n","Ep 1 (Step 002820): Train loss 6.429, Val loss 6.373\n","Ep 1 (Step 002825): Train loss 6.229, Val loss 6.379\n","Ep 1 (Step 002830): Train loss 6.283, Val loss 6.382\n","Ep 1 (Step 002835): Train loss 6.172, Val loss 6.371\n","Ep 1 (Step 002840): Train loss 6.344, Val loss 6.369\n","Ep 1 (Step 002845): Train loss 6.291, Val loss 6.370\n","Ep 1 (Step 002850): Train loss 6.334, Val loss 6.367\n","Ep 1 (Step 002855): Train loss 6.301, Val loss 6.374\n","Ep 1 (Step 002860): Train loss 6.246, Val loss 6.369\n","Ep 1 (Step 002865): Train loss 6.233, Val loss 6.369\n","Ep 1 (Step 002870): Train loss 6.286, Val loss 6.385\n","Ep 1 (Step 002875): Train loss 6.216, Val loss 6.377\n","Ep 1 (Step 002880): Train loss 6.334, Val loss 6.367\n","Ep 1 (Step 002885): Train loss 6.376, Val loss 6.394\n","Ep 1 (Step 002890): Train loss 6.304, Val loss 6.375\n","Ep 1 (Step 002895): Train loss 6.328, Val loss 6.370\n","Ep 1 (Step 002900): Train loss 6.359, Val loss 6.372\n","Ep 1 (Step 002905): Train loss 6.314, Val loss 6.385\n","Ep 1 (Step 002910): Train loss 6.293, Val loss 6.376\n","Ep 1 (Step 002915): Train loss 6.259, Val loss 6.372\n","Ep 1 (Step 002920): Train loss 6.207, Val loss 6.371\n","Ep 1 (Step 002925): Train loss 6.265, Val loss 6.368\n","Ep 1 (Step 002930): Train loss 6.344, Val loss 6.379\n","Ep 1 (Step 002935): Train loss 6.259, Val loss 6.361\n","Ep 1 (Step 002940): Train loss 6.189, Val loss 6.370\n","Ep 1 (Step 002945): Train loss 6.296, Val loss 6.360\n","Ep 1 (Step 002950): Train loss 6.272, Val loss 6.362\n","Ep 1 (Step 002955): Train loss 6.191, Val loss 6.377\n","Ep 1 (Step 002960): Train loss 6.301, Val loss 6.351\n","Ep 1 (Step 002965): Train loss 6.279, Val loss 6.360\n","Ep 1 (Step 002970): Train loss 6.278, Val loss 6.354\n","Ep 1 (Step 002975): Train loss 6.232, Val loss 6.369\n","Ep 1 (Step 002980): Train loss 6.248, Val loss 6.361\n","Ep 1 (Step 002985): Train loss 6.299, Val loss 6.352\n","Ep 1 (Step 002990): Train loss 6.248, Val loss 6.357\n","Ep 1 (Step 002995): Train loss 6.207, Val loss 6.356\n","Ep 1 (Step 003000): Train loss 6.269, Val loss 6.350\n","Ep 1 (Step 003005): Train loss 6.187, Val loss 6.357\n","Ep 1 (Step 003010): Train loss 6.309, Val loss 6.356\n","Ep 1 (Step 003015): Train loss 6.258, Val loss 6.355\n","Ep 1 (Step 003020): Train loss 6.379, Val loss 6.345\n","Ep 1 (Step 003025): Train loss 6.235, Val loss 6.352\n","Ep 1 (Step 003030): Train loss 6.113, Val loss 6.342\n","Ep 1 (Step 003035): Train loss 6.312, Val loss 6.340\n","Ep 1 (Step 003040): Train loss 6.129, Val loss 6.347\n","Ep 1 (Step 003045): Train loss 6.164, Val loss 6.340\n","Ep 1 (Step 003050): Train loss 6.316, Val loss 6.338\n","Ep 1 (Step 003055): Train loss 6.229, Val loss 6.335\n","Ep 1 (Step 003060): Train loss 6.186, Val loss 6.337\n","Ep 1 (Step 003065): Train loss 6.068, Val loss 6.336\n","Ep 1 (Step 003070): Train loss 6.274, Val loss 6.345\n","Ep 1 (Step 003075): Train loss 6.181, Val loss 6.336\n","Ep 1 (Step 003080): Train loss 6.346, Val loss 6.330\n","Ep 1 (Step 003085): Train loss 6.258, Val loss 6.332\n","Ep 1 (Step 003090): Train loss 6.244, Val loss 6.340\n","Ep 1 (Step 003095): Train loss 6.251, Val loss 6.341\n","Ep 1 (Step 003100): Train loss 6.164, Val loss 6.334\n","Ep 1 (Step 003105): Train loss 6.231, Val loss 6.335\n","Ep 1 (Step 003110): Train loss 6.182, Val loss 6.331\n","Ep 1 (Step 003115): Train loss 6.218, Val loss 6.334\n","Ep 1 (Step 003120): Train loss 6.260, Val loss 6.334\n","Ep 1 (Step 003125): Train loss 6.508, Val loss 6.357\n","Ep 1 (Step 003130): Train loss 6.263, Val loss 6.351\n","Ep 1 (Step 003135): Train loss 6.261, Val loss 6.336\n","Ep 1 (Step 003140): Train loss 6.115, Val loss 6.335\n","Ep 1 (Step 003145): Train loss 6.263, Val loss 6.331\n","Ep 1 (Step 003150): Train loss 6.350, Val loss 6.340\n","Ep 1 (Step 003155): Train loss 6.240, Val loss 6.331\n","Ep 1 (Step 003160): Train loss 6.238, Val loss 6.334\n","Ep 1 (Step 003165): Train loss 6.338, Val loss 6.328\n","Ep 1 (Step 003170): Train loss 6.249, Val loss 6.323\n","Ep 1 (Step 003175): Train loss 6.242, Val loss 6.316\n","Ep 1 (Step 003180): Train loss 6.240, Val loss 6.318\n","Ep 1 (Step 003185): Train loss 6.281, Val loss 6.322\n","Ep 1 (Step 003190): Train loss 6.329, Val loss 6.307\n","Ep 1 (Step 003195): Train loss 6.271, Val loss 6.313\n","Ep 1 (Step 003200): Train loss 6.191, Val loss 6.314\n","Ep 1 (Step 003205): Train loss 6.176, Val loss 6.318\n","Ep 1 (Step 003210): Train loss 6.332, Val loss 6.307\n","Ep 1 (Step 003215): Train loss 6.200, Val loss 6.313\n","Ep 1 (Step 003220): Train loss 6.236, Val loss 6.307\n","Ep 1 (Step 003225): Train loss 6.265, Val loss 6.328\n","Ep 1 (Step 003230): Train loss 6.279, Val loss 6.304\n","Ep 1 (Step 003235): Train loss 6.199, Val loss 6.303\n","Ep 1 (Step 003240): Train loss 6.370, Val loss 6.318\n","Ep 1 (Step 003245): Train loss 6.306, Val loss 6.305\n","Ep 1 (Step 003250): Train loss 6.130, Val loss 6.314\n","Ep 1 (Step 003255): Train loss 6.146, Val loss 6.313\n","Ep 1 (Step 003260): Train loss 6.295, Val loss 6.306\n","Ep 1 (Step 003265): Train loss 6.176, Val loss 6.323\n","Ep 1 (Step 003270): Train loss 6.263, Val loss 6.307\n","Ep 1 (Step 003275): Train loss 6.207, Val loss 6.310\n","Ep 1 (Step 003280): Train loss 6.257, Val loss 6.304\n","Ep 1 (Step 003285): Train loss 6.357, Val loss 6.296\n","Ep 1 (Step 003290): Train loss 6.262, Val loss 6.300\n","Ep 1 (Step 003295): Train loss 6.306, Val loss 6.301\n","Ep 1 (Step 003300): Train loss 6.164, Val loss 6.316\n","Ep 1 (Step 003305): Train loss 6.242, Val loss 6.296\n","Ep 1 (Step 003310): Train loss 6.227, Val loss 6.299\n","Ep 1 (Step 003315): Train loss 6.161, Val loss 6.298\n","Ep 1 (Step 003320): Train loss 6.332, Val loss 6.287\n","Ep 1 (Step 003325): Train loss 6.230, Val loss 6.296\n","Ep 1 (Step 003330): Train loss 6.109, Val loss 6.304\n","Ep 1 (Step 003335): Train loss 6.097, Val loss 6.284\n","Ep 1 (Step 003340): Train loss 6.285, Val loss 6.286\n","Ep 1 (Step 003345): Train loss 6.207, Val loss 6.278\n","Ep 1 (Step 003350): Train loss 6.233, Val loss 6.282\n","Ep 1 (Step 003355): Train loss 6.177, Val loss 6.282\n","Ep 1 (Step 003360): Train loss 6.200, Val loss 6.290\n","Ep 1 (Step 003365): Train loss 6.321, Val loss 6.284\n","Ep 1 (Step 003370): Train loss 6.220, Val loss 6.291\n","Ep 1 (Step 003375): Train loss 6.245, Val loss 6.285\n","Ep 1 (Step 003380): Train loss 6.236, Val loss 6.274\n","Ep 1 (Step 003385): Train loss 6.014, Val loss 6.286\n","Ep 1 (Step 003390): Train loss 6.194, Val loss 6.287\n","Ep 1 (Step 003395): Train loss 6.139, Val loss 6.283\n","Ep 1 (Step 003400): Train loss 6.186, Val loss 6.276\n","Ep 1 (Step 003405): Train loss 6.146, Val loss 6.286\n","Ep 1 (Step 003410): Train loss 6.052, Val loss 6.290\n","Ep 1 (Step 003415): Train loss 6.119, Val loss 6.295\n","Ep 1 (Step 003420): Train loss 6.164, Val loss 6.278\n","Ep 1 (Step 003425): Train loss 6.219, Val loss 6.284\n","Ep 1 (Step 003430): Train loss 6.274, Val loss 6.281\n","Ep 1 (Step 003435): Train loss 6.191, Val loss 6.280\n","Ep 1 (Step 003440): Train loss 6.215, Val loss 6.288\n","Ep 1 (Step 003445): Train loss 6.219, Val loss 6.287\n","Ep 1 (Step 003450): Train loss 6.204, Val loss 6.302\n","Ep 1 (Step 003455): Train loss 6.105, Val loss 6.300\n","Ep 1 (Step 003460): Train loss 5.881, Val loss 6.276\n","Ep 1 (Step 003465): Train loss 6.236, Val loss 6.279\n","Ep 1 (Step 003470): Train loss 6.184, Val loss 6.273\n","Ep 1 (Step 003475): Train loss 6.206, Val loss 6.283\n","Ep 1 (Step 003480): Train loss 6.163, Val loss 6.277\n","Ep 1 (Step 003485): Train loss 6.363, Val loss 6.281\n","Ep 1 (Step 003490): Train loss 6.190, Val loss 6.270\n","Ep 1 (Step 003495): Train loss 6.169, Val loss 6.271\n","Ep 1 (Step 003500): Train loss 6.159, Val loss 6.268\n","Ep 1 (Step 003505): Train loss 6.327, Val loss 6.267\n","Ep 1 (Step 003510): Train loss 6.122, Val loss 6.277\n","Ep 1 (Step 003515): Train loss 6.180, Val loss 6.276\n","Ep 1 (Step 003520): Train loss 6.148, Val loss 6.276\n","Ep 1 (Step 003525): Train loss 6.090, Val loss 6.284\n","Ep 1 (Step 003530): Train loss 6.204, Val loss 6.275\n","Ep 1 (Step 003535): Train loss 6.190, Val loss 6.272\n","Ep 1 (Step 003540): Train loss 6.194, Val loss 6.268\n","Ep 1 (Step 003545): Train loss 6.199, Val loss 6.271\n","Ep 1 (Step 003550): Train loss 6.078, Val loss 6.286\n","Ep 1 (Step 003555): Train loss 6.021, Val loss 6.269\n","Ep 1 (Step 003560): Train loss 6.144, Val loss 6.271\n","Ep 1 (Step 003565): Train loss 6.147, Val loss 6.273\n","Ep 1 (Step 003570): Train loss 6.204, Val loss 6.268\n","Ep 1 (Step 003575): Train loss 6.215, Val loss 6.280\n","Ep 1 (Step 003580): Train loss 6.205, Val loss 6.275\n","Ep 1 (Step 003585): Train loss 6.241, Val loss 6.265\n","Ep 1 (Step 003590): Train loss 6.103, Val loss 6.270\n","Ep 1 (Step 003595): Train loss 6.073, Val loss 6.260\n","Ep 1 (Step 003600): Train loss 6.143, Val loss 6.260\n","Ep 1 (Step 003605): Train loss 6.097, Val loss 6.280\n","Ep 1 (Step 003610): Train loss 6.249, Val loss 6.266\n","Ep 1 (Step 003615): Train loss 6.234, Val loss 6.273\n","Ep 1 (Step 003620): Train loss 6.112, Val loss 6.263\n","Ep 1 (Step 003625): Train loss 6.296, Val loss 6.259\n","Ep 1 (Step 003630): Train loss 6.270, Val loss 6.256\n","Ep 1 (Step 003635): Train loss 6.122, Val loss 6.263\n","Ep 1 (Step 003640): Train loss 6.069, Val loss 6.261\n","Ep 1 (Step 003645): Train loss 6.230, Val loss 6.255\n","Ep 1 (Step 003650): Train loss 6.174, Val loss 6.263\n","Ep 1 (Step 003655): Train loss 6.107, Val loss 6.264\n","Ep 1 (Step 003660): Train loss 6.254, Val loss 6.265\n","Ep 1 (Step 003665): Train loss 6.128, Val loss 6.256\n","Ep 1 (Step 003670): Train loss 6.257, Val loss 6.266\n","Ep 1 (Step 003675): Train loss 5.953, Val loss 6.263\n","Ep 1 (Step 003680): Train loss 6.192, Val loss 6.268\n","Ep 1 (Step 003685): Train loss 6.193, Val loss 6.252\n","Ep 1 (Step 003690): Train loss 6.046, Val loss 6.249\n","Ep 1 (Step 003695): Train loss 6.073, Val loss 6.261\n","Ep 1 (Step 003700): Train loss 6.043, Val loss 6.251\n","Ep 1 (Step 003705): Train loss 6.137, Val loss 6.256\n","Ep 1 (Step 003710): Train loss 6.241, Val loss 6.264\n","Ep 1 (Step 003715): Train loss 6.275, Val loss 6.251\n","Ep 1 (Step 003720): Train loss 6.105, Val loss 6.247\n","Ep 1 (Step 003725): Train loss 6.316, Val loss 6.257\n","Ep 1 (Step 003730): Train loss 6.136, Val loss 6.254\n","Ep 1 (Step 003735): Train loss 6.027, Val loss 6.245\n","Ep 1 (Step 003740): Train loss 6.129, Val loss 6.248\n","Ep 1 (Step 003745): Train loss 6.212, Val loss 6.245\n","Ep 1 (Step 003750): Train loss 6.099, Val loss 6.239\n","Ep 1 (Step 003755): Train loss 6.095, Val loss 6.236\n","Ep 1 (Step 003760): Train loss 6.211, Val loss 6.241\n","Ep 1 (Step 003765): Train loss 6.082, Val loss 6.249\n","Ep 1 (Step 003770): Train loss 6.130, Val loss 6.241\n","Ep 1 (Step 003775): Train loss 6.137, Val loss 6.243\n","Ep 1 (Step 003780): Train loss 6.070, Val loss 6.237\n","Ep 1 (Step 003785): Train loss 6.115, Val loss 6.237\n","Ep 1 (Step 003790): Train loss 6.102, Val loss 6.242\n","Ep 1 (Step 003795): Train loss 6.131, Val loss 6.242\n","Ep 1 (Step 003800): Train loss 6.144, Val loss 6.247\n","Ep 1 (Step 003805): Train loss 6.208, Val loss 6.253\n","Ep 1 (Step 003810): Train loss 6.179, Val loss 6.242\n","Ep 1 (Step 003815): Train loss 6.193, Val loss 6.236\n","Ep 1 (Step 003820): Train loss 5.853, Val loss 6.251\n","Ep 1 (Step 003825): Train loss 6.072, Val loss 6.238\n","Ep 1 (Step 003830): Train loss 6.167, Val loss 6.233\n","Ep 1 (Step 003835): Train loss 6.088, Val loss 6.236\n","Ep 1 (Step 003840): Train loss 6.046, Val loss 6.243\n","Ep 1 (Step 003845): Train loss 6.131, Val loss 6.243\n","Ep 1 (Step 003850): Train loss 5.948, Val loss 6.239\n","Ep 1 (Step 003855): Train loss 6.187, Val loss 6.233\n","Ep 1 (Step 003860): Train loss 6.072, Val loss 6.229\n","Ep 1 (Step 003865): Train loss 6.093, Val loss 6.231\n","Ep 1 (Step 003870): Train loss 6.127, Val loss 6.228\n","Ep 1 (Step 003875): Train loss 6.089, Val loss 6.232\n","Ep 1 (Step 003880): Train loss 6.015, Val loss 6.223\n","Ep 1 (Step 003885): Train loss 6.187, Val loss 6.224\n","Ep 1 (Step 003890): Train loss 6.201, Val loss 6.220\n","Ep 1 (Step 003895): Train loss 6.122, Val loss 6.218\n","Ep 1 (Step 003900): Train loss 6.116, Val loss 6.224\n","Ep 1 (Step 003905): Train loss 6.098, Val loss 6.225\n","Ep 1 (Step 003910): Train loss 6.123, Val loss 6.221\n","Ep 1 (Step 003915): Train loss 6.048, Val loss 6.221\n","Ep 1 (Step 003920): Train loss 6.120, Val loss 6.210\n","Ep 1 (Step 003925): Train loss 6.026, Val loss 6.225\n","Ep 1 (Step 003930): Train loss 6.138, Val loss 6.214\n","Ep 1 (Step 003935): Train loss 6.024, Val loss 6.210\n","Ep 1 (Step 003940): Train loss 6.226, Val loss 6.237\n","Ep 1 (Step 003945): Train loss 6.059, Val loss 6.213\n","Ep 1 (Step 003950): Train loss 6.177, Val loss 6.208\n","Ep 1 (Step 003955): Train loss 6.176, Val loss 6.216\n","Ep 1 (Step 003960): Train loss 6.171, Val loss 6.208\n","Ep 1 (Step 003965): Train loss 6.040, Val loss 6.203\n","Ep 1 (Step 003970): Train loss 6.151, Val loss 6.219\n","Ep 1 (Step 003975): Train loss 6.067, Val loss 6.209\n","Ep 1 (Step 003980): Train loss 6.077, Val loss 6.220\n","Ep 1 (Step 003985): Train loss 6.288, Val loss 6.216\n","Ep 1 (Step 003990): Train loss 5.979, Val loss 6.217\n","Ep 1 (Step 003995): Train loss 6.039, Val loss 6.228\n","Ep 1 (Step 004000): Train loss 6.052, Val loss 6.210\n","Ep 1 (Step 004005): Train loss 6.169, Val loss 6.221\n","Ep 1 (Step 004010): Train loss 6.092, Val loss 6.209\n","Ep 1 (Step 004015): Train loss 6.019, Val loss 6.216\n","Ep 1 (Step 004020): Train loss 6.124, Val loss 6.214\n","Ep 1 (Step 004025): Train loss 6.127, Val loss 6.215\n","Ep 1 (Step 004030): Train loss 5.970, Val loss 6.221\n","Ep 1 (Step 004035): Train loss 6.127, Val loss 6.218\n","Ep 1 (Step 004040): Train loss 5.965, Val loss 6.217\n","Ep 1 (Step 004045): Train loss 6.093, Val loss 6.219\n","Ep 1 (Step 004050): Train loss 6.056, Val loss 6.217\n"]}]}]}