{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEjf+FSm/+fYMbidcswZz1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Data Cleaning\n","(Removing Duplicates, Incomplete Rows, Stopwords)"],"metadata":{"id":"Zlgx4b_gBSif"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"Y2b3-cy9-bIo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732088946622,"user_tz":-330,"elapsed":289555,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"0728f752-2a8d-4f2c-88b3-1de5080729ee"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["    aa  aaa  aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa   ab  abandon  abandoned  \\\n","0  0.0  0.0                                 0.0  0.0      0.0        0.0   \n","1  0.0  0.0                                 0.0  0.0      0.0        0.0   \n","2  0.0  0.0                                 0.0  0.0      0.0        0.0   \n","3  0.0  0.0                                 0.0  0.0      0.0        0.0   \n","4  0.0  0.0                                 0.0  0.0      0.0        0.0   \n","\n","   abandoning  abandonment  abc  abdomen  ...  zero  zoloft  zombie  zombies  \\\n","0         0.0          0.0  0.0      0.0  ...   0.0     0.0     0.0      0.0   \n","1         0.0          0.0  0.0      0.0  ...   0.0     0.0     0.0      0.0   \n","2         0.0          0.0  0.0      0.0  ...   0.0     0.0     0.0      0.0   \n","3         0.0          0.0  0.0      0.0  ...   0.0     0.0     0.0      0.0   \n","4         0.0          0.0  0.0      0.0  ...   0.0     0.0     0.0      0.0   \n","\n","   zone  zones  zoning  zoom  zuckerberg  zyprexa  \n","0   0.0    0.0     0.0   0.0         0.0      0.0  \n","1   0.0    0.0     0.0   0.0         0.0      0.0  \n","2   0.0    0.0     0.0   0.0         0.0      0.0  \n","3   0.0    0.0     0.0   0.0         0.0      0.0  \n","4   0.0    0.0     0.0   0.0         0.0      0.0  \n","\n","[5 rows x 10000 columns]\n"]}],"source":["import pandas as pd\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# Download stopwords (if you haven't already)\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","# Download the 'punkt_tab' resource\n","nltk.download('punkt_tab')  # This line is added to download the necessary data\n","\n","# Load the dataset\n","df = pd.read_csv('mental_health.csv')\n","\n","# 1. Handling Missing Values\n","# Remove rows with missing text\n","df.dropna(subset=['text'], inplace=True)\n","\n","# 2. Removing duplicates (if any)\n","df.drop_duplicates(subset=['text'], inplace=True)\n","\n","# 3. Text Preprocessing\n","\n","# Define a list of negative words to retain\n","negative_words = {\"not\", \"no\", \"nor\", \"never\", \"nothing\", \"nowhere\", \"neither\", \"cannot\", \"n't\", \"without\", \"barely\", \"hardly\", \"scarcely\"}\n","\n","# Define a function to clean the text\n","def clean_text(text):\n","    # Remove URLs\n","    text = re.sub(r'http\\S+', '', text)\n","\n","    # Remove mentions (@username)\n","    text = re.sub(r'@\\w+', '', text)\n","\n","    # Remove special characters, numbers, and punctuations\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # Convert text to lowercase\n","    text = text.lower()\n","\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Remove stopwords, but keep negative words\n","    tokens = [word for word in tokens if word not in stopwords.words('english') or word in negative_words]\n","\n","    # Join the tokens back into a single string\n","    clean_text = ' '.join(tokens)\n","\n","    return clean_text\n","\n","# Apply the cleaning function to the 'text' column\n","df['cleaned_text'] = df['text'].apply(clean_text)\n","\n","# 4. Feature Extraction using TF-IDF Vectorization\n","\n","# Initialize the TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=10000)  # You can adjust the max_features\n","\n","# Fit and transform the cleaned text data\n","X = vectorizer.fit_transform(df['cleaned_text'])\n","\n","# Convert the result to a DataFrame for easier understanding (optional)\n","X_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","# You now have a cleaned and vectorized dataset.\n","print(X_df.head())\n","\n","# Save the preprocessed dataset (optional)\n","df.to_csv('preprocessed_mental_health.csv', index=False)"]},{"cell_type":"code","source":["!jupyter nbconvert --to html 02_Data_Preprocessing.ipynb\n","#"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_V5fjOJEuyU","executionInfo":{"status":"ok","timestamp":1731141538167,"user_tz":-330,"elapsed":2603,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"3a77ce60-1c66-4dc9-e3cc-ae4f3b2883b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook 02_Data_Preprocessing.ipynb to html\n","[NbConvertApp] Writing 280507 bytes to 02_Data_Preprocessing.html\n"]}]}]}