{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNbX8lMKhl3sSzS/d6hUcr6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6kUkrqg9dPxu","executionInfo":{"status":"ok","timestamp":1733633072350,"user_tz":-330,"elapsed":8302,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"3532f58c-1914-46dc-e46f-9c343ef6c1cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting praw\n","  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n","Collecting prawcore<3,>=2.4 (from praw)\n","  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n","Collecting update_checker>=0.18 (from praw)\n","  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n","Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n","Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n","Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Installing collected packages: update_checker, prawcore, praw\n","Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"]}],"source":["!pip install praw"]},{"cell_type":"markdown","source":["# more data"],"metadata":{"id":"-iYDMiX06igD"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"praw\")\n","\n","import logging\n","\n","# Suppress PRAW warnings\n","logging.getLogger(\"praw\").setLevel(logging.CRITICAL)\n","\n","\n","import praw\n","import pandas as pd\n","import time\n","\n","# Reddit API credentials\n","reddit = praw.Reddit(client_id='fgsA4XN83I9FEZdiZvRDtw',\n","                     client_secret='WLahZdE7ybEoPcIgN-MuMSqWurCDHQ',\n","                     user_agent='Mental Health Scraper')\n","\n","# Search for subreddits containing 'ptsd'\n","# query = 'bipolar'  # Substring to search for\n","# matching_subreddits = []\n","# for subreddit in reddit.subreddits.search(query, limit=None):\n","#     if 'bipolar' in subreddit.display_name.lower():  # Case-insensitive check\n","#         matching_subreddits.append(subreddit.display_name)\n","\n","# List of keywords to exclude from subreddit names\n","exclude_keywords = ['anxiety', 'bipolar', 'ptsd', 'depression','normal', 'news', 'technology', 'gaming', 'movies', 'books', 'AskReddit', 'worldnews']\n","# Initialize a list to store the matching subreddits\n","matching_subreddits = []\n","# Set a reasonable limit for the number of subreddits to search\n","limit = 100  # Adjust this limit as needed\n","# Perform the search for subreddits\n","for submission in reddit.subreddits.popular(limit=limit):  # Use `popular` to get a list of popular subreddits\n","    if all(keyword not in submission.display_name.lower() for keyword in exclude_keywords):\n","        matching_subreddits.append(submission.display_name)\n","# Limit to the top 15 subreddits / change the indices as per need\n","print(len(matching_subreddits))\n","# matching_subreddits = matching_subreddits[50:]\n","print(f\"Found subreddits: {matching_subreddits}\")\n","\n","# Number of posts to scrape per subreddit and type\n","total_posts_per_subreddit = 20000\n","posts_per_type = total_posts_per_subreddit  # Divide among 'hot', 'new', 'top'\n","\n","# List to store all posts\n","all_data = []\n","\n","# Scrape posts from each matching subreddit\n","post_types = ['hot', 'new', 'top']\n","\n","num=1\n","count=0\n","\n","try:\n","    for subreddit_name in matching_subreddits:\n","        print(f\"Scraping subreddit: {subreddit_name}\")\n","        subreddit = reddit.subreddit(subreddit_name)\n","        count=0\n","        for post_type in post_types:\n","            if post_type == 'hot':\n","                subreddit_posts = subreddit.hot(limit=posts_per_type)\n","            elif post_type == 'new':\n","                subreddit_posts = subreddit.new(limit=posts_per_type)\n","            elif post_type == 'top':\n","                subreddit_posts = subreddit.top(limit=posts_per_type)\n","\n","            for post in subreddit_posts:\n","                post_content = post.title + \" \" + post.selftext  # Combine title and body\n","                all_data.append([post_content, 'normal'])  # Add text, category, subreddit name\n","                count+=1\n","\n","            # Avoid hitting Reddit's rate limit\n","            time.sleep(2)\n","\n","        print(f\"{num}. Total Scraped {len(all_data)}. From {subreddit_name} is {count}\")\n","        num+=1\n","\n","except Exception as e:\n","    pass\n","finally:\n","    # Convert collected data to a DataFrame\n","    df = pd.DataFrame(all_data, columns=['text', 'mental_health_issue'])\n","    # Save to CSV\n","    output_file = 'all_normal2.csv'\n","    df.to_csv(output_file, index=False)\n","\n","    # Display the first 10 rows\n","    print(f\"Data saved to {output_file}\")\n","    print(df.head(10))\n"],"metadata":{"id":"rBILqvbzG_mM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Cleaning"],"metadata":{"id":"8vx3FFiI-cH3"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# Download stopwords (if you haven't already)\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","# Download the 'punkt_tab' resource\n","nltk.download('punkt_tab')  # This line is added to download the necessary data\n","\n","# Load the dataset\n","df = pd.read_csv('final_normal.csv')\n","\n","# 1. Handling Missing Values\n","# Remove rows with missing text\n","df.dropna(subset=['text'], inplace=True)\n","\n","# 2. Removing duplicates (if any)\n","df.drop_duplicates(subset=['text'], inplace=True)\n","\n","# 3. Text Preprocessing\n","\n","# Define a list of negative words to retain\n","negative_words = {\"not\", \"no\", \"nor\", \"never\", \"nothing\", \"nowhere\", \"neither\", \"cannot\", \"n't\", \"without\", \"barely\", \"hardly\", \"scarcely\"}\n","\n","# Define a function to clean the text\n","def clean_text(text):\n","    # Remove URLs\n","    text = re.sub(r'http\\S+', '', text)\n","\n","    # Remove mentions (@username)\n","    text = re.sub(r'@\\w+', '', text)\n","\n","    # Remove special characters, numbers, and punctuations\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # Convert text to lowercase\n","    text = text.lower()\n","\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Remove stopwords, but keep negative words\n","    tokens = [word for word in tokens if word not in stopwords.words('english') or word in negative_words]\n","\n","    # Join the tokens back into a single string\n","    clean_text = ' '.join(tokens)\n","\n","    return clean_text\n","\n","# Apply the cleaning function to the 'text' column\n","df['cleaned_text'] = df['text'].apply(clean_text)\n","\n","# Save the preprocessed dataset (optional)\n","df.to_csv('final_preprocessed_normal.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4k0GIjor7wQe","executionInfo":{"status":"ok","timestamp":1733653699576,"user_tz":-330,"elapsed":623614,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"faec2d10-391a-4163-9eaa-ade97c3ec358"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# Combine all datasets"],"metadata":{"id":"TCk1Bp-u6hzA"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# File paths for the two datasets\n","file1 = 'all_normal.csv'\n","file2 = 'all_normal2.csv'\n","\n","# Read the datasets\n","df1 = pd.read_csv(file1)\n","df2 = pd.read_csv(file2)\n","\n","# Combine the datasets\n","combined_df = pd.concat([df1, df2], ignore_index=True)\n","\n","# Save the combined dataset to a new CSV file\n","output_file = 'final_normal.csv'\n","combined_df.to_csv(output_file, index=False)\n","\n","# Display the number of records and the first few rows\n","print(f\"Combined dataset saved as {output_file} with {len(combined_df)} records.\")\n","print(combined_df.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1BR5bfhsqjlw","executionInfo":{"status":"ok","timestamp":1733653026103,"user_tz":-330,"elapsed":2459,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"3d886a48-3185-4b68-bedb-cbcb34df65c3"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined dataset saved as final_normal.csv with 131725 records.\n","                                                text mental_health_issue\n","0  Rules **UPDATED** 1. If you're here you must p...              normal\n","1  Happy Cakeday, r/Normal! Today you're 11 Let's...              normal\n","2  Happy Cakeday, r/Normal! Today you're 10 Let's...              normal\n","3  I am also a person I am very normal. Today I t...              normal\n","4  Happy Cakeday, r/Normal! Today you're 9 Let's ...              normal\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# Get all CSV files in the current directory\n","csv_files = [f for f in os.listdir() if f.endswith('.csv')]\n","\n","# Function to combine datasets and save to a new CSV\n","def combine_and_save(file_list, output_filename):\n","    # Read and combine all CSV files\n","    df_list = [pd.read_csv(file) for file in file_list]\n","    combined_df = pd.concat(df_list, ignore_index=True)\n","\n","    # Save to a new CSV file\n","    combined_df.to_csv(output_filename, index=False)\n","\n","    # Return the count of records\n","    return len(combined_df)\n","\n","# Combine files related to anxiety\n","anxiety_files = [f for f in csv_files if 'all_anxiety' in f]\n","anxiety_count = combine_and_save(anxiety_files, 'final_anxiety.csv')\n","print(f\"Combined anxiety files: {anxiety_count} records\")\n","\n","# Combine files related to PTSD\n","ptsd_files = [f for f in csv_files if 'all_ptsd' in f]\n","ptsd_count = combine_and_save(ptsd_files, 'final_ptsd.csv')\n","print(f\"Combined PTSD files: {ptsd_count} records\")\n","\n","# Combine files related to depression\n","depression_files = [f for f in csv_files if 'all_depression' in f]\n","depression_count = combine_and_save(depression_files, 'final_depression.csv')\n","print(f\"Combined depression files: {depression_count} records\")\n","\n","# Combine files related to bipolar\n","bipolar_files = [f for f in csv_files if 'all_bipolar' in f]\n","bipolar_count = combine_and_save(bipolar_files, 'final_bipolar.csv')\n","print(f\"Combined bipolar files: {bipolar_count} records\")\n","\n","# Combine files related to preprocessed anxiety\n","preprocessed_anxiety_files = [f for f in csv_files if 'preprocessed_anxiety' in f]\n","preprocessed_anxiety_count = combine_and_save(preprocessed_anxiety_files, 'final_preprocessed_anxiety.csv')\n","print(f\"Combined preprocessed anxiety files: {preprocessed_anxiety_count} records\")\n","\n","# Combine files related to preprocessed depression\n","preprocessed_depression_files = [f for f in csv_files if 'preprocessed_depression' in f]\n","preprocessed_depression_count = combine_and_save(preprocessed_depression_files, 'final_preprocessed_depression.csv')\n","print(f\"Combined preprocessed depression files: {preprocessed_depression_count} records\")\n","\n","# Combine files related to preprocessed bipolar\n","preprocessed_bipolar_files = [f for f in csv_files if 'preprocessed_bipolar' in f]\n","preprocessed_bipolar_count = combine_and_save(preprocessed_bipolar_files, 'final_preprocessed_bipolar.csv')\n","print(f\"Combined preprocessed bipolar files: {preprocessed_bipolar_count} records\")\n","\n","# Combine files related to preprocessed PTSD\n","preprocessed_ptsd_files = [f for f in csv_files if 'preprocessed_ptsd' in f]\n","preprocessed_ptsd_count = combine_and_save(preprocessed_ptsd_files, 'final_preprocessed_ptsd.csv')\n","print(f\"Combined preprocessed PTSD files: {preprocessed_ptsd_count} records\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nqeFtTmVgeoM","executionInfo":{"status":"ok","timestamp":1733648639326,"user_tz":-330,"elapsed":9507,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"5949ad8c-3452-4037-dadb-eb905a54aa10"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined anxiety files: 38580 records\n","Combined PTSD files: 34019 records\n","Combined depression files: 36908 records\n","Combined bipolar files: 30420 records\n","Combined preprocessed anxiety files: 20456 records\n","Combined preprocessed depression files: 19446 records\n","Combined preprocessed bipolar files: 15746 records\n","Combined preprocessed PTSD files: 16386 records\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# File path for the dataset\n","file = 'final_preprocessed_normal.csv'\n","\n","# Read the dataset\n","df = pd.read_csv(file)\n","\n","# Get the first 40,000 records\n","first_40k_df = df.head(40000)\n","\n","# Save the new dataset to a CSV file\n","output_file = 'final_preprocessed_normal1.csv'\n","first_40k_df.to_csv(output_file, index=False)\n","\n","# Display the number of records and the first few rows\n","print(f\"New dataset with first 40,000 records saved as {output_file}.\")\n","print(first_40k_df.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5k642mjGWK-","executionInfo":{"status":"ok","timestamp":1733658744924,"user_tz":-330,"elapsed":1687,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"663cb265-617f-4d98-9a30-f0b458efa892"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["New dataset with first 40,000 records saved as final_preprocessed_normal1.csv.\n","                                                text mental_health_issue  \\\n","0  Rules **UPDATED** 1. If you're here you must p...              normal   \n","1  Happy Cakeday, r/Normal! Today you're 11 Let's...              normal   \n","2  Happy Cakeday, r/Normal! Today you're 10 Let's...              normal   \n","3  I am also a person I am very normal. Today I t...              normal   \n","4  Happy Cakeday, r/Normal! Today you're 9 Let's ...              normal   \n","\n","                                        cleaned_text  \n","0  rules updated youre must post something normal...  \n","1  happy cakeday rnormal today youre lets look ba...  \n","2  happy cakeday rnormal today youre lets look ba...  \n","3  also person normal today took dogs walk park a...  \n","4  happy cakeday rnormal today youre lets look ba...  \n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# List of all the CSV file names\n","csv_files = ['final_preprocessed_normal1.csv', 'final_preprocessed_ptsd.csv', 'final_preprocessed_anxiety.csv',\n","             'final_preprocessed_depression.csv', 'final_preprocessed_bipolar.csv']\n","\n","# Display the number of records in each CSV file\n","for file in csv_files:\n","    df = pd.read_csv(file)\n","    print(f\"Number of records in {file}: {len(df)}\")\n","\n","# Read and combine all CSV files\n","df_list = [pd.read_csv(file) for file in csv_files]\n","combined_df = pd.concat(df_list, ignore_index=True)\n","\n","# Display the number of records in the final combined CSV\n","print(f\"Number of records in the combined CSV before shuffling: {len(combined_df)}\")\n","\n","# Shuffle the rows to jumble them up\n","shuffled_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# Save the combined and shuffled dataset to a new CSV\n","shuffled_df.to_csv('preprocessed_mental_health.csv', index=False)\n","\n","# Display the number of records in the final shuffled CSV\n","print(f\"Number of records in the shuffled CSV: {len(shuffled_df)}\")\n","\n","# Display the first few rows\n","print(shuffled_df.head(10))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K6l8m_s1HeV3","executionInfo":{"status":"ok","timestamp":1733658880771,"user_tz":-330,"elapsed":4919,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"7a024d21-2bce-4e53-d608-93e1f9eeb773"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of records in final_preprocessed_normal1.csv: 40000\n","Number of records in final_preprocessed_ptsd.csv: 16386\n","Number of records in final_preprocessed_anxiety.csv: 20456\n","Number of records in final_preprocessed_depression.csv: 19446\n","Number of records in final_preprocessed_bipolar.csv: 15746\n","Number of records in the combined CSV before shuffling: 112034\n","Number of records in the shuffled CSV: 112034\n","                                                text mental_health_issue  \\\n","0  Do you ever just wait for someone to give up o...          depression   \n","1  Interesting tips here on effectively observing...                ptsd   \n","2  Question for a choice to make on a video I've ...              normal   \n","3           What motivates you to better yourself?                normal   \n","4  So almost a week has gone since the allegation...              normal   \n","5  Battle perfectionism with these six weird phra...             anxiety   \n","6  Scared for freeway drive Hey, I (16f) posted h...             anxiety   \n","7  After much polling, I've discovered that Kansa...              normal   \n","8  Polish FM, Warsaw mayor confirm presidential a...              normal   \n","9  AITAH for letting her go? “first of all I’m so...              normal   \n","\n","                                        cleaned_text  \n","0  ever wait someone give justify negative though...  \n","1  interesting tips effectively observing thought...  \n","2  question choice make video ive recorded succes...  \n","3                                   motivates better  \n","4  almost week gone since allegations twitch staf...  \n","5             battle perfectionism six weird phrases  \n","6  scared freeway drive hey f posted june talking...  \n","7  much polling ive discovered kansans dont under...  \n","8  polish fm warsaw mayor confirm presidential am...  \n","9  aitah letting go first im sorry bad english kn...  \n"]}]}]}