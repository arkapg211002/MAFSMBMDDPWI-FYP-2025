{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y ffmpeg libsm6 libxext6\n",
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install -y portaudio19-dev\n",
        "\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install pydub\n",
        "!pip install sounddevice\n",
        "!pip install wavio\n",
        "!pip install numpy\n",
        "!pip install openai-whisper\n",
        "!pip install PyAudio\n",
        "!pip install SpeechRecognition\n",
        "\n",
        "!pip install deep-translator\n",
        "!pip install joblib\n",
        "!pip install pandas\n",
        "!pip install Pillow\n",
        "!pip install praw\n",
        "!pip install protobuf\n",
        "!pip install pytesseract\n",
        "!pip install Requests\n",
        "!pip install scikit-learn\n",
        "!pip install google-generativeai\n",
        "\n",
        "!pip install tweepy\n",
        "\n",
        "!pip install librosa\n",
        "\n",
        "!pip install tensorflow\n",
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install xgboost\n",
        "!pip install deepface\n",
        "!pip install tf_keras\n",
        "\n",
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asoV06p5D5W-",
        "outputId": "fbd169ba-8aac-4f69-9bc6-5c31cac6a86c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsm6 is already the newest version (2:1.2.3-1build2).\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,920 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123630 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 1s (278 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 123677 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.40.2-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.40.2 watchdog-6.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting sounddevice\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
            "Downloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice\n",
            "Successfully installed sounddevice-0.5.1\n",
            "Collecting wavio\n",
            "  Downloading wavio-0.0.9-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from wavio) (1.26.4)\n",
            "Downloading wavio-0.0.9-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: wavio\n",
            "Successfully installed wavio-0.0.9\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=c547b9571258245fd316d00fe66fe5480b3d22cff26508fd3bea3fb6b0040442\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n",
            "Collecting PyAudio\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m239.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: PyAudio\n",
            "  Building wheel for PyAudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyAudio: filename=PyAudio-0.2.14-cp310-cp310-linux_x86_64.whl size=63858 sha256=b1d4683a368de9fcba889cbc238dffb72cc859ceb1c40364e28f4fa24d96172a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/21/f4/0b51d41ba79e51b16295cbb096ec49f334792814d545b508c5\n",
            "Successfully built PyAudio\n",
            "Installing collected packages: PyAudio\n",
            "Successfully installed PyAudio-0.2.14\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n",
            "Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.11.0\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.8.30)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (4.25.5)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (11.0.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Requirement already satisfied: Requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from Requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from Requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from Requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from Requests) (2024.8.30)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.151.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.25.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.23.4)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.68.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.10/dist-packages (4.14.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2024.8.30)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Collecting deepface\n",
            "  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.2.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.66.6)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (11.0.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.10.0.84)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.17.1)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (3.5.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from deepface) (3.0.3)\n",
            "Collecting flask-cors>=4.0.1 (from deepface)\n",
            "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (2.5.0)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (3.16.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2024.8.30)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
            "Downloading deepface-0.0.93-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=a4245fd37f6c62c96f395c14238bb21e908a2bbe28e2d7b09e7c877324046df4\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built fire\n",
            "Installing collected packages: lz4, gunicorn, fire, mtcnn, flask-cors, retina-face, deepface\n",
            "Successfully installed deepface-0.0.93 fire-0.7.0 flask-cors-5.0.0 gunicorn-23.0.0 lz4-4.3.3 mtcnn-1.0.0 retina-face-0.0.17\n",
            "Requirement already satisfied: tf_keras in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: tensorflow<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tf_keras) (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf_keras) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf_keras) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf_keras) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf_keras) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf_keras) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf_keras) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf_keras) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf_keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf_keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf_keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf_keras) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr8RRghPDeJj",
        "outputId": "0e86d552-83b8-4049-8624-a1f18a20c638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing v11.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile v11.py\n",
        "\n",
        "import pickle\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import praw\n",
        "from PIL import Image\n",
        "from deep_translator import GoogleTranslator\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from collections import Counter\n",
        "import google.generativeai as genai\n",
        "import pickle\n",
        "import cv2\n",
        "import numpy as np\n",
        "import whisper\n",
        "import tempfile\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "\n",
        "import tweepy\n",
        "\n",
        "import re\n",
        "import librosa\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "from tensorflow.keras.models import load_model, Model, Sequential\n",
        "from tensorflow.keras.utils import pad_sequences, custom_object_scope, to_categorical\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Input, Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Layer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from deepface import DeepFace\n",
        "\n",
        "import torch\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "import pytesseract\n",
        "\n",
        "# Configure Tesseract and FFMPEG\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "os.environ[\"FFMPEG_BINARY\"] = \"/usr/bin/ffmpeg\"\n",
        "\n",
        "# Load Whisper model for audio transcription\n",
        "# whisper_model = whisper.load_model(\"base\")\n",
        "@st.cache_resource\n",
        "def load_whisper_model():\n",
        "    return whisper.load_model(\"base\")\n",
        "\n",
        "whisper_model = load_whisper_model()\n",
        "\n",
        "# ------------- ENSEMBLE LEARNING REQUIREMENTS -----------------\n",
        "\n",
        "# Define functions to load each model and resource with caching\n",
        "@st.cache_resource\n",
        "def load_lr_model():\n",
        "    return joblib.load('LRmodel.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_lr_vectorizer():\n",
        "    return joblib.load('LRvectorizer.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_nb_model():\n",
        "    return joblib.load('NBmodel.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_nb_vectorizer():\n",
        "    return joblib.load('NBvectorizer.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_svm_model():\n",
        "    return joblib.load('SVMmodel.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_svm_vectorizer():\n",
        "    return joblib.load('SVMvectorizer.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_label_encoder():\n",
        "    return joblib.load('label_encoder.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_xgb_model():\n",
        "    return joblib.load('xgb_model.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_tfidf_vectorizer():\n",
        "    return joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_lstm_label_encoder():\n",
        "    return joblib.load('LSTM_label_encoder.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_lstm_tokenizer():\n",
        "    return joblib.load('LSTM_tokenizer.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_lstm_model():\n",
        "    return load_model('lstm_model.h5')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_meta_learner_rf():\n",
        "    return joblib.load('meta_learner_rf.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_transformer_label_encoder():\n",
        "    return joblib.load('Tlabel_encoder.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_transformer_vectorizer():\n",
        "    return joblib.load('Tvectorizer_layer.pkl')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load all models and resources by calling the above functions\n",
        "lr_model = load_lr_model()\n",
        "lr_vectorizer = load_lr_vectorizer()\n",
        "nb_model = load_nb_model()\n",
        "nb_vectorizer = load_nb_vectorizer()\n",
        "svm_model = load_svm_model()\n",
        "svm_vectorizer = load_svm_vectorizer()\n",
        "label_encoder = load_label_encoder()\n",
        "xgb_model = load_xgb_model()\n",
        "tfidf_vectorizer = load_tfidf_vectorizer()\n",
        "lstm_label_encoder = load_lstm_label_encoder()  # Optional: Only if separate from main label encoder\n",
        "lstm_tokenizer = load_lstm_tokenizer()\n",
        "lstm_model = load_lstm_model()\n",
        "meta_learner_rf = load_meta_learner_rf()\n",
        "\n",
        "t_label_encoder = load_transformer_label_encoder()\n",
        "t_vectorizer = load_transformer_vectorizer()\n",
        "\n",
        "# Define the custom layers\n",
        "class EmbeddingLayer(Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(EmbeddingLayer, self).__init__(**kwargs)\n",
        "        self.word_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embedding = Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, tokens):\n",
        "        sequence_length = tf.shape(tokens)[-1]\n",
        "        positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
        "        positions_encoding = self.position_embedding(positions)\n",
        "        words_encoding = self.word_embedding(tokens)\n",
        "        return positions_encoding + words_encoding\n",
        "\n",
        "\n",
        "class EncoderLayer(Layer):\n",
        "    def __init__(self, total_heads, total_dense_units, embed_dim, **kwargs):\n",
        "        super(EncoderLayer, self).__init__(**kwargs)\n",
        "        self.multihead = MultiHeadAttention(num_heads=total_heads, key_dim=embed_dim)\n",
        "        self.nnw = Sequential([Dense(total_dense_units, activation=\"relu\"), Dense(embed_dim)])\n",
        "        self.normalize_layer = LayerNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.multihead(inputs, inputs)\n",
        "        normalize_attn = self.normalize_layer(inputs + attn_output)\n",
        "        nnw_output = self.nnw(normalize_attn)\n",
        "        final_output = self.normalize_layer(normalize_attn + nnw_output)\n",
        "        return final_output\n",
        "\n",
        "# Load the saved transformer model with custom objects\n",
        "custom_objects = {\n",
        "    \"EmbeddingLayer\": EmbeddingLayer,\n",
        "    \"EncoderLayer\": EncoderLayer\n",
        "}\n",
        "\n",
        "@st.cache_resource\n",
        "def load_transformer_model():\n",
        "    return load_model('Ttransformer_model.h5', custom_objects=custom_objects)\n",
        "\n",
        "transformer_model = load_transformer_model()\n",
        "\n",
        "\n",
        "# ------------- ENSEMBLE LEARNING REQUIREMENTS -----------------\n",
        "\n",
        "# Initialize Reddit API\n",
        "reddit = praw.Reddit(client_id='DAOso5_7CHzXzdtd-070fg',\n",
        "                     client_secret='JtdGFRDM10avSQFYthzYUQNfLeI8rQ',\n",
        "                     user_agent='Mental Health')\n",
        "\n",
        "# Initialize Twitter API\n",
        "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAADhOxAEAAAAA5L4saD5wsvoQlB2qrUtYkmJ0DiY%3Dmjn2X0dux3zvrsY7j0HYwN95UGDEHPWQrm7EWX63nhW77NXSNE\"\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "\n",
        "# Configure Gemini API for wellbeing insights\n",
        "genai.configure(api_key=\"AIzaSyD-pu0AuG2dbzzspRfgS8DjO10Ffh08JiU\")\n",
        "generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 40,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "gemini_model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "\n",
        "# Twitter\n",
        "def fetch_image_content(image_url):\n",
        "    \"\"\"Fetch and process an image from a URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(image_url, timeout=10)\n",
        "        response.raise_for_status()  # Ensure the request was successful\n",
        "        return Image.open(BytesIO(response.content))\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error fetching image: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_latest_tweets_with_images(username, max_items=10):\n",
        "    \"\"\"Fetch latest tweets with text and associated images.\"\"\"\n",
        "    # Fetch user details to get user ID\n",
        "    user = client.get_user(username=username)\n",
        "    if not user.data:\n",
        "        return [], []\n",
        "\n",
        "    user_id = user.data.id\n",
        "\n",
        "    # Fetch the latest tweets (exclude retweets and replies)\n",
        "    response = client.get_users_tweets(\n",
        "        id=user_id,\n",
        "        tweet_fields=[\"attachments\"],\n",
        "        expansions=[\"attachments.media_keys\"],\n",
        "        media_fields=[\"url\"],\n",
        "        exclude=[\"retweets\", \"replies\"],\n",
        "        max_results=max_items\n",
        "    )\n",
        "\n",
        "    tweet_data = []\n",
        "\n",
        "    if response.data:\n",
        "        for tweet in response.data:\n",
        "            # Extract text\n",
        "            text = tweet.text\n",
        "\n",
        "            # Extract images if available\n",
        "            images = []\n",
        "            if hasattr(tweet, \"attachments\") and tweet.attachments is not None:\n",
        "                if \"media_keys\" in tweet.attachments:\n",
        "                    for media_key in tweet.attachments[\"media_keys\"]:\n",
        "                        media = next(\n",
        "                            (media for media in response.includes.get(\"media\", []) if media[\"media_key\"] == media_key), None\n",
        "                        )\n",
        "                        if media and media.type == \"photo\":\n",
        "                            images.append(media.url)\n",
        "\n",
        "            # Append tweet data\n",
        "            tweet_data.append({\"text\": text, \"images\": images})\n",
        "\n",
        "    return tweet_data\n",
        "\n",
        "\n",
        "\n",
        "# Function to fetch text-based posts from Reddit\n",
        "def fetch_user_text_posts(username):\n",
        "    try:\n",
        "        user = reddit.redditor(username)\n",
        "        posts = [post.title + \" \" + post.selftext for post in user.submissions.new(limit=20)]\n",
        "        return posts\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error fetching text posts: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to fetch image-based posts from Reddit and perform OCR\n",
        "def fetch_user_images_and_extract_text(username):\n",
        "    try:\n",
        "        user = reddit.redditor(username)\n",
        "        images = [post.url for post in user.submissions.new(limit=20) if post.url.endswith(('.jpg', '.jpeg', '.png', '.webp', '.bmp', '.tiff'))]\n",
        "\n",
        "        extracted_texts = []\n",
        "        all_emotions = {'happy': 0, 'sad': 0, 'angry': 0, 'disgust': 0, 'fear': 0, 'surprise': 0, 'neutral': 0}\n",
        "\n",
        "        combined_caption = \"\"\n",
        "\n",
        "        for image_url in images:\n",
        "            try:\n",
        "                response = requests.get(image_url)\n",
        "                image = Image.open(BytesIO(response.content))\n",
        "                st.image(image, caption=\"Fetched Image\", use_container_width=True)  # Updated to use_container_width\n",
        "                # Generate and display the caption\n",
        "                caption = generate_caption(image)\n",
        "                st.success(caption)\n",
        "                combined_caption += caption + \" \"\n",
        "\n",
        "                # Extract text from the image and handle cases where extracted text is a list\n",
        "                extracted_text = extract_text_from_image(image)\n",
        "                if isinstance(extracted_text, list):\n",
        "                    extracted_text = \"\\n\".join(extracted_text)  # Join the list into a single string\n",
        "                if extracted_text.strip():  # Strip leading/trailing spaces\n",
        "                    translated_text = GoogleTranslator(source='auto', target='en').translate(extracted_text)\n",
        "                    extracted_texts.append(translated_text)\n",
        "                    st.write(\"Extracted and Translated Text from Image:\")\n",
        "                    st.text(translated_text)\n",
        "\n",
        "                # Analyze facial emotions in the image\n",
        "                dominant_emotion = detect_emotions_from_image(image)\n",
        "                if dominant_emotion:\n",
        "                    st.success(f\"Dominant Emotion Detected: {dominant_emotion}\")\n",
        "                    all_emotions[dominant_emotion] += 1\n",
        "                else:\n",
        "                    st.error(\"No faces detected or error in emotion analysis.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error processing image {image_url}: {e}\")\n",
        "\n",
        "        # After processing all images, analyze the emotion counts and provide a suggestion\n",
        "        if all_emotions:\n",
        "            dominant_emotion = max(all_emotions, key=all_emotions.get)\n",
        "            st.success(f\"Most Frequent Emotion Across All Images or no Images(Default): {dominant_emotion}\")\n",
        "            emotion_summary = \", \".join([f\"{emotion}: {count}\" for emotion, count in all_emotions.items()])\n",
        "            analyze_with_gemini(dominant_emotion, all_emotions)\n",
        "        else:\n",
        "            st.error(\"No images processed or error in emotion analysis.\")\n",
        "\n",
        "        return extracted_texts, combined_caption\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error fetching images: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "\n",
        "# Function to classify text and display result\n",
        "def classify_text(text):\n",
        "    # Preprocess the input for each base model\n",
        "    lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "    svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "    nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "    xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "    lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "    transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "    # Pad sequences for LSTM\n",
        "    lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Get probabilities from all base models\n",
        "    lr_proba = lr_model.predict_proba(lr_features)\n",
        "    svm_proba = svm_model.predict_proba(svm_features)\n",
        "    nb_proba = nb_model.predict_proba(nb_features)\n",
        "    xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "    lstm_proba = lstm_model.predict(lstm_features)\n",
        "    transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "    # Combine probabilities as input for the meta-learner\n",
        "    stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "    # Predict using the meta-learner\n",
        "    final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "    final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    top_issue = label_encoder.inverse_transform(final_prediction)[0]\n",
        "    top_probability = final_prediction_proba[0].max()\n",
        "\n",
        "    response = get_actual_issue(text,top_issue)\n",
        "    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "        top_issue = response\n",
        "\n",
        "    # Display the results\n",
        "    st.success(f\"The most likely mental health concern from the text provided is: {top_issue} with a probability of {top_probability:.2%}\")\n",
        "\n",
        "    # Pass to a custom insight function if needed\n",
        "    get_wellbeing_insight(text, top_issue)\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "\n",
        "# Function to get wellbeing insights from Gemini model\n",
        "def get_wellbeing_insight(text, top_issue):\n",
        "    try:\n",
        "        chat_session = gemini_model.start_chat(history=[])\n",
        "        prompt = f\"\"\" The Ryff Scale is based on six factors: autonomy, environmental mastery, personal growth, positive relations with others, purpose in life, and self-acceptance. Higher total scores indicate higher psychological well-being. Following are explanations of each criterion, and an example statement from the Ryff Inventory to measure each criterion: Autonomy: High scores indicate that the respondent is independent and regulates his or her behavior independent of social pressures. An example statement for this criterion is \"I have confidence in my opinions, even if they are contrary to the general consensus.\" Environmental Mastery: High scores indicate that the respondent makes effective use of opportunities and has a sense of mastery in managing environmental factors and activities, including managing everyday affairs and creating situations to benefit personal needs. An example statement for this criterion is \"In general, I feel I am in charge of the situation in which I live.\"Personal Growth: High scores indicate that the respondent continues to develop, is welcoming to new experiences, and recognizes improvement in behavior and self over time. An example statement for this criterion is \"I think it is important to have new experiences that challenge how you think about yourself and the world.\"Positive Relations with Others: High scores reflect the respondent's engagement in meaningful relationships with others that include reciprocal empathy, intimacy, and affection. An example statement for this criterion is \"People would describe me as a giving person, willing to share my time with others.\" Purpose in Life: High scores reflect the respondent's strong goal orientation and conviction that life holds meaning. An example statement for this criterion is \"Some people wander aimlessly through life, but I am not one of them.\"Self-Acceptance: High scores reflect the respondent's positive attitude about his or her self. An example statement for this criterion is \"I like most aspects of my personality.\" Now, please use the above information, along with image captions (if added) that have been added and the mental health issue: {top_issue}, to generate a short paragraph for each of the following subtopics, discussing how the issue may relate to these factors of mental well-being: 1. **Autonomy**: How might {top_issue} impact a person's ability to be independent and self-regulate behavior? 2. **Environmental Mastery**: Discuss how {top_issue} may affect a person's ability to manage their environment and activities. 3. **Personal Growth**: What impact might {top_issue} have on an individual's development, openness to new experiences, and recognition of self-improvement? 4. **Positive Relations with Others**: How does {top_issue} influence the ability to maintain meaningful and empathetic relationships? 5. **Purpose in Life**: How might {top_issue} shape an individual's sense of purpose or goal orientation in life? 6. **Self-Acceptance**: What role does {top_issue} play in a person's self-image and acceptance of themselves? Based on these subtopics, provide practical advice to improve or reduce the impact of {top_issue}.\"\"\"\n",
        "\n",
        "        response = chat_session.send_message(prompt)\n",
        "\n",
        "        st.write(\"### Wellbeing Insight:\")\n",
        "        st.write(response.text)\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error retrieving wellbeing insights: {e}\")\n",
        "\n",
        "# Function to extract text from image using Tesseract\n",
        "def extract_text_from_image(image):\n",
        "    extracted_text = pytesseract.image_to_string(image)\n",
        "    return extracted_text.splitlines()\n",
        "\n",
        "# Function to extract text from an image using Tesseract\n",
        "def extract_text_from_image_video(image):\n",
        "    extracted_text = pytesseract.image_to_string(image)\n",
        "    return extracted_text if extracted_text else \"\"  # Return empty string if no text is found\n",
        "\n",
        "\n",
        "# Function to extract audio from a video file and classify it\n",
        "# Function to extract 20 frames from a video file\n",
        "def extract_frames(video_path, num_frames=20):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    frames = []\n",
        "    frame_interval = total_frames // num_frames  # Calculate frame interval\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n",
        "\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def transcribe_audio_from_video(video_file):\n",
        "    try:\n",
        "        # Save the uploaded video file to a temporary file\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_video_file:\n",
        "            temp_video_file.write(video_file.read())\n",
        "            temp_video_path = temp_video_file.name\n",
        "\n",
        "        audio_path = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False).name\n",
        "\n",
        "        # Extract audio from video using subprocess\n",
        "        subprocess.run([\"ffmpeg\", \"-i\", temp_video_path, \"-q:a\", \"0\", \"-map\", \"a\", audio_path, \"-y\"])\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "        # Use Whisper to transcribe the audio\n",
        "        result = whisper_model.transcribe(audio_path)\n",
        "\n",
        "        # Get the transcribed text and translate if necessary\n",
        "        transcribed_text = result[\"text\"]\n",
        "        translated_text = GoogleTranslator(source=\"auto\", target=\"en\").translate(transcribed_text)\n",
        "\n",
        "        # Clean up temporary files\n",
        "        os.remove(temp_video_path)\n",
        "        os.remove(audio_path)\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        # Display a user-friendly message if the video is too long or another error occurs\n",
        "        if \"duration\" in str(e).lower() or \"length\" in str(e).lower():\n",
        "            return \"The video is too long to process. Please upload a shorter video.\"\n",
        "        else:\n",
        "            return f\"An error occurred: {e}\"\n",
        "\n",
        "# Function to translate text using DeepL\n",
        "def translate_text(text, target_lang=\"en\"):\n",
        "    try:\n",
        "        if text:\n",
        "            translated_text = GoogleTranslator(source=\"auto\", target=target_lang).translate(text)\n",
        "            return translated_text\n",
        "        return \"\"  # Return empty string if text is empty or None\n",
        "    except Exception as e:\n",
        "        return f\"Error translating text: {str(e)}\"\n",
        "\n",
        "# Function to extract audio from a video file\n",
        "def extract_audio_from_video(video_path):\n",
        "    try:\n",
        "        # Generate a temporary audio file path\n",
        "        audio_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\").name\n",
        "\n",
        "        # Use FFmpeg to extract audio from video\n",
        "        subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\", audio_path, \"-y\"])\n",
        "\n",
        "        # Return the path of the extracted audio\n",
        "        return audio_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting audio: {str(e)}\"\n",
        "\n",
        "# Function to analyze audio mood based on extracted audio\n",
        "def analyze_audio_mood(video_path):\n",
        "    try:\n",
        "        # Extract audio from the video (assuming extract_audio_from_video is implemented)\n",
        "        audio_path = extract_audio_from_video(video_path)\n",
        "\n",
        "        # Load the audio file using librosa\n",
        "        y, sr = librosa.load(audio_path)\n",
        "\n",
        "        # Extract MFCCs (Mel-frequency cepstral coefficients) from the audio signal\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "        # Divide the MFCC array into 4 frequency bands and calculate scalar mean for each band\n",
        "\n",
        "        # Low Frequencies: MFCC 0, 1, 2\n",
        "        low_freq_mfcc = np.mean(mfcc[0:3], axis=1)\n",
        "        mean_low = np.mean(low_freq_mfcc)  # Scalar mean for low frequencies\n",
        "\n",
        "        # Mid-Low Frequencies: MFCC 3, 4\n",
        "        mid_low_freq_mfcc = np.mean(mfcc[3:5], axis=1)\n",
        "        mean_mid_low = np.mean(mid_low_freq_mfcc)  # Scalar mean for mid-low frequencies\n",
        "\n",
        "        # Mid-High Frequencies: MFCC 5, 6, 7\n",
        "        mid_high_freq_mfcc = np.mean(mfcc[5:8], axis=1)\n",
        "        mean_mid_high = np.mean(mid_high_freq_mfcc)  # Scalar mean for mid-high frequencies\n",
        "\n",
        "        # High Frequencies: MFCC 8, 9, 10, 11, 12\n",
        "        high_freq_mfcc = np.mean(mfcc[8:13], axis=1)\n",
        "        mean_high = np.mean(high_freq_mfcc)  # Scalar mean for high frequencies\n",
        "\n",
        "        # Now use these scalar means for classification\n",
        "\n",
        "        if mean_high <= mean_low and mean_high <= mean_mid_low and mean_high <= mean_mid_high:\n",
        "            return \"Audio sounds normal, with no dominant emotion detected\"\n",
        "\n",
        "        elif mean_mid_high <= mean_low and mean_mid_high <= mean_mid_low and mean_mid_high <= mean_high:\n",
        "            return \"Audio sounds neutral, calm, or peaceful\"\n",
        "\n",
        "        elif mean_mid_low <= mean_low and mean_mid_low <= mean_mid_high and mean_mid_low <= mean_high:\n",
        "            return \"Audio sounds slightly melancholic or neutral\"\n",
        "\n",
        "        elif mean_low <= mean_mid_low and mean_low <= mean_mid_high and mean_low <= mean_high:\n",
        "            return \"Audio sounds calm or melancholic, with less intensity\"\n",
        "\n",
        "        elif mean_high > mean_low and mean_high > mean_mid_low and mean_high <= mean_mid_high:\n",
        "            return \"Audio sounds depressive or anxious in nature\"\n",
        "\n",
        "        else :\n",
        "            return \"Audio sounds upbeat and energetic (Happy)\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error analyzing audio mood: {str(e)}\"\n",
        "\n",
        "\n",
        "# ----------------- Adding Retrain Model functionality\n",
        "\n",
        "# File path for dataset\n",
        "dataset_path = 'preprocessed_mental_health.csv'\n",
        "\n",
        "def update_dataset(new_text, mental_health_issue):\n",
        "    \"\"\"\n",
        "    Append new data to the dataset and save it.\n",
        "\n",
        "    Args:\n",
        "        new_text (str): The new text to be added.\n",
        "        mental_health_issue (str): The mental health issue associated with the text.\n",
        "    \"\"\"\n",
        "    if os.path.exists(dataset_path):\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "    else:\n",
        "        # If the dataset doesn't exist, create a new one\n",
        "        dataset = pd.DataFrame(columns=['text', 'mental_health_issue', 'cleaned_text'])\n",
        "\n",
        "    # Clean the text (adjust cleaning based on your preprocessing)\n",
        "    cleaned_text = new_text.lower()\n",
        "\n",
        "    # Create a new DataFrame for the new row\n",
        "    new_row = pd.DataFrame({\n",
        "        'text': [new_text],\n",
        "        'mental_health_issue': [mental_health_issue],\n",
        "        'cleaned_text': [cleaned_text]\n",
        "    })\n",
        "\n",
        "    # Use pd.concat to append the new row\n",
        "    dataset = pd.concat([dataset, new_row], ignore_index=True)\n",
        "\n",
        "    # Save the updated dataset\n",
        "    dataset.to_csv(dataset_path, index=False)\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "\n",
        "def retrain_model():\n",
        "    # Load Logistic Regression model and vectorizer\n",
        "    with open('LRmodel.pkl', 'rb') as file:\n",
        "        lr_model = pickle.load(file)\n",
        "\n",
        "    with open('LRvectorizer.pkl', 'rb') as file:\n",
        "        lr_vectorizer = pickle.load(file)\n",
        "\n",
        "    # Load SVM model and vectorizer\n",
        "    with open('SVMmodel.pkl', 'rb') as file:\n",
        "        svm_model = pickle.load(file)\n",
        "\n",
        "    with open('SVMvectorizer.pkl', 'rb') as file:\n",
        "        svm_vectorizer = pickle.load(file)\n",
        "\n",
        "    # Load XGBoost model, vectorizer, and label encoder\n",
        "    with open('xgb_model.pkl', 'rb') as file:\n",
        "        xgb_model = pickle.load(file)\n",
        "\n",
        "    with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
        "        tfidf_vectorizer = pickle.load(file)\n",
        "\n",
        "    with open('label_encoder.pkl', 'rb') as file:\n",
        "        label_encoder = pickle.load(file)\n",
        "\n",
        "    # Load LSTM model, tokenizer, and label encoder\n",
        "    lstm_model = load_model('lstm_model.h5')\n",
        "\n",
        "    with open('LSTM_tokenizer.pkl', 'rb') as file:\n",
        "        lstm_tokenizer = pickle.load(file)\n",
        "\n",
        "    # Load Naive Bayes model and vectorizer\n",
        "    with open('NBmodel.pkl', 'rb') as file:\n",
        "        nb_model = pickle.load(file)\n",
        "\n",
        "    with open('NBvectorizer.pkl', 'rb') as file:\n",
        "        nb_vectorizer = pickle.load(file)\n",
        "\n",
        "    # Load the transformer model and associated files\n",
        "    with open('Tlabel_encoder.pkl', 'rb') as file:\n",
        "        t_label_encoder = pickle.load(file)\n",
        "\n",
        "    with open('Tvectorizer_layer.pkl', 'rb') as file:\n",
        "        t_vectorize_layer = pickle.load(file)\n",
        "\n",
        "    transformer_model = load_model('Ttransformer_model.h5', custom_objects=custom_objects)\n",
        "\n",
        "    # Load the test dataset\n",
        "    data = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "    # Check if 'cleaned_text' column exists\n",
        "    if 'cleaned_text' not in data.columns:\n",
        "        raise ValueError(\"The dataset must have a 'cleaned_text' column.\")\n",
        "\n",
        "    # Remove rows with missing values in 'cleaned_text'\n",
        "    data.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "    # Split features and target\n",
        "    X_test = data['cleaned_text']\n",
        "    y_test = data['mental_health_issue']\n",
        "\n",
        "    # Encode target labels\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "\n",
        "    # Process the text for each model\n",
        "    X_test_lr = lr_vectorizer.transform(X_test)  # Logistic Regression vectorizer\n",
        "    X_test_svm = svm_vectorizer.transform(X_test)  # SVM vectorizer\n",
        "    X_test_xgb = tfidf_vectorizer.transform(X_test)  # XGBoost vectorizer\n",
        "    X_test_nb = nb_vectorizer.transform(X_test)  # Naive Bayes vectorizer\n",
        "    X_test_lstm = lstm_tokenizer.texts_to_sequences(X_test)  # LSTM tokenizer\n",
        "    # Preprocess the text for the transformer model\n",
        "    X_test_transformer = t_vectorize_layer(X_test)\n",
        "\n",
        "    # Pad sequences for LSTM\n",
        "    X_test_lstm = pad_sequences(X_test_lstm, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Get predictions from the base models\n",
        "    lr_predictions_proba = lr_model.predict_proba(X_test_lr)  # Logistic Regression probabilities\n",
        "    svm_predictions_proba = svm_model.predict_proba(X_test_svm)  # SVM probabilities\n",
        "    xgb_predictions_proba = xgb_model.predict_proba(X_test_xgb)  # XGBoost probabilities\n",
        "    nb_predictions_proba = nb_model.predict_proba(X_test_nb)  # Naive Bayes probabilities\n",
        "    lstm_predictions_proba = lstm_model.predict(X_test_lstm)  # LSTM probabilities\n",
        "    # Get probabilities from the transformer model\n",
        "    transformer_predictions_proba = transformer_model.predict(X_test_transformer)\n",
        "\n",
        "    # Stack the predictions of all models to create the feature matrix for the meta-learner\n",
        "    stacked_features = np.hstack((\n",
        "        lr_predictions_proba,\n",
        "        svm_predictions_proba,\n",
        "        xgb_predictions_proba,\n",
        "        nb_predictions_proba,\n",
        "        lstm_predictions_proba,\n",
        "        transformer_predictions_proba\n",
        "    ))\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
        "    stacked_features, y_test, test_size=0.2, random_state=42, stratify=y_test\n",
        "    )\n",
        "\n",
        "    # Train Random Forest as the meta-learner\n",
        "    meta_learner_rf = RandomForestClassifier(\n",
        "      max_depth=None,            # Maximum depth of each tree\n",
        "      min_samples_split=20,      # Minimum number of samples to split a node\n",
        "\n",
        "      min_samples_leaf=1,        # Minimum number of samples in a leaf node\n",
        "      max_features='sqrt',       # Number of features to consider at each split\n",
        "      bootstrap=False,            # Whether to use bootstrapping\n",
        "\n",
        "      random_state=42            # For reproducibility\n",
        "    )\n",
        "    meta_learner_rf.fit(X_train1, y_train1)\n",
        "\n",
        "    # Save the trained XGBoost meta-learner\n",
        "    with open('meta_learner_rf.pkl', 'wb') as file:\n",
        "        pickle.dump(meta_learner_rf, file)\n",
        "\n",
        "    # Predict using the XGBoost meta-learner\n",
        "    final_predictions_lr = meta_learner_rf.predict(X_test1)\n",
        "\n",
        "    # Evaluate the XGBoost ensemble model\n",
        "    accuracy_rf = accuracy_score(y_test1, final_predictions_lr)\n",
        "\n",
        "    return meta_learner_rf, accuracy_rf\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "\n",
        "def update_and_retrain(example_text, example_issue):\n",
        "    \"\"\"\n",
        "    Updates the dataset with new data, displays the last three rows,\n",
        "    and retrains the Logistic Regression model.\n",
        "\n",
        "    Args:\n",
        "        example_text (str): The new input text to be added.\n",
        "        example_issue (str): The predicted mental health issue to be added.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Update dataset\n",
        "        update_dataset(example_text, example_issue)\n",
        "\n",
        "        # Display the last three rows of the updated dataset\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "        st.write(\"Updated Dataset (Last 3 Rows):\")\n",
        "        st.write(dataset.tail(3))\n",
        "\n",
        "        # Notify user about retraining process\n",
        "        st.info(\"Model is being retrained...\")\n",
        "\n",
        "        # Retrain model\n",
        "        model, accuracy = retrain_model()\n",
        "\n",
        "        # Display retraining success message\n",
        "        if model:\n",
        "            st.success(f\"Model retrained successfully! Accuracy: {accuracy * 100:.2f}%\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "\n",
        "# Function to classify text, display result and retrain model\n",
        "def classify_text_retrain_model(text):\n",
        "    # Preprocess the input for each base model\n",
        "    lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "    svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "    nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "    xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "    lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "    transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "    # Pad sequences for LSTM\n",
        "    lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Get probabilities from all base models\n",
        "    lr_proba = lr_model.predict_proba(lr_features)\n",
        "    svm_proba = svm_model.predict_proba(svm_features)\n",
        "    nb_proba = nb_model.predict_proba(nb_features)\n",
        "    xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "    lstm_proba = lstm_model.predict(lstm_features)\n",
        "    transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "    # Combine probabilities as input for the meta-learner\n",
        "    stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "    # Predict using the meta-learner\n",
        "    final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "    final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    top_issue = label_encoder.inverse_transform(final_prediction)[0]\n",
        "    top_probability = final_prediction_proba[0].max()\n",
        "\n",
        "    response = get_actual_issue(text,top_issue)\n",
        "    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "        top_issue = response\n",
        "\n",
        "    # Display the results\n",
        "    st.success(f\"The most likely mental health concern from all the text obtained is: {top_issue} with a probability of {top_probability:.2%}\")\n",
        "\n",
        "    # Pass to a custom insight function if needed\n",
        "    get_wellbeing_insight(text, top_issue)\n",
        "\n",
        "    # Adding Model Retraining Functionality\n",
        "    update_and_retrain(text, top_issue)\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "\n",
        "# ----------------- Adding Retrain Model functionality\n",
        "\n",
        "\n",
        "# ---------------------- Adding Facial Recognition for video\n",
        "\n",
        "def detect_emotions_from_frame(frame):\n",
        "    try:\n",
        "        # Use DeepFace to analyze emotions\n",
        "        result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
        "        return result[0]['dominant_emotion']\n",
        "    except Exception as e:\n",
        "        print(f\"No expression or error detecting emotion: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_emotions_from_frames(frames):\n",
        "    emotion_counts = {'happy': 0, 'sad': 0, 'angry': 0, 'disgust': 0, 'fear': 0, 'surprise': 0, 'neutral': 0}\n",
        "    frame_emotions = []\n",
        "\n",
        "    for idx, frame in enumerate(frames):\n",
        "        emotion = detect_emotions_from_frame(frame)\n",
        "        if emotion:\n",
        "            frame_emotions.append(emotion)\n",
        "            if emotion in emotion_counts:\n",
        "                emotion_counts[emotion] += 1\n",
        "\n",
        "    return emotion_counts, frame_emotions\n",
        "\n",
        "def display_emotion_summary(emotion_counts):\n",
        "    # Convert the emotion counts to a DataFrame for display\n",
        "    emotion_df = pd.DataFrame(list(emotion_counts.items()), columns=['Emotion', 'Count'])\n",
        "    st.write(\"Emotion Analysis Summary:\")\n",
        "    st.table(emotion_df)\n",
        "    return max(emotion_counts, key=emotion_counts.get)\n",
        "\n",
        "def analyze_with_gemini(dominant_emotion, emotion_counts):\n",
        "    \"\"\"\n",
        "    Analyze the detected emotions using Gemini API with exception handling and chat session.\n",
        "\n",
        "    Parameters:\n",
        "        dominant_emotion (str): The most frequent emotion detected across frames.\n",
        "        emotion_counts (dict): A dictionary with emotion types as keys and their counts as values.\n",
        "\n",
        "    Returns:\n",
        "        str: The response text from the Gemini API or an error message if the API call fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Start a chat session with the Gemini API\n",
        "        chat_session = gemini_model.start_chat(history=[])\n",
        "\n",
        "        # Create a detailed summary of emotion counts for the prompt\n",
        "        emotion_summary = \", \".join([f\"{emotion}: {count}\" for emotion, count in emotion_counts.items()])\n",
        "\n",
        "        # Craft the prompt with the dominant emotion and emotion summary\n",
        "        # prompt = (\n",
        "           # f\"The detected dominant emotion is '{dominant_emotion}', and the counts for each emotion are as follows: {emotion_summary}. \"\n",
        "          #  f\"Analyze this data in the context of possible mental health issues (e.g., depression, anxiety, PTSD, or bipolar) and provide a suggestion.\"\n",
        "        #)\n",
        "        prompt = f\"The detected dominant emotion is '{dominant_emotion}'. {emotion_summary}. Based on this information, analyze the potential implications for mental health conditions such as depression, anxiety, PTSD, or bipolar disorder. Provide insights into how these emotions might relate to these mental health issues and suggest actionable advice or strategies to improve mental well-being. Give only three lines.\"\n",
        "\n",
        "\n",
        "        # Send the prompt via the chat session\n",
        "        response = chat_session.send_message(prompt)\n",
        "        st.write(response.text)\n",
        "    except Exception as e:\n",
        "        # Log the error (optional, for debugging purposes)\n",
        "        print(f\"Error in Gemini API call: {e}\")\n",
        "\n",
        "        # Return a user-friendly error message\n",
        "        return \"An error occurred while communicating with the Gemini API. Please try again later.\"\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------- Adding Facial Recognition for video\n",
        "\n",
        "# ---------------------- Adding Facial Recognition for image\n",
        "\n",
        "def detect_emotions_from_image(image):\n",
        "    \"\"\"\n",
        "    Detect the dominant emotion from a single image using DeepFace.\n",
        "\n",
        "    Parameters:\n",
        "        image (PIL.Image.Image or np.ndarray): The input image to analyze.\n",
        "\n",
        "    Returns:\n",
        "        str: The dominant emotion detected, or None if no emotion is detected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert PIL image to NumPy array if needed\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        # Use DeepFace to analyze emotions\n",
        "        result = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
        "        return result[0]['dominant_emotion']\n",
        "    except Exception as e:\n",
        "        print(f\"No expression or error detecting emotion: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_emotions_from_image(image):\n",
        "    \"\"\"\n",
        "    Analyze emotions from an image and count occurrences of each emotion.\n",
        "\n",
        "    Parameters:\n",
        "        image (PIL.Image.Image or np.ndarray): The input image to analyze.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, int], List[str]]:\n",
        "            - A dictionary with emotion counts.\n",
        "            - A list of detected emotions for each face.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert PIL image to NumPy array if needed\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        # Initialize emotion counters\n",
        "        emotion_counts = {'happy': 0, 'sad': 0, 'angry': 0, 'disgust': 0, 'fear': 0, 'surprise': 0, 'neutral': 0}\n",
        "        detected_emotions = []\n",
        "\n",
        "        # Use DeepFace to detect and analyze faces\n",
        "        results = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
        "\n",
        "        # If results contain multiple faces, process each one\n",
        "        for result in results:\n",
        "            emotion = result.get('dominant_emotion')\n",
        "            if emotion:\n",
        "                detected_emotions.append(emotion)\n",
        "                if emotion in emotion_counts:\n",
        "                    emotion_counts[emotion] += 1\n",
        "\n",
        "        return emotion_counts, detected_emotions\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing emotions from image: {e}\")\n",
        "        return {}, []\n",
        "\n",
        "\n",
        "# ---------------------- Adding Facial Recognition for image\n",
        "\n",
        "\n",
        "# ---------------------- Get Image Description\n",
        "\n",
        "# Function to load the model (cached for efficiency)\n",
        "@st.cache_resource\n",
        "def load_model_img():\n",
        "    model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "    feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, feature_extractor, tokenizer\n",
        "\n",
        "# Load the model\n",
        "IDmodel, IDfeature_extractor, IDtokenizer = load_model_img()\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IDmodel.to(device)\n",
        "\n",
        "# Function to generate caption\n",
        "def generate_caption(image):\n",
        "    # Preprocess the image\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(mode=\"RGB\")\n",
        "    pixel_values = IDfeature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    # Generate caption (you can adjust max_length and num_beams as needed)\n",
        "    with torch.no_grad():\n",
        "        output_ids = IDmodel.generate(pixel_values, max_length=16, num_beams=4)\n",
        "    caption = IDtokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "\n",
        "def classify_text_with_desc(text,text2):\n",
        "    # Preprocess the input for each base model\n",
        "    lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "    svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "    nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "    xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "    lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "    transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "    # Pad sequences for LSTM\n",
        "    lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Get probabilities from all base models\n",
        "    lr_proba = lr_model.predict_proba(lr_features)\n",
        "    svm_proba = svm_model.predict_proba(svm_features)\n",
        "    nb_proba = nb_model.predict_proba(nb_features)\n",
        "    xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "    lstm_proba = lstm_model.predict(lstm_features)\n",
        "    transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "    # Combine probabilities as input for the meta-learner\n",
        "    stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "    # Predict using the meta-learner\n",
        "    final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "    final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    top_issue = label_encoder.inverse_transform(final_prediction)[0]\n",
        "    top_probability = final_prediction_proba[0].max()\n",
        "\n",
        "    response = get_actual_issue(text+\" \"+text2,top_issue)\n",
        "    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "        top_issue = response\n",
        "\n",
        "    # Display the results\n",
        "    st.success(f\"The most likely mental health concern from all the text obtained is: {top_issue} with a probability of {top_probability:.2%}\")\n",
        "\n",
        "    get_wellbeing_insight(text+\" \"+text2, top_issue)\n",
        "\n",
        "def classify_text_retrain_model_desc(text,text2):\n",
        "    # Preprocess the input for each base model\n",
        "    lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "    svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "    nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "    xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "    lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "    transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "    # Pad sequences for LSTM\n",
        "    lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Get probabilities from all base models\n",
        "    lr_proba = lr_model.predict_proba(lr_features)\n",
        "    svm_proba = svm_model.predict_proba(svm_features)\n",
        "    nb_proba = nb_model.predict_proba(nb_features)\n",
        "    xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "    lstm_proba = lstm_model.predict(lstm_features)\n",
        "    transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "    # Combine probabilities as input for the meta-learner\n",
        "    stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "    # Predict using the meta-learner\n",
        "    final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "    final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    top_issue = label_encoder.inverse_transform(final_prediction)[0]\n",
        "    top_probability = final_prediction_proba[0].max()\n",
        "\n",
        "    response = get_actual_issue(text+\" \"+text2,top_issue)\n",
        "    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "        top_issue = response\n",
        "\n",
        "    # Display the results\n",
        "    st.success(f\"The most likely mental health concern from all the text obtained is: {top_issue} with a probability of {top_probability:.2%}\")\n",
        "\n",
        "    get_wellbeing_insight(text+\" \"+text2, top_issue)\n",
        "\n",
        "    # Adding Model Retraining Functionality\n",
        "    update_and_retrain(text, top_issue)\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "\n",
        "\n",
        "# ---------------------- Get Image Description\n",
        "\n",
        "\n",
        "# ---------------------- Get Video Description\n",
        "\n",
        "# Function to load the model (cached for efficiency)\n",
        "@st.cache_resource\n",
        "def load_image_captioning_model():\n",
        "    model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "    feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, feature_extractor, tokenizer\n",
        "\n",
        "@st.cache_resource\n",
        "def load_summary_pipeline():\n",
        "    return pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Load models\n",
        "caption_model, Vfeature_extractor, Vtokenizer = load_image_captioning_model()\n",
        "summary_pipeline = load_summary_pipeline()\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "caption_model.to(device)\n",
        "\n",
        "# Function to generate captions for an image\n",
        "def generate_caption_video(image):\n",
        "    # Convert the numpy array (video frame) to a PIL Image\n",
        "    if isinstance(image, np.ndarray):\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(mode=\"RGB\")\n",
        "\n",
        "    # Preprocess the image\n",
        "    pixel_values = Vfeature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        output_ids = caption_model.generate(pixel_values, max_length=16, num_beams=4)\n",
        "    caption = Vtokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Function to generate an overall description of the video\n",
        "def describe_video(frames):\n",
        "    # Generate captions for each frame\n",
        "    captions = [generate_caption_video(frame) for frame in frames]\n",
        "    combined_captions = \" \".join(captions)\n",
        "\n",
        "    # Summarize the captions to get an overall description\n",
        "    summary = summary_pipeline(combined_captions, max_length=50, min_length=25, do_sample=False)[0][\"summary_text\"]\n",
        "    return captions, summary\n",
        "\n",
        "# ---------------------- Get Video Description\n",
        "def get_actual_issue(text,top_issue):\n",
        "    try:\n",
        "        chat_session = gemini_model.start_chat(history=[])\n",
        "        prompt = f\"The given text is : {text}. Is this statement normal or depression or ptsd or anxiety or bipolar? I need only one word answer. I need no explanation. My model gave {top_issue}. I want to confirm. Please give the exactly correct one word answer about what you think.\"\n",
        "        response = chat_session.send_message(prompt)\n",
        "        return response.text.strip().lower()\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Define the Streamlit app\n",
        "def run_app():\n",
        "    st.title(\"Mental Health Disorder Detection\")\n",
        "\n",
        "    option = st.sidebar.selectbox(\n",
        "        \"Choose an option\",\n",
        "        [\"Text Input\", \"Image Upload\", \"Video Upload\", \"Reddit Username Analysis\", \"Twitter Username Analysis\"]\n",
        "    )\n",
        "\n",
        "    # Text Input\n",
        "    if option == \"Text Input\":\n",
        "        st.subheader(\"Enter Text to Classify Mental Health Issue\")\n",
        "        input_text = st.text_area(\"Enter your text here:\")\n",
        "\n",
        "        if st.button(\"Classify Text\"):\n",
        "            if input_text.strip() == \"\":\n",
        "                st.write(\"Please enter some text to classify.\")\n",
        "            else:\n",
        "                translated_text = GoogleTranslator(source='auto', target='en').translate(input_text)\n",
        "                st.write(\"Translated Text (to English):\")\n",
        "                st.write(translated_text)\n",
        "                classify_text(translated_text)\n",
        "        # Adding model retraining\n",
        "        elif st.button(\"Classify Text and Retrain Model\"):\n",
        "            if input_text.strip() == \"\":\n",
        "                st.write(\"Please enter some text to classify.\")\n",
        "            else:\n",
        "                translated_text = GoogleTranslator(source='auto', target='en').translate(input_text)\n",
        "                st.subheader(\"Translated Text (to English):\")\n",
        "                st.write(translated_text)\n",
        "                classify_text_retrain_model(translated_text)\n",
        "\n",
        "\n",
        "    # Image Upload\n",
        "    elif option == \"Image Upload\":\n",
        "        st.subheader(\"Upload an Image to Extract and Classify Text\")\n",
        "        uploaded_image = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"jpeg\", \"png\", \"webp\", \"bmp\", \"tiff\"])\n",
        "\n",
        "        if uploaded_image is not None:\n",
        "            image = Image.open(uploaded_image)\n",
        "            st.image(image, caption=\"Uploaded Image\", use_container_width=True)\n",
        "\n",
        "            # Generate and display the caption\n",
        "            caption = generate_caption(image)\n",
        "            st.success(caption)\n",
        "\n",
        "            # Step 1: Extract text from the image\n",
        "            extracted_text = extract_text_from_image(image)\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(\"\\n\".join(extracted_text))\n",
        "\n",
        "            st.subheader(\"Translated Text (to English)\")\n",
        "            st.text(translated_text)\n",
        "\n",
        "            # Step 2: Detect faces and analyze emotions\n",
        "            # st.write(\"Detecting faces and analyzing emotions...\")\n",
        "            emotion_counts, detected_emotions = analyze_emotions_from_image(image)\n",
        "\n",
        "            if emotion_counts:\n",
        "                # Display emotion counts in a table\n",
        "                # st.write(\"Detected Emotions:\")\n",
        "                st.table(pd.DataFrame.from_dict(emotion_counts, orient=\"index\", columns=[\"Count\"]))\n",
        "\n",
        "                # Determine the dominant emotion\n",
        "                dominant_emotion = max(emotion_counts, key=emotion_counts.get)\n",
        "                st.success(f\"Dominant Emotion: **{dominant_emotion}**\")\n",
        "\n",
        "                # Step 2: Use Gemini API for mental health analysis\n",
        "                # st.write(\"### Facial Recognition Insight\")\n",
        "                analyze_with_gemini(dominant_emotion, emotion_counts)\n",
        "\n",
        "            else:\n",
        "                st.error(\"No faces detected in the uploaded image.\")\n",
        "\n",
        "            # Step 3: Classify extracted text\n",
        "            if st.button(\"Classify Extracted Text\"):\n",
        "                if not translated_text or translated_text.strip() == \"\":\n",
        "                    st.write(\"It is normal with probability 100%\")\n",
        "                else:\n",
        "                    classify_text_with_desc(translated_text,caption)\n",
        "\n",
        "            # Adding model retraining option\n",
        "            elif st.button(\"Classify Extracted Text and Retrain Model\"):\n",
        "                if not translated_text or translated_text.strip() == \"\":\n",
        "                    st.write(\"It is normal with probability 100%\")\n",
        "                else:\n",
        "                    classify_text_retrain_model_desc(translated_text,caption)\n",
        "\n",
        "\n",
        "    # Video Upload\n",
        "    elif option == \"Video Upload\":\n",
        "        st.subheader(\"Upload a Video to Extract and Classify Text\")\n",
        "        # File upload widget\n",
        "        video_file = st.file_uploader(\"Choose a video file\", type=[\"mp4\", \"mov\", \"avi\"])\n",
        "\n",
        "        if video_file:\n",
        "            # Save the uploaded video file temporarily\n",
        "            video_path = \"/tmp/uploaded_video.mp4\"\n",
        "            with open(video_path, \"wb\") as f:\n",
        "                f.write(video_file.getbuffer())\n",
        "\n",
        "            st.video(video_file)  # Display the uploaded video\n",
        "\n",
        "            # Extract frames from the uploaded video\n",
        "            frames = extract_frames(video_path)\n",
        "            combined_text = \"\"\n",
        "\n",
        "            st.subheader(\"Extracting frames from video...\")\n",
        "\n",
        "            # Emotion recognition\n",
        "            emotion_counts, frame_emotions = analyze_emotions_from_frames(frames)\n",
        "\n",
        "            # Display results\n",
        "            for idx, emotion in enumerate(frame_emotions):\n",
        "                st.image(frames[idx], caption=f\"Frame {idx + 1} - Emotion: {emotion}\")\n",
        "\n",
        "            # Display summary table and most frequent emotion\n",
        "            dominant_emotion = display_emotion_summary(emotion_counts)\n",
        "            st.success(f\"Dominant Emotion: **{dominant_emotion}**\")\n",
        "\n",
        "            # Use the dominant emotion and emotion counts to craft a Gemini API prompt\n",
        "            # st.write(\"### Facial Recognition Insight\")\n",
        "            analyze_with_gemini(dominant_emotion, emotion_counts)\n",
        "\n",
        "            # st.write(\"Gemini API Response:\")\n",
        "            #st.text(gemini_response)\n",
        "\n",
        "            for idx, frame in enumerate(frames):\n",
        "                # st.image(frame, caption=f\"Frame {idx + 1}\", use_column_width=True)\n",
        "                text_from_frame = extract_text_from_image_video(frame)\n",
        "\n",
        "                if text_from_frame and text_from_frame not in combined_text:\n",
        "                    combined_text += text_from_frame + \" \"\n",
        "\n",
        "            # Generate and display descriptions\n",
        "            frame_captions, overall_description = describe_video(frames)\n",
        "            st.subheader(\"Overall Description\")\n",
        "            st.success(overall_description)\n",
        "\n",
        "\n",
        "            st.subheader(\"Text Extracted from Video Frames:\")\n",
        "            st.text(combined_text)\n",
        "\n",
        "            # Translate the extracted text from frames\n",
        "            translated_frame_text = translate_text(combined_text)\n",
        "            # st.write(\"Translated Text from Video Frames:\")\n",
        "            # st.text(translated_frame_text)\n",
        "\n",
        "            # Extract audio and transcribe it\n",
        "            # st.write(\"Transcribing Audio from Video...\")\n",
        "            transcribed_audio_text = transcribe_audio_from_video(video_file)\n",
        "\n",
        "            st.subheader(\"Transcribed Audio Text:\")\n",
        "            st.text(transcribed_audio_text)\n",
        "\n",
        "            translated_audio_text = translate_text(transcribed_audio_text)\n",
        "            # st.write(\"Translated Audio Text:\")\n",
        "            # st.text(translated_audio_text)\n",
        "\n",
        "            # Combine the text extracted from both images and audio\n",
        "            full_combined_text = combined_text + \" \" + transcribed_audio_text\n",
        "            st.subheader(\"Combined Extracted Text (from both video frames and audio):\")\n",
        "            st.text(full_combined_text)\n",
        "\n",
        "            translated_combined_text = translate_text(full_combined_text)\n",
        "            st.subheader(\"Translated Combined Text (Frames + Audio):\")\n",
        "            st.text(translated_combined_text)\n",
        "\n",
        "            # Analyze audio mood\n",
        "            st.subheader(\"Analyzing Audio Mood...\")\n",
        "            mood_result = analyze_audio_mood(video_path)\n",
        "            st.write(mood_result)\n",
        "\n",
        "            cleaned_text = re.sub(r\"[^a-zA-Z0-9.,!? ]\", \"\", translated_combined_text)\n",
        "\n",
        "            if st.button(\"Classify Extracted Text\"):\n",
        "                if not cleaned_text or cleaned_text.strip() == \"\":\n",
        "                    # If audio_text is empty or contains only whitespace\n",
        "                    st.write(\"It is normal with probability 100%\")\n",
        "                else:\n",
        "                    classify_text_with_desc(cleaned_text,overall_description)\n",
        "            # Adding model retraining\n",
        "            elif st.button(\"Classify Extracted Text and Retrain Model\"):\n",
        "                if not cleaned_text or cleaned_text.strip() == \"\":\n",
        "                    # If audio_text is empty or contains only whitespace\n",
        "                    st.write(\"It is normal with probability 100%\")\n",
        "                else:\n",
        "                    classify_text_retrain_model_desc(cleaned_text,overall_description)\n",
        "\n",
        "\n",
        "    # Reddit Username Analysis\n",
        "    elif option == \"Reddit Username Analysis\":\n",
        "        st.subheader(\"Enter Reddit Username for Analysis\")\n",
        "        username = st.text_input(\"Enter Reddit username:\")\n",
        "\n",
        "        if st.button(\"Analyze\"):\n",
        "            if username.strip() == \"\":\n",
        "                st.write(\"Please enter a Reddit username.\")\n",
        "            else:\n",
        "                # Fetch and display text posts\n",
        "                text_posts = fetch_user_text_posts(username)\n",
        "                if text_posts:\n",
        "                    st.write(\"Recent Text Posts:\")\n",
        "                    st.write(text_posts[:3])  # Display a few posts for review\n",
        "\n",
        "                # Fetch and display image-based posts with extracted text\n",
        "                image_texts, image_caption = fetch_user_images_and_extract_text(username)\n",
        "\n",
        "                # Combine text from both text posts and image text\n",
        "                all_text = text_posts + image_texts\n",
        "                if all_text:\n",
        "                    predictions = []\n",
        "\n",
        "                    for text in all_text:\n",
        "                        # Preprocess the input for each base model\n",
        "                        lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "                        svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "                        nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "                        xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "                        lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "                        transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "                        # Pad sequences for LSTM\n",
        "                        lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "                        # Get probabilities from all base models\n",
        "                        lr_proba = lr_model.predict_proba(lr_features)\n",
        "                        svm_proba = svm_model.predict_proba(svm_features)\n",
        "                        nb_proba = nb_model.predict_proba(nb_features)\n",
        "                        xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "                        lstm_proba = lstm_model.predict(lstm_features)\n",
        "                        transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "                        # Combine probabilities as input for the meta-learner\n",
        "                        stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "                        # Predict using the meta-learner\n",
        "                        final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "                        final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "                        decoded_prediction = label_encoder.inverse_transform(final_prediction)[0]\n",
        "\n",
        "                        # Append the prediction\n",
        "                        predictions.append(decoded_prediction)\n",
        "\n",
        "                    # Count the most common mental health issue\n",
        "                    issue_counts = Counter(predictions)\n",
        "                    top_issue, top_count = issue_counts.most_common(1)[0]\n",
        "                    top_percentage = (top_count / len(predictions)) * 100\n",
        "\n",
        "                    response = get_actual_issue(\" \".join(all_text)+\" Image captions are as follows : \"+image_caption,top_issue)\n",
        "                    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "                        top_issue = response\n",
        "\n",
        "                    # Display results\n",
        "                    st.success(f\"The most frequently detected mental health concern from all the text obtained is: {top_issue} with a probability of {top_percentage:.2f}% from the analyzed text.\")\n",
        "                    issue_distribution = pd.DataFrame(issue_counts.items(), columns=['Mental Health Issue', 'Count'])\n",
        "                    st.write(\"Mental health issue distribution across posts:\")\n",
        "                    st.write(issue_distribution)\n",
        "\n",
        "                    # Call the Gemini model to get well-being insights\n",
        "                    get_wellbeing_insight(\" \".join(all_text)+\" Image captions are as follows : \"+image_caption, top_issue)\n",
        "\n",
        "                    # Adding Model Retraining Functionality\n",
        "                    # update_and_retrain(\" \".join(all_text), top_issue)\n",
        "\n",
        "                else:\n",
        "                    st.write(\"No valid text found for analysis.\")\n",
        "        # Adding Model Retraining\n",
        "        elif st.button(\"Analyze and Retrain Model\"):\n",
        "            if username.strip() == \"\":\n",
        "                st.write(\"Please enter a Reddit username.\")\n",
        "            else:\n",
        "                # Fetch and display text posts\n",
        "                text_posts = fetch_user_text_posts(username)\n",
        "                if text_posts:\n",
        "                    st.write(\"Recent Text Posts:\")\n",
        "                    st.write(text_posts[:3])  # Display a few posts for review\n",
        "\n",
        "                # Fetch and display image-based posts with extracted text\n",
        "                image_texts, image_caption = fetch_user_images_and_extract_text(username)\n",
        "\n",
        "                # Combine text from both text posts and image text\n",
        "                all_text = text_posts + image_texts\n",
        "                if all_text:\n",
        "                    predictions = []\n",
        "                    for text in all_text:\n",
        "                        # Preprocess the input for each base model\n",
        "                        lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "                        svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "                        nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "                        xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "                        lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "                        transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "                        # Pad sequences for LSTM\n",
        "                        lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "                        # Get probabilities from all base models\n",
        "                        lr_proba = lr_model.predict_proba(lr_features)\n",
        "                        svm_proba = svm_model.predict_proba(svm_features)\n",
        "                        nb_proba = nb_model.predict_proba(nb_features)\n",
        "                        xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "                        lstm_proba = lstm_model.predict(lstm_features)\n",
        "                        transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "                        # Combine probabilities as input for the meta-learner\n",
        "                        stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "                        # Predict using the meta-learner\n",
        "                        final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "                        final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "                        decoded_prediction = label_encoder.inverse_transform(final_prediction)[0]\n",
        "\n",
        "                        # Append the prediction\n",
        "                        predictions.append(decoded_prediction)\n",
        "\n",
        "                    # Count the most common mental health issue\n",
        "                    issue_counts = Counter(predictions)\n",
        "                    top_issue, top_count = issue_counts.most_common(1)[0]\n",
        "                    top_percentage = (top_count / len(predictions)) * 100\n",
        "\n",
        "                    response = get_actual_issue(\" \".join(all_text)+\" Image captions are as follows : \"+image_caption,top_issue)\n",
        "                    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "                        top_issue = response\n",
        "\n",
        "                    # Display results\n",
        "                    st.success(f\"The most frequently detected mental health concern from all the text obtained is: {top_issue} with a probability of {top_percentage:.2f}% from the analyzed text.\")\n",
        "                    issue_distribution = pd.DataFrame(issue_counts.items(), columns=['Mental Health Issue', 'Count'])\n",
        "                    st.write(\"Mental health issue distribution across posts:\")\n",
        "                    st.write(issue_distribution)\n",
        "\n",
        "                    # Call the Gemini model to get well-being insights\n",
        "                    get_wellbeing_insight(\" \".join(all_text)+\" The image captions are as follows : \"+image_caption, top_issue)\n",
        "\n",
        "                    # Adding Model Retraining Functionality\n",
        "                    update_and_retrain(\" \".join(all_text), top_issue)\n",
        "\n",
        "                else:\n",
        "                    st.write(\"No valid text found for analysis.\")\n",
        "\n",
        "\n",
        "    # Twitter Username Analysis\n",
        "    elif option == \"Twitter Username Analysis\":\n",
        "        st.subheader(\"Enter Twitter Username for Analysis\")\n",
        "        username = st.text_input(\"Enter Twitter username:\")\n",
        "\n",
        "        if st.button(\"Analyze\"):\n",
        "            if username.strip() == \"\":\n",
        "                st.write(\"Please enter a Twitter username.\")\n",
        "            else:\n",
        "                # Fetch the latest tweets with associated images\n",
        "                tweets_with_images = get_latest_tweets_with_images(username)\n",
        "\n",
        "                # Extract text content from tweets\n",
        "                text_posts = [tweet['text'] for tweet in tweets_with_images if tweet['text']]\n",
        "                st.write(\"Recent Text Posts from Tweets:\")\n",
        "                st.write(text_posts[:3])  # Display a few posts for review\n",
        "\n",
        "                # Extract and process text from associated images\n",
        "                image_texts = []\n",
        "                all_emotions = {'happy': 0, 'sad': 0, 'angry': 0, 'disgust': 0, 'fear': 0, 'surprise': 0, 'neutral': 0}\n",
        "                combined_caption = \"\"\n",
        "                for tweet in tweets_with_images:\n",
        "                    for image_url in tweet['images']:\n",
        "                        image = fetch_image_content(image_url)\n",
        "                        if image:\n",
        "                            st.image(image, caption=f\"Image from Tweet\", use_container_width=True)\n",
        "                            # Generate and display the caption\n",
        "                            caption = generate_caption(image)\n",
        "                            st.success(caption)\n",
        "                            combined_caption += caption + \" \"\n",
        "                        if image:\n",
        "                            extracted_text = extract_text_from_image(image)  # Assuming a text extraction function is defined\n",
        "                            if extracted_text:\n",
        "                                image_texts.append(extracted_text)\n",
        "                            # Analyze facial emotions in the image\n",
        "                            dominant_emotion = detect_emotions_from_image(image)\n",
        "                            if dominant_emotion:\n",
        "                                st.success(f\"Dominant Emotion Detected: {dominant_emotion}\")\n",
        "                                all_emotions[dominant_emotion] += 1\n",
        "                            else:\n",
        "                                st.error(\"No faces detected or error in emotion analysis.\")\n",
        "\n",
        "                # After processing all images, analyze the emotion counts and provide a suggestion\n",
        "                if all_emotions:\n",
        "                    dominant_emotion = max(all_emotions, key=all_emotions.get)\n",
        "                    st.success(f\"Most Frequent Emotion Across All Images or no Images(Default): {dominant_emotion}\")\n",
        "                    emotion_summary = \", \".join([f\"{emotion}: {count}\" for emotion, count in all_emotions.items()])\n",
        "                    analyze_with_gemini(dominant_emotion, all_emotions)\n",
        "                else:\n",
        "                    st.error(\"No images processed or error in emotion analysis.\")\n",
        "\n",
        "                # Combine text from both tweet text and extracted image text\n",
        "                all_text = text_posts + image_texts\n",
        "\n",
        "                # Ensure all entries in all_text are strings\n",
        "                all_text = [str(text) for text in all_text if text]\n",
        "\n",
        "                if all_text:\n",
        "                    predictions = []\n",
        "                    for text in all_text:\n",
        "                        try:\n",
        "                            # Preprocess the input for each base model\n",
        "                            lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "                            svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "                            nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "                            xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "                            lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "                            transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "                            # Pad sequences for LSTM\n",
        "                            lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "                            # Get probabilities from all base models\n",
        "                            lr_proba = lr_model.predict_proba(lr_features)\n",
        "                            svm_proba = svm_model.predict_proba(svm_features)\n",
        "                            nb_proba = nb_model.predict_proba(nb_features)\n",
        "                            xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "                            lstm_proba = lstm_model.predict(lstm_features)\n",
        "                            transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "                            # Combine probabilities as input for the meta-learner\n",
        "                            stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "                            # Predict using the meta-learner\n",
        "                            final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "                            final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "                            decoded_prediction = label_encoder.inverse_transform(final_prediction)[0]\n",
        "\n",
        "                            # Append the prediction\n",
        "                            predictions.append(decoded_prediction)\n",
        "                        except Exception as e:\n",
        "                            st.write(f\"Error processing text: {text[:50]}... - {e}\")\n",
        "                            continue\n",
        "\n",
        "                    # Count the most common mental health issue\n",
        "                    issue_counts = Counter(predictions)\n",
        "                    top_issue, top_count = issue_counts.most_common(1)[0]\n",
        "                    top_percentage = (top_count / len(predictions)) * 100\n",
        "\n",
        "                    response = get_actual_issue(\" \".join(all_text)+\" The image captions are as follows :  \"+combined_caption,top_issue)\n",
        "                    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "                        top_issue = response\n",
        "\n",
        "                    st.success(f\"The most frequently detected mental health concern obtained from all the text obtained is: {top_issue}, with a probability of {top_percentage:.2f}% from the analyzed text.\")\n",
        "                    issue_distribution = pd.DataFrame(issue_counts.items(), columns=['Mental Health Issue', 'Count'])\n",
        "                    st.write(\"Mental health issue distribution across posts:\")\n",
        "                    st.write(issue_distribution)\n",
        "\n",
        "                    # Call the Gemini model to get well-being insights\n",
        "                    get_wellbeing_insight(\" \".join(all_text)+\" The image captions are as follows :  \"+combined_caption, top_issue)\n",
        "\n",
        "                    # Adding Model Retraining Functionality\n",
        "                    # update_and_retrain(\" \".join(all_text), top_issue)\n",
        "\n",
        "                else:\n",
        "                    st.write(\"No valid text found for analysis.\")\n",
        "\n",
        "        # Adding Retrain Model\n",
        "        elif st.button(\"Analyze and Retrain Model\"):\n",
        "            if username.strip() == \"\":\n",
        "                st.write(\"Please enter a Twitter username.\")\n",
        "            else:\n",
        "                # Fetch the latest tweets with associated images\n",
        "                tweets_with_images = get_latest_tweets_with_images(username)\n",
        "\n",
        "                # Extract text content from tweets\n",
        "                text_posts = [tweet['text'] for tweet in tweets_with_images if tweet['text']]\n",
        "                st.write(\"Recent Text Posts from Tweets:\")\n",
        "                st.write(text_posts[:3])  # Display a few posts for review\n",
        "\n",
        "                # Extract and process text from associated images\n",
        "                image_texts = []\n",
        "                all_emotions = {'happy': 0, 'sad': 0, 'angry': 0, 'disgust': 0, 'fear': 0, 'surprise': 0, 'neutral': 0}\n",
        "                combined_caption = \"\"\n",
        "                for tweet in tweets_with_images:\n",
        "                    for image_url in tweet['images']:\n",
        "                        image = fetch_image_content(image_url)\n",
        "                        if image:\n",
        "                            st.image(image, caption=f\"Image from Tweet\", use_container_width=True)\n",
        "                            # Generate and display the caption\n",
        "                            caption = generate_caption(image)\n",
        "                            st.success(caption)\n",
        "                            combined_caption += caption + \" \"\n",
        "                        if image:\n",
        "                            extracted_text = extract_text_from_image(image)  # Assuming a text extraction function is defined\n",
        "                            if extracted_text:\n",
        "                                image_texts.append(extracted_text)\n",
        "                            # Analyze facial emotions in the image\n",
        "                            dominant_emotion = detect_emotions_from_image(image)\n",
        "                            if dominant_emotion:\n",
        "                                st.success(f\"Dominant Emotion Detected: {dominant_emotion}\")\n",
        "                                all_emotions[dominant_emotion] += 1\n",
        "                            else:\n",
        "                                st.error(\"No faces detected or error in emotion analysis.\")\n",
        "\n",
        "                # After processing all images, analyze the emotion counts and provide a suggestion\n",
        "                if all_emotions:\n",
        "                    dominant_emotion = max(all_emotions, key=all_emotions.get)\n",
        "                    st.success(f\"Most Frequent Emotion Across All Images or no Images(Default): {dominant_emotion}\")\n",
        "                    emotion_summary = \", \".join([f\"{emotion}: {count}\" for emotion, count in all_emotions.items()])\n",
        "                    analyze_with_gemini(dominant_emotion, all_emotions)\n",
        "                else:\n",
        "                    st.error(\"No images processed or error in emotion analysis.\")\n",
        "\n",
        "                # Combine text from both tweet text and extracted image text\n",
        "                all_text = text_posts + image_texts\n",
        "\n",
        "                # Ensure all entries in all_text are strings\n",
        "                all_text = [str(text) for text in all_text if text]\n",
        "\n",
        "                if all_text:\n",
        "                    predictions = []\n",
        "                    for text in all_text:\n",
        "                        try:\n",
        "                            # Preprocess the input for each base model\n",
        "                            lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "                            svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "                            nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "                            xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "                            lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "                            transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "                            # Pad sequences for LSTM\n",
        "                            lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "                            # Get probabilities from all base models\n",
        "                            lr_proba = lr_model.predict_proba(lr_features)\n",
        "                            svm_proba = svm_model.predict_proba(svm_features)\n",
        "                            nb_proba = nb_model.predict_proba(nb_features)\n",
        "                            xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "                            lstm_proba = lstm_model.predict(lstm_features)\n",
        "                            transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "                            # Combine probabilities as input for the meta-learner\n",
        "                            stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "                            # Predict using the meta-learner\n",
        "                            final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "                            final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "                            decoded_prediction = label_encoder.inverse_transform(final_prediction)[0]\n",
        "\n",
        "                            # Append the prediction\n",
        "                            predictions.append(decoded_prediction)\n",
        "                        except Exception as e:\n",
        "                            st.write(f\"Error processing text: {text[:50]}... - {e}\")\n",
        "                            continue\n",
        "\n",
        "                    # Count the most common mental health issue\n",
        "                    issue_counts = Counter(predictions)\n",
        "                    top_issue, top_count = issue_counts.most_common(1)[0]\n",
        "                    top_percentage = (top_count / len(predictions)) * 100\n",
        "\n",
        "                    response = get_actual_issue(\" \".join(all_text)+\" The image captions are as follows :  \"+combined_caption,top_issue)\n",
        "                    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "                        top_issue = response\n",
        "\n",
        "                    st.success(f\"The most frequently detected mental health concern from all the text obtained is: {top_issue}, with a probability of {top_percentage:.2f}% from the analyzed text.\")\n",
        "                    issue_distribution = pd.DataFrame(issue_counts.items(), columns=['Mental Health Issue', 'Count'])\n",
        "                    st.write(\"Mental health issue distribution across posts:\")\n",
        "                    st.write(issue_distribution)\n",
        "\n",
        "                    # Call the Gemini model to get well-being insights\n",
        "                    get_wellbeing_insight(\" \".join(all_text)+\" The image options are as follows :  \"+combined_caption, top_issue)\n",
        "\n",
        "                    # Adding Model Retraining Functionality\n",
        "                    update_and_retrain(\" \".join(all_text), top_issue)\n",
        "\n",
        "                else:\n",
        "                    st.write(\"No valid text found for analysis.\")\n",
        "\n",
        "\n",
        "# Run the app\n",
        "if __name__ == '__main__':\n",
        "    run_app()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your authtoken\n",
        "ngrok.set_auth_token(\"2ohUKqk37HcGbvwN0s8Y1E2WNxE_39z1gVF3bYq9vFSEm7Wzq\") # Replace YOUR_AUTHTOKEN with your actual authtoken\n",
        "\n",
        "# Kill any existing ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit with nohup\n",
        "!nohup streamlit run v11.py &\n",
        "\n",
        "# Create a public URL with ngrok to access the app\n",
        "public_url = ngrok.connect(addr='8501')\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_wbOymdKD8E",
        "outputId": "5528dd15-a0f3-4587-83e4-a07fe4f2d03a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "Public URL: NgrokTunnel: \"https://6080-35-231-199-138.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "-YNvtCe4LVIo"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}