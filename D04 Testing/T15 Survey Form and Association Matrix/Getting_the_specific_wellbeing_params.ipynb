{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Weighted Sum"
      ],
      "metadata": {
        "id": "Ku8kApJUruFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q30n582LqMxk",
        "outputId": "7d080e1d-1bb9-472b-aeef-c5ac78eabbdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/am.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define row names (assuming they are in order in the dataset)\n",
        "row_names = [\n",
        "    \"self acceptance\", \"positive relations with others\", \"autonomy\",\n",
        "    \"environmental mastery\", \"purpose in life\", \"personal growth\"\n",
        "]\n",
        "\n",
        "# Extract the probability columns (assumed order: anxiety, bipolar, depression, normal, ptsd)\n",
        "issue_columns = [\"anxiety\", \"bipolar\", \"depression\", \"normal\", \"ptsd\"]\n",
        "matrix = df[issue_columns].values  # Convert to NumPy matrix\n",
        "\n",
        "# Define example probabilities (ensure they sum to 1 or adjust accordingly)\n",
        "probabilities = np.array([0.2, 0.3, 0.1, 0.3, 0.1])\n",
        "\n",
        "# Compute weighted sums for each row\n",
        "weighted_sums = np.dot(matrix, probabilities)\n",
        "\n",
        "# Find the index of the row with the highest weighted sum\n",
        "max_index = np.argmax(weighted_sums)\n",
        "max_row_name = row_names[max_index]\n",
        "\n",
        "# Display results in Streamlit\n",
        "st.title(\"Weighted Sum Analysis\")\n",
        "\n",
        "# Show original table\n",
        "st.subheader(\"Data Table\")\n",
        "st.table(df)\n",
        "\n",
        "# Show computed weighted sums\n",
        "st.subheader(\"Weighted Sums\")\n",
        "df[\"Weighted Sum\"] = weighted_sums\n",
        "st.dataframe(df[[\"Weighted Sum\"]])\n",
        "\n",
        "# Display the highest weighted sum row\n",
        "st.subheader(\"Row with Highest Weighted Sum\")\n",
        "st.write(f\"Index: {max_index}\")\n",
        "st.write(f\"Row Name: {max_row_name}\")\n",
        "st.write(f\"Weighted Sum: {weighted_sums[max_index]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarity"
      ],
      "metadata": {
        "id": "kP8QwMFzxWBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test2.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity # new added\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/am.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define row names (assuming they are in order in the dataset)\n",
        "row_names = [\n",
        "    \"self acceptance\", \"positive relations with others\", \"autonomy\",\n",
        "    \"environmental mastery\", \"purpose in life\", \"personal growth\"\n",
        "]\n",
        "\n",
        "# Extract the probability columns (assumed order: anxiety, bipolar, depression, normal, ptsd)\n",
        "issue_columns = [\"anxiety\", \"bipolar\", \"depression\", \"normal\", \"ptsd\"]\n",
        "matrix = df[issue_columns].values  # Convert to NumPy matrix\n",
        "\n",
        "# Define example probabilities (vector to compare against)\n",
        "probabilities = np.array([0.2, 0.3, 0.1, 0.3, 0.1]).reshape(1, -1)  # Reshape for cosine similarity\n",
        "\n",
        "# Compute cosine similarity for each row\n",
        "cosine_similarities = cosine_similarity(matrix, probabilities).flatten()\n",
        "\n",
        "# Find the index of the row with the highest cosine similarity\n",
        "max_index = np.argmax(cosine_similarities)\n",
        "max_row_name = row_names[max_index]\n",
        "\n",
        "# Display results in Streamlit\n",
        "st.title(\"Cosine Similarity Analysis\")\n",
        "\n",
        "# Show original table\n",
        "st.subheader(\"Data Table\")\n",
        "st.table(df)\n",
        "\n",
        "# Show computed cosine similarities\n",
        "st.subheader(\"Cosine Similarities\")\n",
        "df[\"Cosine Similarity\"] = cosine_similarities\n",
        "st.dataframe(df[[\"Cosine Similarity\"]])\n",
        "\n",
        "# Display the highest similarity row\n",
        "st.subheader(\"Row with Highest Cosine Similarity\")\n",
        "st.write(f\"Index: {max_index}\")\n",
        "st.write(f\"Row Name: {max_row_name}\")\n",
        "st.write(f\"Cosine Similarity Score: {cosine_similarities[max_index]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmxON8S6xYEu",
        "outputId": "1c1ce8cb-7a40-42aa-a3ea-6fc1256ade4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Euclidean Distance"
      ],
      "metadata": {
        "id": "Zgo7zmr928Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test3.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import euclidean  # New import\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/am.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define row names (assuming they are in order in the dataset)\n",
        "row_names = [\n",
        "    \"self acceptance\", \"positive relations with others\", \"autonomy\",\n",
        "    \"environmental mastery\", \"purpose in life\", \"personal growth\"\n",
        "]\n",
        "\n",
        "# Extract the probability columns (assumed order: anxiety, bipolar, depression, normal, ptsd)\n",
        "issue_columns = [\"anxiety\", \"bipolar\", \"depression\", \"normal\", \"ptsd\"]\n",
        "matrix = df[issue_columns].values  # Convert to NumPy matrix\n",
        "\n",
        "# Define example probabilities (vector to compare against)\n",
        "probabilities = np.array([0.2, 0.3, 0.1, 0.3, 0.1])\n",
        "\n",
        "# Compute Euclidean distances for each row\n",
        "euclidean_distances = np.array([euclidean(row, probabilities) for row in matrix])\n",
        "\n",
        "# Find the index of the row with the smallest Euclidean distance\n",
        "min_index = np.argmin(euclidean_distances)\n",
        "min_row_name = row_names[min_index]\n",
        "\n",
        "# Display results in Streamlit\n",
        "st.title(\"Euclidean Distance Analysis\")\n",
        "\n",
        "# Show original table\n",
        "st.subheader(\"Data Table\")\n",
        "st.table(df)\n",
        "\n",
        "# Show computed Euclidean distances\n",
        "st.subheader(\"Euclidean Distances\")\n",
        "df[\"Euclidean Distance\"] = euclidean_distances\n",
        "st.dataframe(df[[\"Euclidean Distance\"]])\n",
        "\n",
        "# Display the closest row (smallest Euclidean distance)\n",
        "st.subheader(\"Row with Smallest Euclidean Distance\")\n",
        "st.write(f\"Index: {min_index}\")\n",
        "st.write(f\"Row Name: {min_row_name}\")\n",
        "st.write(f\"Euclidean Distance: {euclidean_distances[min_index]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcEp84k-2-m7",
        "outputId": "0254c5ca-58dd-48c4-8b8e-b9e0b2d9b2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBY2le6UuUuj",
        "outputId": "030f95ef-469e-4093-84c0-2e44adae57a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.42.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.25.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.42.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.42.0 watchdog-6.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combination of Three"
      ],
      "metadata": {
        "id": "Okjk48ub4Lbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity # new added\n",
        "from scipy.spatial.distance import euclidean  # New import\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/am.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define row names (assuming they are in order in the dataset)\n",
        "row_names = [\n",
        "    \"self acceptance\", \"positive relations with others\", \"autonomy\",\n",
        "    \"environmental mastery\", \"purpose in life\", \"personal growth\"\n",
        "]\n",
        "\n",
        "# Extract the probability columns (assumed order: anxiety, bipolar, depression, normal, ptsd)\n",
        "issue_columns = [\"anxiety\", \"bipolar\", \"depression\", \"normal\", \"ptsd\"]\n",
        "matrix = df[issue_columns].values  # Convert to NumPy matrix\n",
        "\n",
        "# Define example probabilities (ensure they sum to 1 or adjust accordingly)\n",
        "probabilities = np.array([0.2, 0.3, 0.1, 0.3, 0.1])\n",
        "\n",
        "# Show original table\n",
        "st.subheader(\"Association Matrix\")\n",
        "st.table(df)\n",
        "\n",
        "def weighted_sum_analysis(matrix,probabilities):\n",
        "    # Compute weighted sums for each row\n",
        "    weighted_sums = np.dot(matrix, probabilities)\n",
        "\n",
        "    # Find the index of the row with the highest weighted sum\n",
        "    max_index = np.argmax(weighted_sums)\n",
        "    max_row_name = row_names[max_index]\n",
        "\n",
        "    with st.expander(\"Weighted Sum Analysis\"):\n",
        "\n",
        "      # Display results in Streamlit\n",
        "      st.title(\"Weighted Sum Analysis\")\n",
        "\n",
        "      # Show computed weighted sums\n",
        "      st.subheader(\"Weighted Sums\")\n",
        "      df[\"Weighted Sum\"] = weighted_sums\n",
        "      st.dataframe(df[[\"Weighted Sum\"]])\n",
        "\n",
        "      # Display the highest weighted sum row\n",
        "      st.subheader(\"Row with Highest Weighted Sum\")\n",
        "      st.success(f\"\"\"\n",
        "      **Index:** {max_index}  \\n\n",
        "      **Row Name:** {max_row_name}  \\n\n",
        "      **Weighted Sum:** {weighted_sums[max_index]}\n",
        "      \"\"\")\n",
        "\n",
        "    return max_index, max_row_name\n",
        "\n",
        "def cosine_similarity_analysis(matrix, probabilities):\n",
        "    # Define example probabilities (vector to compare against)\n",
        "    probabilities = np.array(probabilities).reshape(1, -1)\n",
        "\n",
        "    # Compute cosine similarity for each row\n",
        "    cosine_similarities = cosine_similarity(matrix, probabilities).flatten()\n",
        "\n",
        "    # Find the index of the row with the highest cosine similarity\n",
        "    max_index = np.argmax(cosine_similarities)\n",
        "    max_row_name = row_names[max_index]\n",
        "\n",
        "    with st.expander(\"Cosine Similarity Analysis\"):\n",
        "      # Display results in Streamlit\n",
        "      st.title(\"Cosine Similarity Analysis\")\n",
        "\n",
        "      # Show computed cosine similarities\n",
        "      st.subheader(\"Cosine Similarities\")\n",
        "      df[\"Cosine Similarity\"] = cosine_similarities\n",
        "      st.dataframe(df[[\"Cosine Similarity\"]])\n",
        "\n",
        "      # Display the highest similarity row\n",
        "      st.subheader(\"Row with Highest Cosine Similarity\")\n",
        "      st.success(f\"\"\"\n",
        "      **Index:** {max_index}  \\n\n",
        "      **Row Name:** {max_row_name}  \\n\n",
        "      **Cosine Similarity Score:** {cosine_similarities[max_index]}\n",
        "      \"\"\")\n",
        "\n",
        "    return max_index, max_row_name\n",
        "\n",
        "def euclidian_distance_analysis(matrix, probabilities):\n",
        "    # Compute Euclidean distances for each row\n",
        "    euclidean_distances = np.array([euclidean(row, probabilities) for row in matrix])\n",
        "\n",
        "    # Find the index of the row with the smallest Euclidean distance\n",
        "    min_index = np.argmin(euclidean_distances)\n",
        "    min_row_name = row_names[min_index]\n",
        "\n",
        "    with st.expander(\"Euclidean Distance Analysis\"):\n",
        "\n",
        "      # Display results in Streamlit\n",
        "      st.title(\"Euclidean Distance Analysis\")\n",
        "\n",
        "      # Show computed Euclidean distances\n",
        "      st.subheader(\"Euclidean Distances\")\n",
        "      df[\"Euclidean Distance\"] = euclidean_distances\n",
        "      st.dataframe(df[[\"Euclidean Distance\"]])\n",
        "\n",
        "      # Display the closest row (smallest Euclidean distance)\n",
        "      st.subheader(\"Row with Smallest Euclidean Distance\")\n",
        "      st.success(f\"\"\"\n",
        "      **Index:** {min_index}  \\n\n",
        "      **Row Name:** {min_row_name}  \\n\n",
        "      **Euclidean Distance:** {euclidean_distances[min_index]}\n",
        "      \"\"\")\n",
        "\n",
        "    return min_index, min_row_name\n",
        "\n",
        "weighted_sum_index, weighted_sum_row_name = weighted_sum_analysis(matrix, probabilities)\n",
        "cosine_similarity_index, cosine_similarity_row_name = cosine_similarity_analysis(matrix, probabilities)\n",
        "euclidian_distance_index, euclidian_distance_row_name = euclidian_distance_analysis(matrix, probabilities)\n",
        "\n",
        "def get_consensus_string(weighted_sum_row_name, cosine_similarity_row_name, euclidean_distance_row_name):\n",
        "    row_names = [weighted_sum_row_name, cosine_similarity_row_name, euclidean_distance_row_name]\n",
        "    unique_names = list(set(row_names))  # Get unique row names\n",
        "\n",
        "    if len(unique_names) == 1:\n",
        "        return unique_names[0]  # All three are the same\n",
        "    elif len(unique_names) == 2:\n",
        "        return \" and \".join(unique_names)  # Two names are the same\n",
        "    else:\n",
        "        return \", \".join(unique_names)  # All three are different\n",
        "\n",
        "# Example usage:\n",
        "consensus_string = get_consensus_string(weighted_sum_row_name, cosine_similarity_row_name, euclidian_distance_row_name)\n",
        "st.info(consensus_string)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgP_RtDY4Oid",
        "outputId": "927c53b5-1d4a-487b-b915-1b9f0a3f0eaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your authtoken\n",
        "ngrok.set_auth_token(\"2ohUKqk37HcGbvwN0s8Y1E2WNxE_39z1gVF3bYq9vFSEm7Wzq\") # Replace YOUR_AUTHTOKEN with your actual authtoken\n",
        "\n",
        "# Kill any existing ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit with nohup\n",
        "!nohup streamlit run test.py &\n",
        "\n",
        "# Create a public URL with ngrok to access the app\n",
        "public_url = ngrok.connect(addr='8501')\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPXqkHU2uadu",
        "outputId": "464af289-88ce-4261-d593-3f2f8e3adc15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "Public URL: NgrokTunnel: \"https://4bd4-34-73-61-0.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "lA1jglxYxbpv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}