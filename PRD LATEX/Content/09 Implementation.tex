% --------------------------- Implementation ---------------------------------


\section{Implementation}

\begin{figure}[h!]  
    \centering
    \includegraphics[width=1.0\textwidth]{Images/ML Model Workflow.png}  
    \caption*{Workflow for getting the model for the web application}
    \label{model workflow}  % Label for referencing the figure
\end{figure}


\subsection{Data Collection and Dataset Preparation}
\begin{table}[H]
    \centering
    \caption*{Stepwise Algorithm for Data Collection and Dataset Combination}
    \label{tab:algorithm}
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Step} & \textbf{Description} \\
        \hline
        1 & \textbf{Import Libraries:} Import \texttt{praw, pandas, time} for data collection and \texttt{sklearn.utils.shuffle} for combining datasets. \\
        \hline
        2 & \textbf{Initialize Reddit API:} Use the provided credentials (\texttt{client\_id}, \texttt{client\_secret}, \texttt{user\_agent}) to create a Reddit instance. \\
        \hline
        3 & \textbf{Define Subreddits and Labels:} Create a dictionary mapping labels to subreddit lists:
            \begin{itemize}[noitemsep, topsep=0pt]
                \item \textbf{normal}: \texttt{news, AskReddit}
                \item \textbf{depression}: \texttt{depression}
                \item \textbf{ptsd}: \texttt{PTSD}
                \item \textbf{anxiety}: \texttt{Anxiety}
                \item \textbf{bipolar}: \texttt{BipolarReddit}
            \end{itemize} \\
        \hline
        4 & \textbf{Set Post Types and Limit:} Define post types (\texttt{hot, new, top}) and set \texttt{posts\_per\_type} to 100. \\
        \hline
        5 & \textbf{Collect Posts:}
            \begin{itemize}[noitemsep, topsep=0pt]
                \item For each label and its associated subreddits, iterate over each post type.
                \item Retrieve posts from the subreddit (using the corresponding post type and a limit of 100).
                \item For each post, combine the title and selftext, and append the result along with its label to a data list.
                \item Pause for 1 second between requests.
            \end{itemize} \\
        \hline
    \end{tabularx}
\end{table}

\begin{table}[H]
    \centering
    \caption*{Stepwise Algorithm for Data Collection and Dataset Combination}
    \label{tab:algorithm}
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Step} & \textbf{Description} \\
        \hline
        6 & \textbf{Save Collected Data:} Convert the data list into a DataFrame with columns \texttt{text} and \texttt{label} and save it as \texttt{\{label\}\_dataset.csv}. \\
        \hline
        7 & \textbf{Load Datasets:} Read the individual CSV files for \texttt{bipolar, depression, normal, anxiety,} and \texttt{ptsd} into separate DataFrames. \\
        \hline
        8 & \textbf{Determine Minimum Length:} Compute \texttt{min\_length} as the minimum number of records among datasets (using \texttt{len(normal\_df)//6} for the normal dataset to balance its count). \\
        \hline
        9 & \textbf{Create a Balanced Pattern:}
            \begin{itemize}[noitemsep, topsep=0pt]
                \item Loop from 0 to \texttt{min\_length - 1}.
                \item For each iteration, append to a new list:
                    \begin{itemize}[noitemsep, topsep=0pt]
                        \item The \texttt{i\textsuperscript{th}} record from \texttt{bipolar\_df}.
                        \item The \texttt{i\textsuperscript{th}} record from \texttt{depression\_df}.
                        \item Six consecutive records from \texttt{normal\_df} (indices \texttt{i*6} to \texttt{(i+1)*6}).
                        \item The \texttt{i\textsuperscript{th}} record from \texttt{anxiety\_df}.
                        \item The \texttt{i\textsuperscript{th}} record from \texttt{ptsd\_df}.
                    \end{itemize}
            \end{itemize} \\
        \hline
        10 & \textbf{Convert to DataFrame:} Transform the balanced list into a DataFrame (\texttt{pattern\_df}). \\
        \hline
        11 & \textbf{Prepare Remaining Data:} Concatenate the leftover records from each dataset (beyond \texttt{min\_length} for all datasets and beyond \texttt{min\_length*6} for the normal dataset) and shuffle them. \\
        \hline
        12 & \textbf{Merge and Save Final Dataset:} Combine \texttt{pattern\_df} with the shuffled remaining data, reset the index, and save the final DataFrame as \texttt{mental\_health\_combined.csv}. \\
        \hline
    \end{tabularx}
\end{table}

\begin{figure}[h!]  
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Dataset.png}  
    \caption*{Obtained Dataset}
    \label{LSTMROC711}  % Label for referencing the figure
\end{figure}


% ----- adding subreddit info ------

\begin{figure}[h!]  
    \centering
    \includegraphics[width=1.0\textwidth]{Images/Data Collection Graph.png}  
    \caption*{Collected Data Statistics}
    \label{LSTMROC7uyiut11}  % Label for referencing the figure
\end{figure}

\noindent
The dataset for mental health classification was compiled from subreddit communities, initially containing 385,80 records across five categories: anxiety (54 subreddits), PTSD (38), depression (80), bipolar disorder (60), and normal mental states (53). After data cleaning to ensure quality and relevance, the dataset was reduced to 167,279 records. A subset of 18,596 cleaned records was used for analysis to address computational constraints, such as memory and processing power, which would make training on the full dataset infeasible on standard hardware. This approach balances efficiency and performance, enabling faster experimentation and iterative model improvement while maintaining a representative sample.


% ----------------- Data Preprocessing



\subsection{Data Cleaning and Feature Extraction}

\begin{table}[H]
    \caption*{Stepwise Algorithm for Data Cleaning and Feature Extraction}
    \label{tab:algorithm}
    \centering
    \begin{tabularx}{\textwidth}{|c|X|}
    \hline
    \textbf{Step} & \textbf{Description} \\
    \hline
    1 & \textbf{Import Libraries \& Resources}: Import \texttt{pandas}, \texttt{re}, \texttt{TfidfVectorizer} from scikit-learn, and NLTK modules. Download the necessary NLTK resources (e.g., \texttt{stopwords}, \texttt{punkt}, and \texttt{punkt\_tab}). \\
    \hline
    2 & \textbf{Load Dataset}: Read the CSV file \texttt{mental\_health.csv} into a pandas DataFrame. \\
    \hline
    3 & \textbf{Handle Missing Values}: Drop any rows where the \texttt{text} field is missing. \\
    \hline
    4 & \textbf{Remove Duplicates}: Eliminate duplicate rows based on the \texttt{text} column. \\
    \hline
    5 & \textbf{Define Negative Words}: Create a set of negative words (e.g., \texttt{not}, \texttt{no}, \texttt{nor}, etc.) that will be retained during stopword removal. \\
    \hline
    6 & \textbf{Define Cleaning Function}: Write the function \texttt{clean\_text(text)} to preprocess text by:
        
      \begin{enumerate}[label=(\alph*), itemsep=0pt, topsep=0pt, partopsep=0pt, parsep=0pt]
        \item Removing URLs using regex.
        \item Removing mentions (e.g., \texttt{@username}).
        \item Removing special characters, numbers, and punctuation.
        \item Converting text to lowercase.
        \item Tokenizing the text using NLTK's \texttt{word\_tokenize}.
        \item Removing stopwords (while keeping negative words).
        \item Rejoining tokens into a cleaned string.
      \end{enumerate} \\
    \hline
\end{tabularx}
\end{table}

\begin{table}[H]
    \caption*{Stepwise Algorithm for Data Cleaning and Feature Extraction}
    \label{tab:algorithm}
    \centering
    \begin{tabularx}{\textwidth}{|c|X|}
    \hline
    \textbf{Step} & \textbf{Description} \\
    \hline
    7 & \textbf{Apply Cleaning Function}: Execute \texttt{clean\_text} on the \texttt{text} column and store the result in a new column called \texttt{cleaned\_text}. \\
    \hline
    8 & \textbf{Initialize TF-IDF Vectorizer}: Create a TF-IDF vectorizer instance with a maximum of 10,000 features. (Other methods include Bag-Of-Words, LIWC, N-Gram, Word2Vec)\\
    \hline
    9 & \textbf{Fit \& Transform Data}: Apply the vectorizer to the \texttt{cleaned\_text} to generate the TF-IDF feature matrix \texttt{X}. \\
    \hline
    10 & \textbf{Optional - Convert to DataFrame}: Convert the TF-IDF sparse matrix \texttt{X} into a pandas DataFrame for easier inspection (e.g., print the first few rows). \\
    \hline
    11 & \textbf{Optional - Save Preprocessed Data}: Save the final preprocessed DataFrame to \texttt{preprocessed\_mental\_health.csv}. \\
    \hline
    \end{tabularx}
\end{table}

\begin{figure}[h!]  
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Data Cleaning and Preprocessing.png}  
    \caption*{After TF-IDF Vectorization}
    \label{Data Collection and Preprocessing}  % Label for referencing the figure
\end{figure}

\noindent \textbf{Datasets:} \\
\textbf{Before Cleaning:} \texttt{mental\_health.csv} \\
\textbf{After Cleaning:} \texttt{preprocessed\_mental\_helth.csv}

\vspace{1em} % Adds a small vertical space

\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|c|X|X|}
    \hline
    \textbf{Stage} & \textbf{Schema} & \textbf{Description} \\
    \hline
    \makecell{Before\\Cleaning} &
    \begin{tabular}[t]{@{}l@{}}
    \texttt{text}: Original text data \\
    \texttt{mental\_issue}: Mental health issues
    \end{tabular}
    &
    Contains raw, unprocessed text data with possible missing values, duplicates, URLs, mentions, and special characters. \\[6pt]
    \hline
    \makecell{After\\Cleaning} &
    \begin{tabular}[t]{@{}l@{}}
    \texttt{text}: Original text data \\
    \texttt{mental\_issue}: Mental health issues \\
    \texttt{cleaned\_text}: Processed text data
    \end{tabular}
    &
    Data is cleaned by removing URLs, mentions, special characters, and extra noise. The text is converted to lowercase, tokenized, stopwords (except key negative words) are removed, and the cleaned text is stored in the new column \texttt{cleaned\_text}. \\[6pt]
    \hline
    \end{tabularx}
    \caption*{Dataset Schema Before and After Data Cleaning}
    \label{tab:dataset_schema}
\end{table}

\noindent
The matrix dimensions for Bag of Words are determined by the number of records, which is 18,597 in this case, and the size of the vocabulary, which represents the number of unique words in the \texttt{cleaned\_text} column. Similarly, for TF-IDF, the dimensions of the matrix are the same as BOW, calculated as the number of records multiplied by the vocabulary size. For N-gram, the matrix size depends on the range of n-grams used. For example, a unigram produces dimensions equivalent to BOW, while bigram or trigram models increase the vocabulary size due to the inclusion of word combinations. Word2Vec, on the other hand, creates a dense vector representation for each word, with dimensions based on the predefined vector size, such as 100 or 200. Aggregating these vectors at the sentence level, typically by averaging, results in a matrix of dimensions equal to the number of records multiplied by the vector dimensions. For LIWC, the dimensions are determined by the number of predefined LIWC categories, which is typically around 70. The resulting matrix dimensions are the number of records multiplied by the number of LIWC categories.

% -------- logistic regression

\subsection{Machine Learning Models}

Algorithm like Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Random Forest, XGBoost and Naive Bayes are implemented for the classification of mental health issues. The code algorithm below demonstrates the implementation. The models are evaluated on the test set using metrics like accuracy and classification reports. Hyperparameter Tuning is performed using RandomizedSearchCV to optimize the model performance. Naive Bayes model after hyperparamter tuning is selected along with the basic Logistic Regression, Support Vector Machine, XGboost for the final ensemble model for the web application. 

\begin{table}[H]
    \caption*{\textbf{Stepwise Algorithm for Machine Learning Models}}
    \label{tab:stepwise_algorithm}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \small
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Step} & \textbf{Description} \\
        \hline
        1 & \textbf{Data Loading and Verification:} Load the dataset (\texttt{preprocessed\_mental\_health.csv}) using pandas, verify required columns (\texttt{cleaned\_text} and \texttt{mental\_health\_issue}), and drop rows with missing values. \\
        \hline
        2 & \textbf{Tokenization and Feature Extraction:} Choose a method (Bag-of-Words, TF-IDF, LIWC, Word2Vec, or N-gram) and transform \texttt{cleaned\_text} into a numerical feature matrix $X$. \\
        \hline
        3 & \textbf{Target Variable Preparation:} Extract the target variable $y$ from the \texttt{mental\_health\_issue} column. \\
        \hline
        4 & \textbf{Dataset Splitting:} Split $X$ and $y$ into training and test sets (e.g., 80/20 split) using \texttt{train\_test\_split} with a fixed random state. \\
        \hline
        5 & \textbf{Model Initialization:} For each algorithm (Logistic Regression, Naive Bayes, SVM, KNN, Random Forest, XGBoost), initialize the model with appropriate hyperparameters. \\
        \hline
        6 & \textbf{Model Training:} Train the selected model on the training data. \\
        \hline
        7 & \textbf{Prediction:} Use the trained model to predict mental health issues on the test set. \\
        \hline
        8 & \textbf{Model Evaluation:} Evaluate performance by computing accuracy, classification reports, and confusion matrices. \\
        \hline
        9 & \textbf{Cross-Validation:} Apply Stratified K-Fold (e.g., 5 folds) cross-validation to record accuracies and calculate the mean and standard deviation. \\
        \hline
        10 & \textbf{Performance Comparison:} Compare evaluation metrics across all models and feature extraction methods to select the best combination. \\
        \hline
        11 & \textbf{Hyperparameter Tuning:} For Logistic Regression, Naive Bayes, SVM, and KNN, optimize hyperparameters using \texttt{RandomizedSearchCV} to search the parameter space and select the optimal configuration. \\
        \hline
    \end{tabularx}
\end{table}



\subsection{Deep Learning Models}

\begin{table}[H]
    \caption*{\textbf{Stepwise Algorithm for LSTM-based Model}}
    \label{tab:lstm_algorithm}
    \centering
    \renewcommand{\arraystretch}{1.3}
    \small
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Step} & \textbf{Description} \\
        \hline
        1 & \textbf{Data Loading:} Load \texttt{preprocessed\_mental\_health.csv} using pandas. \\
        \hline
        2 & \textbf{Feature \& Target Preparation:}  
              \begin{itemize}[noitemsep, topsep=0pt]
                  \item Extract text ($X$) and target ($y$).
                  \item Encode $y$ using \texttt{LabelEncoder} and convert to one-hot with \texttt{to\_categorical}.
              \end{itemize} \\
        \hline
        3 & \textbf{Tokenization \& Padding:}  
              \begin{itemize}[noitemsep, topsep=0pt]
                  \item Initialize Keras \texttt{Tokenizer} with \texttt{num\_words=25000}.
                  \item Fit on $X$, convert texts to sequences, and pad to \texttt{max\_length=128} using \texttt{pad\_sequences}.
              \end{itemize} \\
        \hline
        4 & \textbf{Cross-Validation Setup:} Define a 5-fold StratifiedKFold (shuffle=True, random\_state=42). \\
        \hline
    \end{tabularx}
\end{table}

\begin{table}[H]
    \caption*{\textbf{Stepwise Algorithm for LSTM-based Model}}
    \label{tab:lstm_algorithm}
    \centering
    \renewcommand{\arraystretch}{1.3}
    \small
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Step} & \textbf{Description} \\
        \hline
        5 & \textbf{LSTM Model Building:} For each fold, build a Keras Sequential model with:
              \begin{itemize}[noitemsep, topsep=0pt]
                  \item \texttt{Embedding(vocab\_size, 128, input\_length=128)}
                  \item \texttt{LSTM(128, return\_sequences=True)}
                  \item \texttt{Dropout(0.2)}
                  \item \texttt{LSTM(64)}
                  \item \texttt{Dropout(0.2)}
                  \item \texttt{Dense(64, activation='relu')}
                  \item \texttt{Dense(num\_classes, activation='softmax')}
              \end{itemize} \\
        \hline
        6 & \textbf{Model Training:} Compile the model with \texttt{optimizer='adamw'} and \texttt{loss='categorical\_crossentropy'}. Train for 20 epochs with a batch size of 16 using the training fold and validate on the validation fold. \\
        \hline
        7 & \textbf{Evaluation per Fold:} 
              \begin{itemize}[noitemsep, topsep=0pt]
                  \item Evaluate the model on the validation set to obtain loss and accuracy.
                  \item Predict probabilities on the validation set and compute the confusion matrix.
              \end{itemize} \\
        \hline
        8 & \textbf{ROC Curve Calculation:}  
              \begin{itemize}[noitemsep, topsep=0pt]
                  \item Concatenate true labels and predictions from all folds.
                  \item Binarize true labels and compute ROC curves and AUC for each class plus a micro-average.
              \end{itemize} \\
        \hline
        9 & \textbf{Visualization:} Plot ROC curves, validation loss/accuracy across folds, and the average confusion matrix using Matplotlib and Seaborn. \\
        \hline
        10 & \textbf{Final Evaluation:} Compute the average cross-validated accuracy and display a classification report. \\
        \hline
    \end{tabularx}
\end{table}


\begin{figure}[h!]  
    \centering
    \includegraphics[width=1.0\textwidth]{Images/LSTM Epoch.png}  
    \caption*{Output for LSTM Epochs}
    \label{LSTm Epochs}  % Label for referencing the figure
\end{figure}

\pagebreak

\begin{figure}[h!]  
    \centering
    \includegraphics[width=1.0\textwidth]{Images/LSTM LA.png}  
    \caption*{LSTM Validation loss and accuracy}
    \label{Accuracy Loss}  % Label for referencing the figure
\end{figure}

\begin{figure}[h!]  
    \centering
    \includegraphics[width=1.0\textwidth]{Images/LSTM EMBED.png}  
    \caption*{LSTM Random and Learned Embeddings}
    \label{lstm embed}  % Label for referencing the figure
\end{figure}

\pagebreak

\begin{figure}[h!]  
    \centering
    \includegraphics[width=0.45\textwidth]{Images/LSTM MODEL.png}  
    \caption*{LSTM Model Architecture}
    \label{lstm arch}  % Label for referencing the figure
\end{figure}


\begin{table}[H]
    \caption*{\textbf{Stepwise Algorithm for Custom Transformer-based Model}}
    \label{tab:transformer_algorithm}
    \centering
    \renewcommand{\arraystretch}{1.3}
    \small
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Step} & \textbf{Description} \\
        \hline
        1 & \textbf{Import Libraries:} Import required packages including NumPy, pandas, Matplotlib, Seaborn, scikit-learn modules (e.g., \texttt{LabelEncoder}, \texttt{confusion\_matrix}), and TensorFlow Keras modules (e.g., \texttt{TextVectorization}, \texttt{Embedding}, \texttt{LSTM}, \texttt{MultiHeadAttention}, etc.). \\
        \hline
        2 & \textbf{Load Dataset:} Use Google Colab's file upload to import \texttt{preprocessed\_mental\_health.csv}. Drop rows with missing \texttt{cleaned\_text} and extract features (\texttt{texts}) and labels (\texttt{labels}). \\
        \hline
        3 & \textbf{Preprocess Labels:} Encode labels with \texttt{LabelEncoder}, convert them to one-hot format via \texttt{to\_categorical}, and retrieve class names. \\
        \hline
        4 & \textbf{Text Vectorization:} Set \texttt{vocab\_size = 25000} and \texttt{sequence\_length = 300}. Create a \texttt{TextVectorization} layer, adapt it on the input texts (using a TensorFlow \texttt{Dataset}), and vectorize the texts. \\
        \hline
    \end{tabularx}
\end{table}

\begin{table}[H]
    \caption*{\textbf{Stepwise Algorithm for Custom Transformer-based Model}}
    \label{tab:transformer_algorithm}
    \centering
    \renewcommand{\arraystretch}{1.3}
    \small
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Step} & \textbf{Description} \\
        \hline
        5 & \textbf{Define Custom Transformer Model:} 
              \begin{itemize}[noitemsep, topsep=0pt]
                  \item \textbf{EmbeddingLayer:} Custom layer that sums word embeddings with positional embeddings.
                  \item \textbf{EncoderLayer:} Custom layer using \texttt{MultiHeadAttention}, followed by a two-layer Dense network (with ReLU activation) and \texttt{LayerNormalization}.
                  \item \textbf{Model Architecture:} Build the model with an Input layer $\to$ \texttt{EmbeddingLayer} $\to$ \texttt{EncoderLayer} $\to$ \texttt{GlobalAveragePooling1D} $\to$ Dense (ReLU) $\to$ Output Dense (softmax). 
                  \item Compile with optimizer \texttt{adamw} and loss \texttt{categorical\_crossentropy}.
              \end{itemize} \\
        \hline
        6 & \textbf{Train the Model:} Convert vectorized texts to a NumPy array and split into training and validation sets (80/20 split, \texttt{random\_state=42}). Train the model for 5 epochs with a batch size of 32. \\
        \hline
        7 & \textbf{Evaluate the Model:} 
              \begin{itemize}[noitemsep, topsep=0pt]
                  \item Evaluate validation loss and accuracy.
                  \item Predict on the validation set and convert predictions to class labels.
                  \item Compute the confusion matrix using scikit-learn's \texttt{confusion\_matrix}.
              \end{itemize} \\
        \hline
        8 & \textbf{Visualization:} Plot the confusion matrix with \texttt{ConfusionMatrixDisplay} (using Matplotlib) and optionally visualize random and learned embeddings. \\
        \hline
    \end{tabularx}
\end{table}

\begin{figure}[h!]  
    \centering
    \includegraphics[width=0.9\textwidth]{Images/T EMBED.png}  
    \caption*{Transformer Model Random and Learned Embeddings}
    \label{lstm t embed}  % Label for referencing the figure
\end{figure}

\begin{figure}[h!]  
    \centering
    \includegraphics[width=0.9\textwidth]{Images/T LOSS EPOCH.png}  
    \caption*{Transformer Epoch, Loss, Accuracy}
    \label{lstm t epch}  % Label for referencing the figure
\end{figure}


\pagebreak

% --------- ENSEMBLE LEARNING 1----------
\subsection{Ensemble Model}

\begin{table}[H]
    \caption*{\textbf{General Stepwise Algorithm for Ensemble Models}}
    \label{tab:ensemble_algorithm}
    \centering
    \renewcommand{\arraystretch}{1.3}
    \small
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Step} & \textbf{Description} \\
        \hline
        1 & \textbf{Import Libraries:} Import required packages including \texttt{numpy}, \texttt{pandas}, \texttt{tensorflow} (Keras modules such as \texttt{MultiHeadAttention}, \texttt{Embedding}, etc.), \texttt{scikit-learn} (e.g., \texttt{RandomForestClassifier}, \texttt{train\_test\_split}, \texttt{confusion\_matrix}), and \texttt{pickle}. \\
        \hline
        2 & \textbf{Load Pre-trained Base Models \& Vectorizers:} Load saved models and vectorizers using \texttt{pickle} and \texttt{load\_model}. Base models include: Logistic Regression (LR), Support Vector Machine (SVM), Naive Bayes (NB), XGBoost, LSTM, and Transformer. Also load corresponding vectorizers/tokenizers (e.g., \texttt{LRvectorizer}, \texttt{SVMvectorizer}, \texttt{NBvectorizer}, \texttt{tfidf\_vectorizer}, \texttt{LSTM\_tokenizer}, \texttt{Tvectorize\_layer}) and label encoders. \\
        \hline
        3 & \textbf{Load \& Preprocess Test Data:} Read \texttt{preprocessed\_mental\_health.csv} with pandas, drop rows with missing \texttt{cleaned\_text}, extract texts and labels, and encode labels using \texttt{LabelEncoder}. \\
        \hline
        4 & \textbf{Text Preprocessing for Each Model:} Transform the raw texts using respective vectorizers/tokenizers:
              \begin{itemize}[noitemsep, topsep=0pt]
                  \item LR, SVM, NB: Use \texttt{LRvectorizer}, \texttt{SVMvectorizer}, \texttt{NBvectorizer}.
                  \item XGBoost: Use \texttt{tfidf\_vectorizer}.
                  \item LSTM: Convert texts using \texttt{LSTM\_tokenizer} and apply \texttt{pad\_sequences}.
                  \item Transformer: Process texts using \texttt{Tvectorize\_layer}.
              \end{itemize} \\
        \hline
        5 & \textbf{Generate Base Model Predictions:} For each base model, compute prediction probabilities:
              \begin{itemize}[noitemsep, topsep=0pt]
                  \item LR: \texttt{lr\_model.predict\_proba}
                  \item SVM: \texttt{svm\_model.predict\_proba}
                  \item NB: \texttt{nb\_model.predict\_proba}
                  \item XGBoost: \texttt{xgb\_model.predict\_proba}
                  \item LSTM: \texttt{lstm\_model.predict}
                  \item Transformer: \texttt{transformer\_model.predict}
              \end{itemize} \\
        \hline
    \end{tabularx}
\end{table}

\begin{table}[H]
    \caption*{\textbf{General Stepwise Algorithm for Ensemble Models}}
    \label{tab:ensemble_algorithm}
    \centering
    \renewcommand{\arraystretch}{1.3}
    \small
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Step} & \textbf{Description} \\
        \hline
        6 & \textbf{Stack Predictions:} Horizontally stack all base model prediction probabilities to form a new feature matrix (\texttt{stacked\_features}) for the meta-learner. \\
        \hline
        7 & \textbf{Ensemble Configuration \& Data Splitting:} Split the stacked features and true labels (using \texttt{train\_test\_split}) into training and testing sets. Specify ensemble configurations:
              \begin{itemize}[noitemsep, topsep=0pt]
                  \item \textbf{Ensemble Model 1:} Base models: LR, XGBoost; Meta-learner: XGBoost.
                  \item \textbf{Ensemble Model 2:} Base models: LR, NB, SVM, XGBoost, LSTM; Meta-learner: XGBoost.
                  \item \textbf{Ensemble Model 3:} Base models: LR, NB, SVM, XGBoost, LSTM; Meta-learner: Random Forest.
                  \item \textbf{Ensemble Model 4:} Same base models using Bagging.
                  \item \textbf{Ensemble Model 5:} Same base models using Blending.
                  \item \textbf{Ensemble Model 6:} Same base models using Weighted Voting.
                  \item \textbf{Ensemble Model 7:} Base models: LR, SVM, NB, LSTM, XGBoost, Transformer; Meta-learner: Random Forest.
              \end{itemize} \\
        \hline
        8 & \textbf{Train Meta-Learner:} Train the meta-learner (e.g., Random Forest, XGBoost, or ensemble strategies like Bagging, Blending, Voting) on the training portion of the stacked feature matrix. \\
        \hline
        9 & \textbf{Evaluate Ensemble Model:} Predict on the test set using the meta-learner and compute evaluation metrics: accuracy, classification report, and confusion matrix. Also, perform cross-validation (e.g., using \texttt{cross\_val\_score}) to assess stability. \\
        \hline
        10 & \textbf{Output Results:} Print final evaluation metrics (accuracy, report, confusion matrix) and cross-validation results. \\
        \hline
    \end{tabularx}
\end{table}


% ------------------------- Implementation Ends -----------------------------
