{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3YBXu/zojFCz5df0ar+8g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sT-abl-VQZjT","outputId":"5c9b4f73-6821-49a8-c0a9-652a56f867e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n"]}],"source":["!apt-get install -y ffmpeg libsm6 libxext6\n","!apt-get install -y tesseract-ocr\n","!apt-get install -y portaudio19-dev\n","\n","!pip install streamlit\n","!pip install pyngrok\n","!pip install pydub\n","!pip install sounddevice\n","!pip install wavio\n","!pip install numpy\n","!pip install openai-whisper\n","!pip install PyAudio\n","!pip install SpeechRecognition\n","\n","!pip install deep-translator\n","!pip install joblib\n","!pip install pandas\n","!pip install Pillow\n","!pip install praw\n","!pip install protobuf\n","!pip install pytesseract\n","!pip install Requests\n","!pip install scikit-learn\n","!pip install google-generativeai\n","\n","!pip install tweepy\n","\n","!pip install librosa\n","\n","!pip install tensorflow\n","!pip install numpy\n","!pip install opencv-python"]},{"cell_type":"code","source":["%%writefile v5.py\n","\n","import streamlit as st\n","import joblib\n","import pandas as pd\n","import praw\n","from PIL import Image\n","from deep_translator import GoogleTranslator\n","import requests\n","from io import BytesIO\n","from collections import Counter\n","import google.generativeai as genai\n","\n","import cv2\n","import numpy as np\n","import whisper\n","import tempfile\n","import os\n","from pydub import AudioSegment\n","import subprocess\n","\n","import tweepy\n","\n","import re\n","import librosa\n","import librosa.display\n","import tensorflow as tf\n","\n","import pytesseract\n","\n","# Configure Tesseract and FFMPEG\n","pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n","os.environ[\"FFMPEG_BINARY\"] = \"/usr/bin/ffmpeg\"\n","\n","# Load Whisper model for audio transcription\n","whisper_model = whisper.load_model(\"base\")\n","\n","# Load the saved logistic regression model and vectorizer\n","model = joblib.load('LRmodel.pkl')\n","vectorizer = joblib.load('LRvectorizer.pkl')\n","\n","# Initialize Reddit API\n","reddit = praw.Reddit(client_id='DAOso5_7CHzXzdtd-070fg',\n","                     client_secret='JtdGFRDM10avSQFYthzYUQNfLeI8rQ',\n","                     user_agent='Mental Health')\n","\n","# Initialize Twitter API\n","BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAN4YxAEAAAAAFEA7p2dAWs4B0AcdhyXtxCY%2FQEE%3DIjVupZjMFoic5nx5goEara0YoYyaf1EecTX0PlxTPnsqb3zmF9\"\n","client = tweepy.Client(bearer_token=BEARER_TOKEN)\n","\n","\n","# Configure Gemini API for wellbeing insights\n","genai.configure(api_key=\"AIzaSyD-pu0AuG2dbzzspRfgS8DjO10Ffh08JiU\")\n","generation_config = {\n","    \"temperature\": 1,\n","    \"top_p\": 0.95,\n","    \"top_k\": 40,\n","    \"max_output_tokens\": 8192,\n","    \"response_mime_type\": \"text/plain\",\n","}\n","gemini_model = genai.GenerativeModel(\n","    model_name=\"gemini-1.5-flash\",\n","    generation_config=generation_config,\n",")\n","\n","\n","# Twitter\n","def fetch_image_content(image_url):\n","    \"\"\"Fetch and process an image from a URL.\"\"\"\n","    try:\n","        response = requests.get(image_url, timeout=10)\n","        response.raise_for_status()  # Ensure the request was successful\n","        return Image.open(BytesIO(response.content))\n","    except Exception as e:\n","        st.write(f\"Error fetching image: {e}\")\n","        return None\n","\n","def get_latest_tweets_with_images(username, max_items=10):\n","    \"\"\"Fetch latest tweets with text and associated images.\"\"\"\n","    # Fetch user details to get user ID\n","    user = client.get_user(username=username)\n","    if not user.data:\n","        return [], []\n","\n","    user_id = user.data.id\n","\n","    # Fetch the latest tweets (exclude retweets and replies)\n","    response = client.get_users_tweets(\n","        id=user_id,\n","        tweet_fields=[\"attachments\"],\n","        expansions=[\"attachments.media_keys\"],\n","        media_fields=[\"url\"],\n","        exclude=[\"retweets\", \"replies\"],\n","        max_results=max_items\n","    )\n","\n","    tweet_data = []\n","\n","    if response.data:\n","        for tweet in response.data:\n","            # Extract text\n","            text = tweet.text\n","\n","            # Extract images if available\n","            images = []\n","            if hasattr(tweet, \"attachments\") and tweet.attachments is not None:\n","                if \"media_keys\" in tweet.attachments:\n","                    for media_key in tweet.attachments[\"media_keys\"]:\n","                        media = next(\n","                            (media for media in response.includes.get(\"media\", []) if media[\"media_key\"] == media_key), None\n","                        )\n","                        if media and media.type == \"photo\":\n","                            images.append(media.url)\n","\n","            # Append tweet data\n","            tweet_data.append({\"text\": text, \"images\": images})\n","\n","    return tweet_data\n","\n","\n","\n","# Function to fetch text-based posts from Reddit\n","def fetch_user_text_posts(username):\n","    try:\n","        user = reddit.redditor(username)\n","        posts = [post.title + \" \" + post.selftext for post in user.submissions.new(limit=20)]\n","        return posts\n","    except Exception as e:\n","        st.write(f\"Error fetching text posts: {e}\")\n","        return []\n","\n","# Function to fetch image-based posts from Reddit and perform OCR\n","def fetch_user_images_and_extract_text(username):\n","    try:\n","        user = reddit.redditor(username)\n","        images = [post.url for post in user.submissions.new(limit=20) if post.url.endswith(('.jpg', '.jpeg', '.png', '.webp', '.bmp', '.tiff'))]\n","\n","        extracted_texts = []\n","        for image_url in images:\n","            try:\n","                response = requests.get(image_url)\n","                image = Image.open(BytesIO(response.content))\n","                st.image(image, caption=\"Fetched Image\", use_column_width=True)\n","\n","                extracted_text = extract_text_from_image(image)\n","                if extracted_text.strip():\n","                    translated_text = GoogleTranslator(source='auto', target='en').translate(extracted_text)\n","                    extracted_texts.append(translated_text)\n","                    st.write(\"Extracted and Translated Text from Image:\")\n","                    st.text(translated_text)\n","            except Exception as e:\n","                st.write(f\"Error processing image {image_url}: {e}\")\n","\n","        return extracted_texts\n","    except Exception as e:\n","        st.write(f\"Error fetching images: {e}\")\n","        return []\n","\n","# Function to classify text and display result\n","def classify_text(text):\n","    input_vectorized = vectorizer.transform([text])\n","    prediction_proba = model.predict_proba(input_vectorized)\n","\n","    issue_labels = model.classes_\n","    proba_df = pd.DataFrame(prediction_proba, columns=issue_labels).T\n","    proba_df.columns = ['Probability']\n","\n","    top_issue = proba_df['Probability'].idxmax()\n","    top_probability = proba_df['Probability'].max()\n","\n","    st.write(f\"The most likely mental health concern is: {top_issue} with a probability of {top_probability:.2%}\")\n","\n","    get_wellbeing_insight(text, top_issue)\n","\n","# Function to get wellbeing insights from Gemini model\n","def get_wellbeing_insight(text, top_issue):\n","    try:\n","        chat_session = gemini_model.start_chat(history=[])\n","        prompt = f\"\"\" The Ryff Scale is based on six factors: autonomy, environmental mastery, personal growth, positive relations with others, purpose in life, and self-acceptance. Higher total scores indicate higher psychological well-being. Following are explanations of each criterion, and an example statement from the Ryff Inventory to measure each criterion: Autonomy: High scores indicate that the respondent is independent and regulates his or her behavior independent of social pressures. An example statement for this criterion is \"I have confidence in my opinions, even if they are contrary to the general consensus.\" Environmental Mastery: High scores indicate that the respondent makes effective use of opportunities and has a sense of mastery in managing environmental factors and activities, including managing everyday affairs and creating situations to benefit personal needs. An example statement for this criterion is \"In general, I feel I am in charge of the situation in which I live.\"Personal Growth: High scores indicate that the respondent continues to develop, is welcoming to new experiences, and recognizes improvement in behavior and self over time. An example statement for this criterion is \"I think it is important to have new experiences that challenge how you think about yourself and the world.\"Positive Relations with Others: High scores reflect the respondent's engagement in meaningful relationships with others that include reciprocal empathy, intimacy, and affection. An example statement for this criterion is \"People would describe me as a giving person, willing to share my time with others.\" Purpose in Life: High scores reflect the respondent's strong goal orientation and conviction that life holds meaning. An example statement for this criterion is \"Some people wander aimlessly through life, but I am not one of them.\"Self-Acceptance: High scores reflect the respondent's positive attitude about his or her self. An example statement for this criterion is \"I like most aspects of my personality.\" Now, please use the above information, along with the mental health issue: {top_issue}, to generate a short paragraph for each of the following subtopics, discussing how the issue may relate to these factors of mental well-being: 1. **Autonomy**: How might {top_issue} impact a person's ability to be independent and self-regulate behavior? 2. **Environmental Mastery**: Discuss how {top_issue} may affect a person's ability to manage their environment and activities. 3. **Personal Growth**: What impact might {top_issue} have on an individual's development, openness to new experiences, and recognition of self-improvement? 4. **Positive Relations with Others**: How does {top_issue} influence the ability to maintain meaningful and empathetic relationships? 5. **Purpose in Life**: How might {top_issue} shape an individual's sense of purpose or goal orientation in life? 6. **Self-Acceptance**: What role does {top_issue} play in a person's self-image and acceptance of themselves? Based on these subtopics, provide practical advice to improve or reduce the impact of {top_issue}.\"\"\"\n","\n","        response = chat_session.send_message(prompt)\n","\n","        st.write(\"### Wellbeing Insight:\")\n","        st.write(response.text)\n","    except Exception as e:\n","        st.write(f\"Error retrieving wellbeing insights: {e}\")\n","\n","# Function to extract text from image using Tesseract\n","def extract_text_from_image(image):\n","    extracted_text = pytesseract.image_to_string(image)\n","    return extracted_text.splitlines()\n","\n","# Function to extract text from an image using Tesseract\n","def extract_text_from_image_video(image):\n","    extracted_text = pytesseract.image_to_string(image)\n","    return extracted_text if extracted_text else \"\"  # Return empty string if no text is found\n","\n","\n","# Function to extract audio from a video file and classify it\n","# Function to extract 20 frames from a video file\n","def extract_frames(video_path, num_frames=20):\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    frames = []\n","    frame_interval = total_frames // num_frames  # Calculate frame interval\n","\n","    for i in range(num_frames):\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n","\n","        ret, frame = cap.read()\n","\n","        if ret:\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frames.append(frame)\n","\n","    cap.release()\n","    return frames\n","\n","\n","\n","def transcribe_audio_from_video(video_file):\n","    try:\n","        # Save the uploaded video file to a temporary file\n","        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_video_file:\n","            temp_video_file.write(video_file.read())\n","            temp_video_path = temp_video_file.name\n","\n","        audio_path = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False).name\n","\n","        # Extract audio from video using subprocess\n","        subprocess.run([\"ffmpeg\", \"-i\", temp_video_path, \"-q:a\", \"0\", \"-map\", \"a\", audio_path, \"-y\"])\n","        audio = AudioSegment.from_file(audio_path)\n","\n","        # Use Whisper to transcribe the audio\n","        result = whisper_model.transcribe(audio_path)\n","\n","        # Get the transcribed text and translate if necessary\n","        transcribed_text = result[\"text\"]\n","        translated_text = GoogleTranslator(source=\"auto\", target=\"en\").translate(transcribed_text)\n","\n","        # Clean up temporary files\n","        os.remove(temp_video_path)\n","        os.remove(audio_path)\n","\n","        return translated_text\n","\n","    except Exception as e:\n","        # Display a user-friendly message if the video is too long or another error occurs\n","        if \"duration\" in str(e).lower() or \"length\" in str(e).lower():\n","            return \"The video is too long to process. Please upload a shorter video.\"\n","        else:\n","            return f\"An error occurred: {e}\"\n","\n","# Function to translate text using DeepL\n","def translate_text(text, target_lang=\"en\"):\n","    try:\n","        if text:\n","            translated_text = GoogleTranslator(source=\"auto\", target=target_lang).translate(text)\n","            return translated_text\n","        return \"\"  # Return empty string if text is empty or None\n","    except Exception as e:\n","        return f\"Error translating text: {str(e)}\"\n","\n","# Function to extract audio from a video file\n","def extract_audio_from_video(video_path):\n","    try:\n","        # Generate a temporary audio file path\n","        audio_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\").name\n","\n","        # Use FFmpeg to extract audio from video\n","        subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\", audio_path, \"-y\"])\n","\n","        # Return the path of the extracted audio\n","        return audio_path\n","\n","    except Exception as e:\n","        return f\"Error extracting audio: {str(e)}\"\n","\n","# Function to analyze audio mood based on extracted audio\n","def analyze_audio_mood(video_path):\n","    try:\n","        # Extract audio from the video (assuming extract_audio_from_video is implemented)\n","        audio_path = extract_audio_from_video(video_path)\n","\n","        # Load the audio file using librosa\n","        y, sr = librosa.load(audio_path)\n","\n","        # Extract MFCCs (Mel-frequency cepstral coefficients) from the audio signal\n","        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n","\n","        # Divide the MFCC array into 4 frequency bands and calculate scalar mean for each band\n","\n","        # Low Frequencies: MFCC 0, 1, 2\n","        low_freq_mfcc = np.mean(mfcc[0:3], axis=1)\n","        mean_low = np.mean(low_freq_mfcc)  # Scalar mean for low frequencies\n","\n","        # Mid-Low Frequencies: MFCC 3, 4\n","        mid_low_freq_mfcc = np.mean(mfcc[3:5], axis=1)\n","        mean_mid_low = np.mean(mid_low_freq_mfcc)  # Scalar mean for mid-low frequencies\n","\n","        # Mid-High Frequencies: MFCC 5, 6, 7\n","        mid_high_freq_mfcc = np.mean(mfcc[5:8], axis=1)\n","        mean_mid_high = np.mean(mid_high_freq_mfcc)  # Scalar mean for mid-high frequencies\n","\n","        # High Frequencies: MFCC 8, 9, 10, 11, 12\n","        high_freq_mfcc = np.mean(mfcc[8:13], axis=1)\n","        mean_high = np.mean(high_freq_mfcc)  # Scalar mean for high frequencies\n","\n","        # Now use these scalar means for classification\n","\n","        if mean_high <= mean_low and mean_high <= mean_mid_low and mean_high <= mean_mid_high:\n","            return \"Audio sounds normal, with no dominant emotion detected\"\n","\n","        elif mean_mid_high <= mean_low and mean_mid_high <= mean_mid_low and mean_mid_high <= mean_high:\n","            return \"Audio sounds neutral, calm, or peaceful\"\n","\n","        elif mean_mid_low <= mean_low and mean_mid_low <= mean_mid_high and mean_mid_low <= mean_high:\n","            return \"Audio sounds slightly melancholic or neutral\"\n","\n","        elif mean_low <= mean_mid_low and mean_low <= mean_mid_high and mean_low <= mean_high:\n","            return \"Audio sounds calm or melancholic, with less intensity\"\n","\n","        elif mean_high > mean_low and mean_high > mean_mid_low and mean_high <= mean_mid_high:\n","            return \"Audio sounds depressive or anxious in nature\"\n","\n","        else :\n","            return \"Audio sounds upbeat and energetic (Happy)\"\n","\n","    except Exception as e:\n","        return f\"Error analyzing audio mood: {str(e)}\"\n","\n","\n","# Define the Streamlit app\n","def run_app():\n","    st.title(\"Mental Health Disorder Detection\")\n","\n","    option = st.sidebar.selectbox(\n","        \"Choose an option\",\n","        [\"Text Input\", \"Image Upload\", \"Video Upload\", \"Reddit Username Analysis\", \"Twitter Username Analysis\"]\n","    )\n","\n","    # Text Input\n","    if option == \"Text Input\":\n","        st.subheader(\"Enter Text to Classify Mental Health Issue\")\n","        input_text = st.text_area(\"Enter your text here:\")\n","\n","        if st.button(\"Classify Text\"):\n","            if input_text.strip() == \"\":\n","                st.write(\"Please enter some text to classify.\")\n","            else:\n","                translated_text = GoogleTranslator(source='auto', target='en').translate(input_text)\n","                st.write(\"Translated Text (to English):\")\n","                st.write(translated_text)\n","                classify_text(translated_text)\n","\n","    # Image Upload\n","    elif option == \"Image Upload\":\n","        st.subheader(\"Upload an Image to Extract and Classify Text\")\n","        uploaded_image = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"jpeg\", \"png\", \"webp\", \"bmp\", \"tiff\"])\n","\n","        if uploaded_image is not None:\n","            image = Image.open(uploaded_image)\n","            st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n","\n","            extracted_text = extract_text_from_image(image)\n","            translated_text = GoogleTranslator(source='auto', target='en').translate(\"\\n\".join(extracted_text))\n","\n","            st.subheader(\"Translated Text (to English)\")\n","            st.text(translated_text)\n","\n","            if st.button(\"Classify Extracted Text\"):\n","                if not translated_text or translated_text.strip() == \"\":\n","                    # If translated_text is empty or contains only whitespace\n","                    st.write(\"It is normal with probability 100%\")\n","                else:\n","                    classify_text(translated_text)\n","\n","    # Video Upload\n","    elif option == \"Video Upload\":\n","        st.subheader(\"Upload a Video to Extract and Classify Text\")\n","        # File upload widget\n","        video_file = st.file_uploader(\"Choose a video file\", type=[\"mp4\", \"mov\", \"avi\"])\n","\n","        if video_file:\n","            # Save the uploaded video file temporarily\n","            video_path = \"/tmp/uploaded_video.mp4\"\n","            with open(video_path, \"wb\") as f:\n","                f.write(video_file.getbuffer())\n","\n","            st.video(video_file)  # Display the uploaded video\n","\n","            # Extract frames from the uploaded video\n","            frames = extract_frames(video_path)\n","            combined_text = \"\"\n","\n","            st.write(\"Extracting frames from video ...\")\n","            for idx, frame in enumerate(frames):\n","                st.image(frame, caption=f\"Frame {idx + 1}\", use_column_width=True)\n","                text_from_frame = extract_text_from_image_video(frame)\n","\n","                if text_from_frame and text_from_frame not in combined_text:\n","                    combined_text += text_from_frame + \" \"\n","\n","            st.write(\"Text Extracted from Video Frames:\")\n","            st.text(combined_text)\n","\n","            # Translate the extracted text from frames\n","            translated_frame_text = translate_text(combined_text)\n","            # st.write(\"Translated Text from Video Frames:\")\n","            # st.text(translated_frame_text)\n","\n","            # Extract audio and transcribe it\n","            # st.write(\"Transcribing Audio from Video...\")\n","            transcribed_audio_text = transcribe_audio_from_video(video_file)\n","\n","            st.write(\"Transcribed Audio Text:\")\n","            st.text(transcribed_audio_text)\n","\n","            translated_audio_text = translate_text(transcribed_audio_text)\n","            # st.write(\"Translated Audio Text:\")\n","            # st.text(translated_audio_text)\n","\n","            # Combine the text extracted from both images and audio\n","            full_combined_text = combined_text + \" \" + transcribed_audio_text\n","            st.write(\"Combined Extracted Text (from both video frames and audio):\")\n","            st.text(full_combined_text)\n","\n","            translated_combined_text = translate_text(full_combined_text)\n","            st.write(\"Translated Combined Text (Frames + Audio):\")\n","            st.text(translated_combined_text)\n","\n","            # Analyze audio mood\n","            st.write(\"Analyzing Audio Mood...\")\n","            mood_result = analyze_audio_mood(video_path)\n","            st.write(mood_result)\n","\n","            cleaned_text = re.sub(r\"[^a-zA-Z0-9.,!? ]\", \"\", translated_combined_text)\n","\n","            if st.button(\"Classify Extracted Text\"):\n","                if not cleaned_text or cleaned_text.strip() == \"\":\n","                    # If audio_text is empty or contains only whitespace\n","                    st.write(\"It is normal with probability 100%\")\n","                else:\n","                    classify_text(cleaned_text)\n","\n","\n","    # Reddit Username Analysis\n","    elif option == \"Reddit Username Analysis\":\n","        st.subheader(\"Enter Reddit Username for Analysis\")\n","        username = st.text_input(\"Enter Reddit username:\")\n","\n","        if st.button(\"Analyze\"):\n","            if username.strip() == \"\":\n","                st.write(\"Please enter a Reddit username.\")\n","            else:\n","                # Fetch and display text posts\n","                text_posts = fetch_user_text_posts(username)\n","                if text_posts:\n","                    st.write(\"Recent Text Posts:\")\n","                    st.write(text_posts[:3])  # Display a few posts for review\n","\n","                # Fetch and display image-based posts with extracted text\n","                image_texts = fetch_user_images_and_extract_text(username)\n","\n","                # Combine text from both text posts and image text\n","                all_text = text_posts + image_texts\n","                if all_text:\n","                    predictions = []\n","                    for text in all_text:\n","                        # Vectorize and classify each post\n","                        input_vectorized = vectorizer.transform([text])\n","                        prediction = model.predict(input_vectorized)\n","                        predictions.append(prediction[0])\n","\n","                    # Count the most common mental health issue\n","                    issue_counts = Counter(predictions)\n","                    top_issue, top_count = issue_counts.most_common(1)[0]\n","                    top_percentage = (top_count / len(predictions)) * 100\n","\n","                    st.write(f\"The most frequently detected mental health concern is: {top_issue} appearing in {top_percentage:.2f}% of analyzed text.\")\n","                    issue_distribution = pd.DataFrame(issue_counts.items(), columns=['Mental Health Issue', 'Count'])\n","                    st.write(\"Mental health issue distribution across posts:\")\n","                    st.write(issue_distribution)\n","\n","                    # Call the Gemini model to get well-being insights\n","                    get_wellbeing_insight(\" \".join(all_text), top_issue)\n","                else:\n","                    st.write(\"No valid text found for analysis.\")\n","\n","    # Twitter Username Analysis\n","    elif option == \"Twitter Username Analysis\":\n","        st.subheader(\"Enter Twitter Username for Analysis\")\n","        username = st.text_input(\"Enter Twitter username:\")\n","\n","        if st.button(\"Analyze\"):\n","            if username.strip() == \"\":\n","                st.write(\"Please enter a Twitter username.\")\n","            else:\n","                # Fetch the latest tweets with associated images\n","                tweets_with_images = get_latest_tweets_with_images(username)\n","\n","                # Extract text content from tweets\n","                text_posts = [tweet['text'] for tweet in tweets_with_images if tweet['text']]\n","                st.write(\"Recent Text Posts from Tweets:\")\n","                st.write(text_posts[:3])  # Display a few posts for review\n","\n","                # Extract and process text from associated images\n","                image_texts = []\n","                for tweet in tweets_with_images:\n","                    for image_url in tweet['images']:\n","                        image = fetch_image_content(image_url)\n","                        if image:\n","                            st.image(image, caption=f\"Image from Tweet\", use_column_width=True)\n","                        if image:\n","                            extracted_text = extract_text_from_image(image)  # Assuming a text extraction function is defined\n","                            if extracted_text:\n","                                image_texts.append(extracted_text)\n","\n","                # Combine text from both tweet text and extracted image text\n","                all_text = text_posts + image_texts\n","\n","                # Ensure all entries in all_text are strings\n","                all_text = [str(text) for text in all_text if text]\n","\n","                if all_text:\n","                    predictions = []\n","                    for text in all_text:\n","                        try:\n","                            # Vectorize and classify each text\n","                            input_vectorized = vectorizer.transform([text])\n","                            prediction = model.predict(input_vectorized)\n","                            predictions.append(prediction[0])\n","                        except Exception as e:\n","                            st.write(f\"Error processing text: {text[:50]}... - {e}\")\n","                            continue\n","\n","                    # Count the most common mental health issue\n","                    issue_counts = Counter(predictions)\n","                    top_issue, top_count = issue_counts.most_common(1)[0]\n","                    top_percentage = (top_count / len(predictions)) * 100\n","\n","                    st.write(f\"The most frequently detected mental health concern is: {top_issue}, appearing in {top_percentage:.2f}% of analyzed text.\")\n","                    issue_distribution = pd.DataFrame(issue_counts.items(), columns=['Mental Health Issue', 'Count'])\n","                    st.write(\"Mental health issue distribution across posts:\")\n","                    st.write(issue_distribution)\n","\n","                    # Call the Gemini model to get well-being insights\n","                    get_wellbeing_insight(\" \".join(all_text), top_issue)\n","                else:\n","                    st.write(\"No valid text found for analysis.\")\n","\n","# Run the app\n","if __name__ == '__main__':\n","    run_app()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkSoqDwpQ4_2","executionInfo":{"status":"ok","timestamp":1731837526235,"user_tz":-330,"elapsed":447,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"e917ef08-1e28-46c9-8de3-7b48a85b7692"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing v5.py\n"]}]},{"cell_type":"code","source":["# Import ngrok\n","from pyngrok import ngrok\n","\n","# Set your authtoken\n","ngrok.set_auth_token(\"2ohUKqk37HcGbvwN0s8Y1E2WNxE_39z1gVF3bYq9vFSEm7Wzq\") # Replace YOUR_AUTHTOKEN with your actual authtoken\n","\n","# Kill any existing ngrok processes\n","ngrok.kill()\n","\n","# Start Streamlit with nohup\n","!nohup streamlit run v5.py &\n","\n","# Create a public URL with ngrok to access the app\n","public_url = ngrok.connect(addr='8501')\n","print(f\"Public URL: {public_url}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BEBJyTsRVa_x","executionInfo":{"status":"ok","timestamp":1731837538745,"user_tz":-330,"elapsed":2245,"user":{"displayName":"Arkapratim Ghosh","userId":"06118460502332503890"}},"outputId":"27cf5407-1b64-4b32-8e44-33a0e3d45f98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nohup: appending output to 'nohup.out'\n","Public URL: NgrokTunnel: \"https://1008-34-145-63-249.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"code","source":["ngrok.kill()"],"metadata":{"id":"WWqZzojDWR1R"},"execution_count":null,"outputs":[]}]}