{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALL"
      ],
      "metadata": {
        "id": "cKEopkcQxMG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw-X_SPXVCL_",
        "outputId": "a8de6619-b71c-4e61-d4f3-31fc34863b73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connecting to r2u.stat.illinois.edu (1\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                                                    \rGet:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,533 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,830 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,956 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,688 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,235 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,649 kB]\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,321 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,664 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,708 kB]\n",
            "Fetched 29.0 MB in 7s (4,370 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KliTOSVgw5-v",
        "outputId": "462ff8b5-2ce1-4100-d586-828019f39db4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsm6 is already the newest version (2:1.2.3-1build2).\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 0s (18.7 MB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 1s (327 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 124994 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.6 [186 kB]\n",
            "Fetched 186 kB in 1s (368 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 125038 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.6_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.42.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.28.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.42.2-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.42.2 watchdog-6.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting sounddevice\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
            "Downloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice\n",
            "Successfully installed sounddevice-0.5.1\n",
            "Collecting wavio\n",
            "  Downloading wavio-0.0.9-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from wavio) (1.26.4)\n",
            "Downloading wavio-0.0.9-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: wavio\n",
            "Successfully installed wavio-0.0.9\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting PyAudio\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: PyAudio\n",
            "  Building wheel for PyAudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyAudio: filename=pyaudio-0.2.14-cp311-cp311-linux_x86_64.whl size=67391 sha256=527d40ec358096e96a8d5df704a543242e236b160dc8e833a4bdcb601befc5d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/b1/c1/67e4ef443de2665d86031d4760508094eab5de37d5d64d9c27\n",
            "Successfully built PyAudio\n",
            "Installing collected packages: PyAudio\n",
            "Successfully installed PyAudio-0.2.14\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.14.1-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Downloading SpeechRecognition-3.14.1-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.1\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (4.25.6)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Requirement already satisfied: Requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from Requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from Requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from Requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from Requests) (2025.1.31)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.25.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.68.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803373 sha256=24fb5b1bd810f2bb8a01a4ff4d05c9c950cbffd520c7ad589305a53b4300c124\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20240930\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.61.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Collecting deepface\n",
            "  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.2.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.67.1)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (11.1.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.11.0.86)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.1.0)\n",
            "Collecting flask-cors>=4.0.1 (from deepface)\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (2.5.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (3.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n",
            "  Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2025.1.31)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
            "Downloading deepface-0.0.93-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=7be17a4f66baa5d8a16c2d7e074ce053babd8d9306049dcd95b962eade6349dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: lz4, gunicorn, fire, mtcnn, flask-cors, retina-face, deepface\n",
            "Successfully installed deepface-0.0.93 fire-0.7.0 flask-cors-5.0.1 gunicorn-23.0.0 lz4-4.4.3 mtcnn-1.0.0 retina-face-0.0.17\n",
            "Requirement already satisfied: tf_keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tf_keras) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf_keras) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.2.19-py3-none-any.whl.metadata (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.9/171.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.2.19-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.2.19\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y ffmpeg libsm6 libxext6\n",
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install -y portaudio19-dev\n",
        "!apt-get install -y poppler-utils\n",
        "\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install pydub\n",
        "!pip install sounddevice\n",
        "!pip install wavio\n",
        "!pip install numpy\n",
        "!pip install PyAudio\n",
        "!pip install SpeechRecognition\n",
        "!pip install pdf2image\n",
        "!pip install deep-translator\n",
        "!pip install joblib\n",
        "!pip install pandas\n",
        "!pip install Pillow\n",
        "!pip install praw\n",
        "!pip install protobuf\n",
        "!pip install pytesseract\n",
        "!pip install Requests\n",
        "!pip install scikit-learn\n",
        "!pip install google-generativeai\n",
        "\n",
        "!pip install tweepy\n",
        "!pip install openai-whisper --no-deps\n",
        "!pip install tiktoken\n",
        "\n",
        "!pip install librosa\n",
        "\n",
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install xgboost\n",
        "!pip install deepface\n",
        "!pip install tf_keras\n",
        "\n",
        "!pip install transformers\n",
        "# !pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install nltk\n",
        "\n",
        "!pip install plotly\n",
        "!pip install matplotlib\n",
        "!pip install scipy\n",
        "!pip install networkx\n",
        "!pip install yt-dlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APP"
      ],
      "metadata": {
        "id": "Zt0CH_IbxRlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile v13.py\n",
        "\n",
        "import pickle\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import praw\n",
        "from PIL import Image\n",
        "from deep_translator import GoogleTranslator\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from collections import Counter\n",
        "import google.generativeai as genai\n",
        "import pickle\n",
        "import cv2\n",
        "import numpy as np\n",
        "import whisper\n",
        "import tempfile\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "import tweepy\n",
        "import re\n",
        "import librosa\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Added\n",
        "from scipy.stats import gaussian_kde\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import networkx as nx\n",
        "import yt_dlp\n",
        "import io\n",
        "from pdf2image import convert_from_bytes\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics.pairwise import cosine_similarity # new added\n",
        "from scipy.spatial.distance import euclidean  # New import\n",
        "# import soundfile as sf\n",
        "\n",
        "from tensorflow.keras.models import load_model, Model, Sequential\n",
        "from tensorflow.keras.utils import pad_sequences, custom_object_scope, to_categorical\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Input, Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Layer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from xgboost import XGBClassifier\n",
        "from deepface import DeepFace\n",
        "import torch\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "import pytesseract\n",
        "# Configure Tesseract and FFMPEG\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "os.environ[\"FFMPEG_BINARY\"] = \"/usr/bin/ffmpeg\"\n",
        "\n",
        "# Define file paths in Colab\n",
        "csv_file_path = \"/content/response.csv\"\n",
        "image_path_survey = \"/content/intro.png\"\n",
        "am_file_path = \"/content/am.csv\"\n",
        "\n",
        "# Load Whisper model for audio transcription\n",
        "# whisper_model = whisper.load_model(\"base\")\n",
        "@st.cache_resource\n",
        "def load_whisper_model():\n",
        "    return whisper.load_model(\"base\")\n",
        "\n",
        "whisper_model = load_whisper_model()\n",
        "\n",
        "# ------------- ENSEMBLE LEARNING REQUIREMENTS -----------------\n",
        "# Define functions to load each model and resource with caching\n",
        "@st.cache_resource\n",
        "def load_lr_model():\n",
        "    return joblib.load('LRmodel.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_lr_vectorizer():\n",
        "    return joblib.load('LRvectorizer.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_nb_model():\n",
        "    return joblib.load('NBmodel.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_nb_vectorizer():\n",
        "    return joblib.load('NBvectorizer.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_svm_model():\n",
        "    return joblib.load('SVMmodel.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_svm_vectorizer():\n",
        "    return joblib.load('SVMvectorizer.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_label_encoder():\n",
        "    return joblib.load('label_encoder.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_xgb_model():\n",
        "    return joblib.load('xgb_model.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_tfidf_vectorizer():\n",
        "    return joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_lstm_label_encoder():\n",
        "    return joblib.load('LSTM_label_encoder.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_lstm_tokenizer():\n",
        "    return joblib.load('LSTM_tokenizer.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_lstm_model():\n",
        "    return load_model('lstm_model.h5')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_meta_learner_rf():\n",
        "    return joblib.load('meta_learner_rf.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_transformer_label_encoder():\n",
        "    return joblib.load('Tlabel_encoder.pkl')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_transformer_vectorizer():\n",
        "    return joblib.load('Tvectorizer_layer.pkl')\n",
        "\n",
        "# Load all models and resources by calling the above functions\n",
        "lr_model = load_lr_model()\n",
        "lr_vectorizer = load_lr_vectorizer()\n",
        "nb_model = load_nb_model()\n",
        "nb_vectorizer = load_nb_vectorizer()\n",
        "svm_model = load_svm_model()\n",
        "svm_vectorizer = load_svm_vectorizer()\n",
        "label_encoder = load_label_encoder()\n",
        "xgb_model = load_xgb_model()\n",
        "tfidf_vectorizer = load_tfidf_vectorizer()\n",
        "lstm_label_encoder = load_lstm_label_encoder()  # Optional: Only if separate from main label encoder\n",
        "lstm_tokenizer = load_lstm_tokenizer()\n",
        "lstm_model = load_lstm_model()\n",
        "meta_learner_rf = load_meta_learner_rf()\n",
        "\n",
        "t_label_encoder = load_transformer_label_encoder()\n",
        "t_vectorizer = load_transformer_vectorizer()\n",
        "t_vectorize_layer = load_transformer_vectorizer()\n",
        "\n",
        "# Define the custom layers\n",
        "class EmbeddingLayer(Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(EmbeddingLayer, self).__init__(**kwargs)\n",
        "        self.word_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embedding = Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, tokens):\n",
        "        sequence_length = tf.shape(tokens)[-1]\n",
        "        positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
        "        positions_encoding = self.position_embedding(positions)\n",
        "        words_encoding = self.word_embedding(tokens)\n",
        "        return positions_encoding + words_encoding\n",
        "\n",
        "class EncoderLayer(Layer):\n",
        "    def __init__(self, total_heads, total_dense_units, embed_dim, **kwargs):\n",
        "        super(EncoderLayer, self).__init__(**kwargs)\n",
        "        self.multihead = MultiHeadAttention(num_heads=total_heads, key_dim=embed_dim)\n",
        "        self.nnw = Sequential([Dense(total_dense_units, activation=\"relu\"), Dense(embed_dim)])\n",
        "        self.normalize_layer = LayerNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.multihead(inputs, inputs)\n",
        "        normalize_attn = self.normalize_layer(inputs + attn_output)\n",
        "        nnw_output = self.nnw(normalize_attn)\n",
        "        final_output = self.normalize_layer(normalize_attn + nnw_output)\n",
        "        return final_output\n",
        "\n",
        "# Load the saved transformer model with custom objects\n",
        "custom_objects = {\n",
        "    \"EmbeddingLayer\": EmbeddingLayer,\n",
        "    \"EncoderLayer\": EncoderLayer\n",
        "}\n",
        "\n",
        "@st.cache_resource\n",
        "def load_transformer_model():\n",
        "    return load_model('Ttransformer_model.h5', custom_objects=custom_objects)\n",
        "\n",
        "transformer_model = load_transformer_model()\n",
        "# ------------- ENSEMBLE LEARNING REQUIREMENTS -----------------\n",
        "\n",
        "# Initialize Reddit API\n",
        "reddit = praw.Reddit(client_id='DAOso5_7CHzXzdtd-070fg',\n",
        "                     client_secret='JtdGFRDM10avSQFYthzYUQNfLeI8rQ',\n",
        "                     user_agent='Mental Health')\n",
        "\n",
        "# Initialize Twitter API\n",
        "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAP5ivAEAAAAAhk%2BBxS3W7EJbjNjUxgKEQ73xcUI%3DjnSuDwdvy0kqOoVpziGzD9LNVaMPCamGS2cf2OngdckiLZSZ1h\"\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "# Configure Gemini API for wellbeing insights\n",
        "genai.configure(api_key=\"AIzaSyDLMw8vdW36QV36LRLxZRMqgCqNt2czSug\")\n",
        "generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 40,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "gemini_model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-2.0-flash\",\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Twitter\n",
        "def fetch_image_content(image_url):\n",
        "    \"\"\"Fetch and process an image from a URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(image_url, timeout=10)\n",
        "        response.raise_for_status()  # Ensure the request was successful\n",
        "        return Image.open(BytesIO(response.content))\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error fetching image: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- twitter video\n",
        "def download_video(video_url, save_path):\n",
        "    \"\"\"Download a video from the given URL and save it locally.\"\"\"\n",
        "    ydl_opts = {\n",
        "        \"quiet\": True,\n",
        "        \"format\": \"best[ext=mp4]\",\n",
        "        \"outtmpl\": save_path,\n",
        "        \"noplaylist\": True,\n",
        "    }\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([video_url])\n",
        "        return save_path\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error downloading video: {e}\")\n",
        "        return None\n",
        "\n",
        "# ----- twitter post with video\n",
        "def get_latest_tweets_with_videos(username, max_items=10):\n",
        "    \"\"\"Fetch latest tweets with text, associated images, and videos.\"\"\"\n",
        "    user = client.get_user(username=username)\n",
        "    if not user.data:\n",
        "        return []\n",
        "\n",
        "    user_id = user.data.id\n",
        "\n",
        "    response = client.get_users_tweets(\n",
        "        id=user_id,\n",
        "        tweet_fields=[\"attachments\"],\n",
        "        expansions=[\"attachments.media_keys\"],\n",
        "        media_fields=[\"url\", \"type\", \"variants\"],\n",
        "        exclude=[\"retweets\", \"replies\"],\n",
        "        max_results=max_items\n",
        "    )\n",
        "\n",
        "    tweet_data = []\n",
        "\n",
        "    if response.data:\n",
        "        for tweet in response.data:\n",
        "            text = tweet.text\n",
        "\n",
        "            images = []\n",
        "            videos = []\n",
        "            if hasattr(tweet, \"attachments\") and tweet.attachments is not None:\n",
        "                if \"media_keys\" in tweet.attachments:\n",
        "                    for media_key in tweet.attachments[\"media_keys\"]:\n",
        "                        media = next(\n",
        "                            (media for media in response.includes.get(\"media\", []) if media[\"media_key\"] == media_key), None\n",
        "                        )\n",
        "                        if media:\n",
        "                            if media.type == \"photo\":\n",
        "                                images.append(media.url)\n",
        "                            elif media.type == \"video\":\n",
        "                                if \"variants\" in media:\n",
        "                                    video_url = max(\n",
        "                                        media.variants,\n",
        "                                        key=lambda v: v.get(\"bitrate\", 0) if v.get(\"content_type\") == \"video/mp4\" else 0\n",
        "                                    ).get(\"url\", \"\")\n",
        "                                    if video_url:\n",
        "                                        videos.append(video_url)\n",
        "\n",
        "            tweet_data.append({\"text\": text, \"images\": images, \"videos\": videos})\n",
        "\n",
        "    return tweet_data\n",
        "\n",
        "# ---- for twitter and reddit video\n",
        "def process_video(video_file):\n",
        "    # If the video is a file path (string)\n",
        "    if isinstance(video_file, str):\n",
        "        video_path = video_file  # It's a path to a file on disk\n",
        "    # If the video is a BytesIO object (Streamlit file uploader)\n",
        "    elif isinstance(video_file, io.BytesIO):\n",
        "        video_path = \"/tmp/uploaded_video.mp4\"\n",
        "        with open(video_path, \"wb\") as f:\n",
        "            f.write(video_file.getbuffer())\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported video format\")\n",
        "\n",
        "    # Extract frames from the uploaded video\n",
        "    frames = extract_frames(video_path)\n",
        "    combined_text = \"\"\n",
        "\n",
        "    # Emotion recognition\n",
        "    emotion_counts, frame_emotions = analyze_emotions_from_frames(frames)\n",
        "\n",
        "    # Display summary table and most frequent emotion\n",
        "    dominant_emotion = display_emotion_summary(emotion_counts)\n",
        "    st.success(f\"Dominant Emotion: **{dominant_emotion}**\")\n",
        "\n",
        "    # Use the dominant emotion and emotion counts to craft a Gemini API prompt\n",
        "    analyze_with_gemini(dominant_emotion, emotion_counts)\n",
        "\n",
        "    st.subheader(\"Analyzing Audio Mood and Tone...\")\n",
        "    analyze_audio_mood(video_path)\n",
        "\n",
        "    # Extract text from frames\n",
        "    for idx, frame in enumerate(frames):\n",
        "        text_from_frame = extract_text_from_image_video(frame)\n",
        "        if text_from_frame and text_from_frame not in combined_text:\n",
        "            combined_text += text_from_frame + \" \"\n",
        "\n",
        "    # Translate the extracted text from frames\n",
        "    translated_frame_text = translate_text(combined_text)\n",
        "\n",
        "    # Extract audio and transcribe it\n",
        "    transcribed_audio_text = transcribe_audio_from_video(video_file)\n",
        "\n",
        "    # Combine the text extracted from both images and audio\n",
        "    full_combined_text = combined_text + \" \" + transcribed_audio_text\n",
        "    translated_combined_text = translate_text(full_combined_text)\n",
        "\n",
        "    # Clean the translated text\n",
        "    cleaned_text = re.sub(r\"[^a-zA-Z0-9.,!? ]\", \"\", translated_combined_text)\n",
        "\n",
        "    # Optionally: You can add additional analysis or return more data if needed.\n",
        "    return cleaned_text\n",
        "\n",
        "def get_latest_tweets_with_images(username, max_items=10):\n",
        "    \"\"\"Fetch latest tweets with text and associated images.\"\"\"\n",
        "    # Fetch user details to get user ID\n",
        "    user = client.get_user(username=username)\n",
        "    if not user.data:\n",
        "        return [], []\n",
        "\n",
        "    user_id = user.data.id\n",
        "\n",
        "    # Fetch the latest tweets (exclude retweets and replies)\n",
        "    response = client.get_users_tweets(\n",
        "        id=user_id,\n",
        "        tweet_fields=[\"attachments\"],\n",
        "        expansions=[\"attachments.media_keys\"],\n",
        "        media_fields=[\"url\"],\n",
        "        exclude=[\"retweets\", \"replies\"],\n",
        "        max_results=max_items\n",
        "    )\n",
        "\n",
        "    tweet_data = []\n",
        "\n",
        "    if response.data:\n",
        "        for tweet in response.data:\n",
        "            # Extract text\n",
        "            text = tweet.text\n",
        "\n",
        "            # Extract images if available\n",
        "            images = []\n",
        "            if hasattr(tweet, \"attachments\") and tweet.attachments is not None:\n",
        "                if \"media_keys\" in tweet.attachments:\n",
        "                    for media_key in tweet.attachments[\"media_keys\"]:\n",
        "                        media = next(\n",
        "                            (media for media in response.includes.get(\"media\", []) if media[\"media_key\"] == media_key), None\n",
        "                        )\n",
        "                        if media and media.type == \"photo\":\n",
        "                            images.append(media.url)\n",
        "\n",
        "            # Append tweet data\n",
        "            tweet_data.append({\"text\": text, \"images\": images})\n",
        "\n",
        "    return tweet_data\n",
        "\n",
        "# Function to fetch text-based posts from Reddit\n",
        "def fetch_user_text_posts(username):\n",
        "    try:\n",
        "        user = reddit.redditor(username)\n",
        "        posts = [post.title + \" \" + post.selftext for post in user.submissions.new(limit=20)]\n",
        "        return posts\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error fetching text posts: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to fetch image-based posts from Reddit and perform OCR\n",
        "def fetch_user_images_and_extract_text(username):\n",
        "    try:\n",
        "        user = reddit.redditor(username)\n",
        "        images = [post.url for post in user.submissions.new(limit=20) if post.url.endswith(('.jpg', '.jpeg', '.png', '.webp', '.bmp', '.tiff'))]\n",
        "\n",
        "        extracted_texts = []\n",
        "        all_emotions = {'happy': 0, 'sad': 0, 'angry': 0, 'disgust': 0, 'fear': 0, 'surprise': 0, 'neutral': 0}\n",
        "\n",
        "        combined_caption = \"\"\n",
        "\n",
        "        for image_url in images:\n",
        "            try:\n",
        "                response = requests.get(image_url)\n",
        "                image = Image.open(BytesIO(response.content))\n",
        "                st.image(image, caption=\"Fetched Image\", use_container_width=True)  # Updated to use_container_width\n",
        "                # Generate and display the caption\n",
        "                caption = generate_caption(image)\n",
        "                st.success(caption)\n",
        "                combined_caption += caption + \" \"\n",
        "\n",
        "                # Extract text from the image and handle cases where extracted text is a list\n",
        "                extracted_text = extract_text_from_image(image)\n",
        "                if isinstance(extracted_text, list):\n",
        "                    extracted_text = \"\\n\".join(extracted_text)  # Join the list into a single string\n",
        "                if extracted_text.strip():  # Strip leading/trailing spaces\n",
        "                    translated_text = GoogleTranslator(source='auto', target='en').translate(extracted_text)\n",
        "                    extracted_texts.append(translated_text)\n",
        "                    st.write(\"Extracted and Translated Text from Image:\")\n",
        "                    st.text(translated_text)\n",
        "\n",
        "                # Analyze facial emotions in the image\n",
        "                dominant_emotion = detect_emotions_from_image(image)\n",
        "                if dominant_emotion:\n",
        "                    st.success(f\"Dominant Emotion Detected: {dominant_emotion}\")\n",
        "                    all_emotions[dominant_emotion] += 1\n",
        "                else:\n",
        "                    st.error(\"No faces detected or error in emotion analysis.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error processing image {image_url}: {e}\")\n",
        "\n",
        "        # After processing all images, analyze the emotion counts and provide a suggestion\n",
        "        if all_emotions:\n",
        "            # Add graph for all_emotions\n",
        "            emotion_df = pd.DataFrame(list(all_emotions.items()), columns=['Emotion', 'Count'])\n",
        "\n",
        "            # Create and display a bar chart for all emotions\n",
        "            fig = px.bar(emotion_df, x='Emotion', y='Count',\n",
        "                        color='Emotion',\n",
        "                        title=\"Aggregated Emotion Counts Across All Images\",\n",
        "                        labels={'Emotion': 'Detected Emotions', 'Count': 'Frequency'},\n",
        "                        text='Count')  # Display count on bars for better clarity\n",
        "\n",
        "            # Show the graph in Streamlit\n",
        "            st.plotly_chart(fig)\n",
        "\n",
        "            dominant_emotion = max(all_emotions, key=all_emotions.get)\n",
        "            st.success(f\"Most Frequent Emotion Across All Images or no Images(Default): {dominant_emotion}\")\n",
        "            emotion_summary = \", \".join([f\"{emotion}: {count}\" for emotion, count in all_emotions.items()])\n",
        "            analyze_with_gemini(dominant_emotion, all_emotions)\n",
        "        else:\n",
        "            st.error(\"No images processed or error in emotion analysis.\")\n",
        "\n",
        "        return extracted_texts, combined_caption\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error fetching images: {e}\")\n",
        "        return []\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "\n",
        "# ---------------- KNOWLEDGE GRAPH -------------------------------\n",
        "def create_knowledge_graph(input_text, classifications, probabilities):\n",
        "    # Initialize a directed graph\n",
        "    graph = nx.DiGraph()\n",
        "\n",
        "    # Add the central node (input text)\n",
        "    graph.add_node(\"Input Text\", size=1500, color=\"#ADD8E6\")  # Light blue for the central node\n",
        "\n",
        "    # Normalize probabilities for better edge length scaling\n",
        "    max_prob = max(probabilities)\n",
        "    min_prob = min(probabilities)\n",
        "    prob_scaled = [(1 - (p - min_prob) / (max_prob - min_prob)) + 0.1 for p in probabilities]  # Invert probabilities for distances\n",
        "\n",
        "    # Add nodes for classifications and connect them to the input text\n",
        "    for classification, probability, scaled_prob in zip(classifications, probabilities, prob_scaled):\n",
        "        prob_percentage = f\"{probability * 100:.2f}%\"\n",
        "        graph.add_node(classification, size=1000, color=\"#E6E6FA\")  # Light lavender for classification nodes\n",
        "        graph.add_edge(\"Input Text\", classification, weight=scaled_prob, label=prob_percentage)\n",
        "\n",
        "    # Extract node colors and sizes\n",
        "    node_colors = [data[\"color\"] for _, data in graph.nodes(data=True)]\n",
        "    node_sizes = [data[\"size\"] for _, data in graph.nodes(data=True)]\n",
        "\n",
        "    # Compute positions using spring layout, scaling edge lengths with inverted probabilities\n",
        "    pos = nx.spring_layout(graph, seed=42, weight='weight')\n",
        "\n",
        "    # Draw the graph\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    nx.draw(\n",
        "        graph, pos, with_labels=True, node_size=node_sizes, node_color=node_colors,\n",
        "        font_size=10, font_weight=\"bold\", edge_color=\"gray\"\n",
        "    )\n",
        "\n",
        "    # Add edge labels for probabilities\n",
        "    edge_labels = nx.get_edge_attributes(graph, \"label\")\n",
        "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_color=\"red\")\n",
        "\n",
        "    # Display the plot in Streamlit\n",
        "    st.pyplot(plt)\n",
        "# ---------------- KNOWLEDGE GRAPH -------------------------------\n",
        "\n",
        "# Function to classify text and display result\n",
        "def classify_text(text):\n",
        "    # Preprocess the input for each base model\n",
        "    lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "    svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "    nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "    xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "    lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "    transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "    # Pad sequences for LSTM\n",
        "    lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Get probabilities from all base models\n",
        "    lr_proba = lr_model.predict_proba(lr_features)\n",
        "    svm_proba = svm_model.predict_proba(svm_features)\n",
        "    nb_proba = nb_model.predict_proba(nb_features)\n",
        "    xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "    lstm_proba = lstm_model.predict(lstm_features)\n",
        "    transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "    # Combine probabilities as input for the meta-learner\n",
        "    stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "    # Predict using the meta-learner\n",
        "    final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "    final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    top_issue = label_encoder.inverse_transform(final_prediction)[0]\n",
        "    top_probability = final_prediction_proba[0].max()\n",
        "\n",
        "    probabilities = final_prediction_proba[0]\n",
        "    response = get_actual_issue(text,top_issue)\n",
        "    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "        # Find the probability of the response (new top issue)\n",
        "        response_index = np.where(label_encoder.classes_ == response)[0][0]\n",
        "        response_probability = probabilities[response_index]\n",
        "\n",
        "        # Find the index of the current top issue\n",
        "        top_issue_index = np.where(label_encoder.classes_ == top_issue)[0][0]\n",
        "\n",
        "        # Swap the probabilities\n",
        "        probabilities[top_issue_index] = response_probability\n",
        "        probabilities[response_index] = top_probability\n",
        "\n",
        "        # Update the top issue to the response\n",
        "        top_issue = response\n",
        "\n",
        "    # Display the results\n",
        "    st.success(f\"The most likely mental health concern from the text provided is: {top_issue} with a probability of {top_probability:.2%}\")\n",
        "\n",
        "    # Collect classifications and probabilities for the knowledge graph\n",
        "    classifications = label_encoder.classes_\n",
        "\n",
        "    # Show the knowledge graph\n",
        "    create_knowledge_graph(text, classifications, probabilities)\n",
        "\n",
        "    # Pass to a custom insight function if needed\n",
        "    get_wellbeing_insight(text, top_issue)\n",
        "\n",
        "    # Show original table\n",
        "    st.subheader(\"Association Matrix\")\n",
        "    st.table(df)\n",
        "\n",
        "    weighted_sum_index, weighted_sum_row_name = weighted_sum_analysis(matrix, probabilities)\n",
        "    cosine_similarity_index, cosine_similarity_row_name = cosine_similarity_analysis(matrix, probabilities)\n",
        "    euclidian_distance_index, euclidian_distance_row_name = euclidian_distance_analysis(matrix, probabilities)\n",
        "\n",
        "    consensus_string = get_consensus_string(weighted_sum_row_name, cosine_similarity_row_name, euclidian_distance_row_name)\n",
        "    # st.info(consensus_string)\n",
        "    get_parameter_insight(consensus_string, top_issue)\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "# Function to get wellbeing insights from Gemini model\n",
        "def get_wellbeing_insight(text, top_issue):\n",
        "    try:\n",
        "        chat_session = gemini_model.start_chat(history=[])\n",
        "        prompt = f\"\"\" The Ryff Scale is based on six factors: autonomy, environmental mastery, personal growth, positive relations with others, purpose in life, and self-acceptance. Higher total scores indicate higher psychological well-being. Following are explanations of each criterion, and an example statement from the Ryff Inventory to measure each criterion: Autonomy: High scores indicate that the respondent is independent and regulates his or her behavior independent of social pressures. An example statement for this criterion is \"I have confidence in my opinions, even if they are contrary to the general consensus.\" Environmental Mastery: High scores indicate that the respondent makes effective use of opportunities and has a sense of mastery in managing environmental factors and activities, including managing everyday affairs and creating situations to benefit personal needs. An example statement for this criterion is \"In general, I feel I am in charge of the situation in which I live.\"Personal Growth: High scores indicate that the respondent continues to develop, is welcoming to new experiences, and recognizes improvement in behavior and self over time. An example statement for this criterion is \"I think it is important to have new experiences that challenge how you think about yourself and the world.\"Positive Relations with Others: High scores reflect the respondent's engagement in meaningful relationships with others that include reciprocal empathy, intimacy, and affection. An example statement for this criterion is \"People would describe me as a giving person, willing to share my time with others.\" Purpose in Life: High scores reflect the respondent's strong goal orientation and conviction that life holds meaning. An example statement for this criterion is \"Some people wander aimlessly through life, but I am not one of them.\"Self-Acceptance: High scores reflect the respondent's positive attitude about his or her self. An example statement for this criterion is \"I like most aspects of my personality.\" Now, please use the above information and {text}, along with image captions (if added) that have been added and the mental health issue: {top_issue}, to generate a short paragraph for each of the following subtopics, discussing how the {top_issue} and {text} may relate to these factors of mental well-being: 1. **Autonomy**: How might {top_issue} impact a person's ability to be independent and self-regulate behavior? 2. **Environmental Mastery**: Discuss how {top_issue} may affect a person's ability to manage their environment and activities. 3. **Personal Growth**: What impact might {top_issue} have on an individual's development, openness to new experiences, and recognition of self-improvement? 4. **Positive Relations with Others**: How does {top_issue} influence the ability to maintain meaningful and empathetic relationships? 5. **Purpose in Life**: How might {top_issue} shape an individual's sense of purpose or goal orientation in life? 6. **Self-Acceptance**: What role does {top_issue} play in a person's self-image and acceptance of themselves? Based on the {text}, provide practical advice to improve or reduce the impact of {top_issue}.\"\"\"\n",
        "\n",
        "        response = chat_session.send_message(prompt)\n",
        "\n",
        "        st.write(\"### Wellbeing Insight:\")\n",
        "        st.write(response.text)\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error retrieving wellbeing insights: {e}\")\n",
        "\n",
        "def get_parameter_insight(text, top_issue):\n",
        "    try:\n",
        "        chat_session = gemini_model.start_chat(history=[])\n",
        "        prompt = f\"\"\" The specific parameters from Ryff Scale of Psychological wellbeing that affects the user are {text}. Based on these parameters {text} and mental issue {top_issue} provide practical advice on how a person with {top_issue} mental issue should work on {text} to improve himself/herself. \"\"\"\n",
        "\n",
        "        response = chat_session.send_message(prompt)\n",
        "\n",
        "        st.write(\"### Specific Parameter Based Insight:\")\n",
        "        st.write(response.text)\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error retrieving wellbeing insights: {e}\")\n",
        "\n",
        "# Function to extract text from image using Tesseract\n",
        "def extract_text_from_image(image):\n",
        "    extracted_text = pytesseract.image_to_string(image)\n",
        "    return extracted_text.splitlines()\n",
        "\n",
        "# Function to extract text from an image using Tesseract\n",
        "def extract_text_from_image_video(image):\n",
        "    extracted_text = pytesseract.image_to_string(image)\n",
        "    return extracted_text if extracted_text else \"\"  # Return empty string if no text is found\n",
        "\n",
        "# Function to extract audio from a video file and classify it\n",
        "# Function to extract 20 frames from a video file\n",
        "def extract_frames(video_path, num_frames=20):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    frames = []\n",
        "    frame_interval = total_frames // num_frames  # Calculate frame interval\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n",
        "\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def transcribe_audio_from_video(video_file):\n",
        "    try:\n",
        "        # Save the uploaded video file to a temporary file\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_video_file:\n",
        "            temp_video_file.write(video_file.read())\n",
        "            temp_video_path = temp_video_file.name\n",
        "\n",
        "        audio_path = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False).name\n",
        "\n",
        "        # Extract audio from video using subprocess\n",
        "        subprocess.run([\"ffmpeg\", \"-i\", temp_video_path, \"-q:a\", \"0\", \"-map\", \"a\", audio_path, \"-y\"])\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "        # Use Whisper to transcribe the audio\n",
        "        result = whisper_model.transcribe(audio_path)\n",
        "\n",
        "        # Get the transcribed text and translate if necessary\n",
        "        transcribed_text = result[\"text\"]\n",
        "        translated_text = GoogleTranslator(source=\"auto\", target=\"en\").translate(transcribed_text)\n",
        "\n",
        "        # Clean up temporary files\n",
        "        os.remove(temp_video_path)\n",
        "        os.remove(audio_path)\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        # Display a user-friendly message if the video is too long or another error occurs\n",
        "        if \"duration\" in str(e).lower() or \"length\" in str(e).lower():\n",
        "            return \"The video is too long to process. Please upload a shorter video.\"\n",
        "        else:\n",
        "            return f\"An error occurred: {e}\"\n",
        "\n",
        "# Function to translate text using DeepL\n",
        "def translate_text(text, target_lang=\"en\"):\n",
        "    try:\n",
        "        if text:\n",
        "            translated_text = GoogleTranslator(source=\"auto\", target=target_lang).translate(text)\n",
        "            return translated_text\n",
        "        return \"\"  # Return empty string if text is empty or None\n",
        "    except Exception as e:\n",
        "        return f\"Error translating text: {str(e)}\"\n",
        "\n",
        "# Function to extract audio from a video file\n",
        "def extract_audio_from_video(video_path):\n",
        "    try:\n",
        "        # Generate a temporary audio file path\n",
        "        audio_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\").name\n",
        "\n",
        "        # Use FFmpeg to extract audio from video\n",
        "        subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\", audio_path, \"-y\"])\n",
        "\n",
        "        # Return the path of the extracted audio\n",
        "        return audio_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting audio: {str(e)}\"\n",
        "\n",
        "\n",
        "# Function to analyze audio mood based on extracted audio\n",
        "def analyze_audio_mood(video_path):\n",
        "    try:\n",
        "        audio_path = extract_audio_from_video(video_path)\n",
        "        myfile = genai.upload_file(audio_path)\n",
        "        prompt = \"Classify the tone and mood of the given audio file based on the following conditions: For **tone**, choose from Calm (moderate pitch, smooth energy, consistent speech rate), Excited (high pitch, rapid speech, dynamic energy), Tense (strained voice, high zero-crossing rate, uneven energy), Flat (low pitch variation, monotone delivery, low spectral contrast), Confident (strong energy, clear articulation, stable rhythm), Fearful (high pitch, irregular pauses, trembling voice), Sad (low pitch, slow speech rate, reduced spectral brightness), or Angry (loud volume, fast speech rate, sharp spectral edges). For **mood**, choose from Relaxed (low tempo, smooth rhythm, low spectral variance), Happy (bright spectral centroid, high tempo, energetic rhythm), Worried (irregular rhythm, increased pauses, unstable pitch), Stressed (high energy, rapid speech, high zero-crossing rate), Melancholic (low tempo, soft volume, monotone delivery), Agitated (fast tempo, irregular pitch changes, high loudness), Detached (low energy, slow speech, long silences), or Energetic (high tempo, bright pitch, strong spectral roll-off). Provide a compact response with the classified tone and mood, and a concise summary of the analysis.\"\n",
        "        result = gemini_model.generate_content([myfile, prompt])\n",
        "        st.info(result.text)\n",
        "\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            # st.error(f\"Error analyzing audio mood: {str(e)}\")\n",
        "            # Extract audio from the video (assuming extract_audio_from_video is implemented)\n",
        "            audio_path = extract_audio_from_video(video_path)\n",
        "            # Load the audio file using librosa\n",
        "            y, sr = librosa.load(audio_path)\n",
        "            # Extract MFCCs (Mel-frequency cepstral coefficients) from the audio signal\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            # Divide the MFCC array into 4 frequency bands and calculate scalar mean for each band\n",
        "            # Low Frequencies: MFCC 0, 1, 2\n",
        "            low_freq_mfcc = np.mean(mfcc[0:3], axis=1)\n",
        "            mean_low = np.mean(low_freq_mfcc)  # Scalar mean for low frequencies\n",
        "            # Mid-Low Frequencies: MFCC 3, 4\n",
        "            mid_low_freq_mfcc = np.mean(mfcc[3:5], axis=1)\n",
        "            mean_mid_low = np.mean(mid_low_freq_mfcc)  # Scalar mean for mid-low frequencies\n",
        "            # Mid-High Frequencies: MFCC 5, 6, 7\n",
        "            mid_high_freq_mfcc = np.mean(mfcc[5:8], axis=1)\n",
        "            mean_mid_high = np.mean(mid_high_freq_mfcc)  # Scalar mean for mid-high frequencies\n",
        "            # High Frequencies: MFCC 8, 9, 10, 11, 12\n",
        "            high_freq_mfcc = np.mean(mfcc[8:13], axis=1)\n",
        "            mean_high = np.mean(high_freq_mfcc)  # Scalar mean for high frequencies\n",
        "            # Now use these scalar means for classification\n",
        "            if mean_high <= mean_low and mean_high <= mean_mid_low and mean_high <= mean_mid_high:\n",
        "                st.info(\"Audio sounds normal, with no dominant emotion detected\")\n",
        "            elif mean_mid_high <= mean_low and mean_mid_high <= mean_mid_low and mean_mid_high <= mean_high:\n",
        "                st.info(\"Audio sounds neutral, calm, or peaceful\")\n",
        "            elif mean_mid_low <= mean_low and mean_mid_low <= mean_mid_high and mean_mid_low <= mean_high:\n",
        "                st.info(\"Audio sounds slightly melancholic or neutral\")\n",
        "            elif mean_low <= mean_mid_low and mean_low <= mean_mid_high and mean_low <= mean_high:\n",
        "                st.info(\"Audio sounds calm or melancholic, with less intensity\")\n",
        "            elif mean_high > mean_low and mean_high > mean_mid_low and mean_high <= mean_mid_high:\n",
        "                st.info(\"Audio sounds depressive or anxious in nature\")\n",
        "            else :\n",
        "                st.info(\"Audio sounds upbeat and energetic (Happy)\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error analyzing audio mood: The video / audio is corrupted or do not exist.\")\n",
        "\n",
        "# ----------------- Adding Retrain Model functionality\n",
        "# File path for dataset\n",
        "dataset_path = 'preprocessed_mental_health.csv'\n",
        "# Download stopwords (if you haven't already)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')  # This line is added to download the necessary data\n",
        "# Define a list of negative words to retain\n",
        "negative_words = {\"not\", \"no\", \"nor\", \"never\", \"nothing\", \"nowhere\", \"neither\", \"cannot\", \"n't\", \"without\", \"barely\", \"hardly\", \"scarcely\"}\n",
        "\n",
        "# Define a function to clean the text\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove special characters, numbers, and punctuations\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords, but keep negative words\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english') or word in negative_words]\n",
        "    # Join the tokens back into a single string\n",
        "    clean_text = ' '.join(tokens)\n",
        "    return clean_text\n",
        "\n",
        "def update_dataset(new_text, mental_health_issue):\n",
        "    if os.path.exists(dataset_path):\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "    else:\n",
        "        # If the dataset doesn't exist, create a new one\n",
        "        dataset = pd.DataFrame(columns=['text', 'mental_health_issue', 'cleaned_text'])\n",
        "\n",
        "    # Clean the text (adjust cleaning based on your preprocessing)\n",
        "    # cleaned_text = new_text.lower()\n",
        "    cleaned_text = clean_text(new_text)\n",
        "\n",
        "    # Create a new DataFrame for the new row\n",
        "    new_row = pd.DataFrame({\n",
        "        'text': [new_text],\n",
        "        'mental_health_issue': [mental_health_issue],\n",
        "        'cleaned_text': [cleaned_text]\n",
        "    })\n",
        "\n",
        "    # Use pd.concat to append the new row\n",
        "    dataset = pd.concat([dataset, new_row], ignore_index=True)\n",
        "\n",
        "    # Save the updated dataset\n",
        "    dataset.to_csv(dataset_path, index=False)\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "def retrain_model():\n",
        "    progress = st.progress(0)\n",
        "    progress_step = 0\n",
        "\n",
        "    # Use an expander to group status messages\n",
        "    with st.expander(\"Detailed Status\", expanded=False):\n",
        "        try:\n",
        "            # Load and process the dataset\n",
        "            data = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "            if 'cleaned_text' not in data.columns:\n",
        "                st.error(\"The dataset must have a 'cleaned_text' column.\")\n",
        "                return\n",
        "\n",
        "            data.dropna(subset=['cleaned_text'], inplace=True)\n",
        "            # Split features and target\n",
        "            X_test = data['cleaned_text']\n",
        "            y_test = data['mental_health_issue']\n",
        "\n",
        "            # Encode target labels\n",
        "            y_test = label_encoder.transform(y_test)\n",
        "            st.success(\"Dataset loaded and processed successfully!\")\n",
        "            progress_step += 10\n",
        "            progress.progress(progress_step)\n",
        "\n",
        "            # Process the text for each model\n",
        "            st.info(\"Processing text for each model...\")\n",
        "            X_test_lr = lr_vectorizer.transform(X_test)  # Logistic Regression vectorizer\n",
        "            X_test_svm = svm_vectorizer.transform(X_test)  # SVM vectorizer\n",
        "            X_test_xgb = tfidf_vectorizer.transform(X_test)  # XGBoost vectorizer\n",
        "            X_test_nb = nb_vectorizer.transform(X_test)  # Naive Bayes vectorizer\n",
        "            X_test_lstm = lstm_tokenizer.texts_to_sequences(X_test)  # LSTM tokenizer\n",
        "            # Preprocess the text for the transformer model\n",
        "            X_test_transformer = t_vectorize_layer(X_test)\n",
        "\n",
        "            # Pad sequences for LSTM\n",
        "            X_test_lstm = pad_sequences(X_test_lstm, maxlen=100, padding='post', truncating='post')\n",
        "            st.success(\"Text processed successfully!\")\n",
        "            progress_step += 10\n",
        "            progress.progress(progress_step)\n",
        "\n",
        "            st.info(\"Predicting using base models...\")\n",
        "            # Get predictions from the base models\n",
        "            lr_predictions_proba = lr_model.predict_proba(X_test_lr)  # Logistic Regression probabilities\n",
        "            svm_predictions_proba = svm_model.predict_proba(X_test_svm)  # SVM probabilities\n",
        "            xgb_predictions_proba = xgb_model.predict_proba(X_test_xgb)  # XGBoost probabilities\n",
        "            nb_predictions_proba = nb_model.predict_proba(X_test_nb)  # Naive Bayes probabilities\n",
        "            lstm_predictions_proba = lstm_model.predict(X_test_lstm)  # LSTM probabilities\n",
        "\n",
        "            progress_step += 10\n",
        "            progress.progress(progress_step)\n",
        "\n",
        "            # Get probabilities from the transformer model\n",
        "            transformer_predictions_proba = transformer_model.predict(X_test_transformer)\n",
        "\n",
        "            # Get predictions and stack them\n",
        "            stacked_features = np.hstack((\n",
        "                lr_predictions_proba,\n",
        "                svm_predictions_proba,\n",
        "                xgb_predictions_proba,\n",
        "                nb_predictions_proba,\n",
        "                lstm_predictions_proba,\n",
        "                transformer_predictions_proba\n",
        "            ))\n",
        "            st.success(\"Predictions generated successfully!\")\n",
        "            progress_step += 10\n",
        "            progress.progress(progress_step)\n",
        "\n",
        "            # Train and evaluate the meta-learner\n",
        "            st.info(\"Training the meta-learner...\")\n",
        "            # Split data into training and test sets\n",
        "            X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
        "            stacked_features, y_test, test_size=0.2, random_state=42, stratify=y_test\n",
        "            )\n",
        "\n",
        "            # Train Random Forest as the meta-learner\n",
        "            meta_learner_rf = RandomForestClassifier(\n",
        "              max_depth=None,            # Maximum depth of each tree\n",
        "              min_samples_split=20,      # Minimum number of samples to split a node\n",
        "\n",
        "              min_samples_leaf=1,        # Minimum number of samples in a leaf node\n",
        "              max_features='sqrt',       # Number of features to consider at each split\n",
        "              bootstrap=False,            # Whether to use bootstrapping\n",
        "\n",
        "              random_state=42            # For reproducibility\n",
        "            )\n",
        "            meta_learner_rf.fit(X_train1, y_train1)\n",
        "            st.success(\"Meta-learner trained successfully!\")\n",
        "            progress_step += 20\n",
        "            progress.progress(progress_step)\n",
        "\n",
        "            # Save the meta-learner and evaluate it\n",
        "            with open('meta_learner_rf.pkl', 'wb') as file:\n",
        "                pickle.dump(meta_learner_rf, file)\n",
        "            final_predictions_lr = meta_learner_rf.predict(X_test1)\n",
        "            accuracy_rf = accuracy_score(y_test1, final_predictions_lr)\n",
        "            st.metric(label=\"Meta-Learner Accuracy\", value=f\"{accuracy_rf:.2%}\")\n",
        "            progress_step += 20\n",
        "            progress.progress(progress_step)\n",
        "\n",
        "            # Cross-validation\n",
        "            st.info(\"Performing cross-validation...\")\n",
        "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            cross_val_accuracies = []\n",
        "            for train_index, test_index in skf.split(stacked_features, y_test):\n",
        "                X_train_fold, X_test_fold = stacked_features[train_index], stacked_features[test_index]\n",
        "                y_train_fold, y_test_fold = y_test[train_index], y_test[test_index]\n",
        "\n",
        "                meta_learner_rf.fit(X_train_fold, y_train_fold)\n",
        "                fold_accuracy = meta_learner_rf.score(X_test_fold, y_test_fold)\n",
        "                cross_val_accuracies.append(fold_accuracy)\n",
        "\n",
        "            # Convert to NumPy array for consistency\n",
        "            cross_val_accuracies = np.array(cross_val_accuracies)\n",
        "\n",
        "            # Calculate mean and standard deviation\n",
        "            mean_val_accuracy = np.mean(cross_val_accuracies)\n",
        "            std_val_accuracy = np.std(cross_val_accuracies)\n",
        "\n",
        "            progress.progress(100)\n",
        "\n",
        "            # Celebrate successful execution\n",
        "            st.success(\"Execution completed successfully!\")\n",
        "            st.balloons()\n",
        "\n",
        "            return meta_learner_rf, mean_val_accuracy\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "def update_and_retrain(example_text, example_issue):\n",
        "    try:\n",
        "        # Update dataset\n",
        "        update_dataset(example_text, example_issue)\n",
        "\n",
        "        # Display the last three rows of the updated dataset\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "        st.write(\"Updated Dataset (Last 3 Rows):\")\n",
        "        st.write(dataset.tail(3))\n",
        "\n",
        "        # Notify user about retraining process\n",
        "        st.info(\"Model is being retrained...\")\n",
        "\n",
        "        # Retrain model\n",
        "        model, accuracy = retrain_model()\n",
        "\n",
        "        # Display retraining success message\n",
        "        if model:\n",
        "            #st.success(f\"Model retrained successfully! Accuracy: {accuracy * 100:.2f}%\")\n",
        "            st.metric(label=\"Retrained Model Accuracy\", value=f\"{accuracy * 100:.2f}%\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "# Function to classify text, display result and retrain model\n",
        "def classify_text_retrain_model(text):\n",
        "    # Preprocess the input for each base model\n",
        "    lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "    svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "    nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "    xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "    lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "    transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "    # Pad sequences for LSTM\n",
        "    lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Get probabilities from all base models\n",
        "    lr_proba = lr_model.predict_proba(lr_features)\n",
        "    svm_proba = svm_model.predict_proba(svm_features)\n",
        "    nb_proba = nb_model.predict_proba(nb_features)\n",
        "    xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "    lstm_proba = lstm_model.predict(lstm_features)\n",
        "    transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "    # Combine probabilities as input for the meta-learner\n",
        "    stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "    # Predict using the meta-learner\n",
        "    final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "    final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    top_issue = label_encoder.inverse_transform(final_prediction)[0]\n",
        "    top_probability = final_prediction_proba[0].max()\n",
        "\n",
        "    probabilities = final_prediction_proba[0]\n",
        "    response = get_actual_issue(text,top_issue)\n",
        "    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "        # Find the probability of the response (new top issue)\n",
        "        response_index = np.where(label_encoder.classes_ == response)[0][0]\n",
        "        response_probability = probabilities[response_index]\n",
        "\n",
        "        # Find the index of the current top issue\n",
        "        top_issue_index = np.where(label_encoder.classes_ == top_issue)[0][0]\n",
        "\n",
        "        # Swap the probabilities\n",
        "        probabilities[top_issue_index] = response_probability\n",
        "        probabilities[response_index] = top_probability\n",
        "\n",
        "        # Update the top issue to the response\n",
        "        top_issue = response\n",
        "\n",
        "    # Display the results\n",
        "    st.success(f\"The most likely mental health concern from all the text obtained is: {top_issue} with a probability of {top_probability:.2%}\")\n",
        "\n",
        "    # Collect classifications and probabilities for the knowledge graph\n",
        "    classifications = label_encoder.classes_\n",
        "\n",
        "    # Show the knowledge graph\n",
        "    create_knowledge_graph(text, classifications, probabilities)\n",
        "\n",
        "    # Pass to a custom insight function if needed\n",
        "    get_wellbeing_insight(text, top_issue)\n",
        "\n",
        "    # Show original table\n",
        "    st.subheader(\"Association Matrix\")\n",
        "    st.table(df)\n",
        "\n",
        "    weighted_sum_index, weighted_sum_row_name = weighted_sum_analysis(matrix, probabilities)\n",
        "    cosine_similarity_index, cosine_similarity_row_name = cosine_similarity_analysis(matrix, probabilities)\n",
        "    euclidian_distance_index, euclidian_distance_row_name = euclidian_distance_analysis(matrix, probabilities)\n",
        "\n",
        "    consensus_string = get_consensus_string(weighted_sum_row_name, cosine_similarity_row_name, euclidian_distance_row_name)\n",
        "    # st.info(consensus_string)\n",
        "    get_parameter_insight(consensus_string, top_issue)\n",
        "\n",
        "    # Adding Model Retraining Functionality\n",
        "    update_and_retrain(text, top_issue)\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "# ----------------- Adding Retrain Model functionality\n",
        "\n",
        "# ---------------------- Adding Facial Recognition for video\n",
        "def detect_emotions_from_frame(frame):\n",
        "    try:\n",
        "        # Use DeepFace to analyze emotions\n",
        "        result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
        "        return result[0]['dominant_emotion']\n",
        "    except Exception as e:\n",
        "        print(f\"No expression or error detecting emotion: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_emotions_from_frames(frames):\n",
        "    emotion_counts = {'happy': 0, 'sad': 0, 'angry': 0, 'disgust': 0, 'fear': 0, 'surprise': 0, 'neutral': 0}\n",
        "    frame_emotions = []\n",
        "\n",
        "    for idx, frame in enumerate(frames):\n",
        "        emotion = detect_emotions_from_frame(frame)\n",
        "        if emotion:\n",
        "            frame_emotions.append(emotion)\n",
        "            if emotion in emotion_counts:\n",
        "                emotion_counts[emotion] += 1\n",
        "\n",
        "    return emotion_counts, frame_emotions\n",
        "\n",
        "def display_emotion_summary(emotion_counts):\n",
        "    try:\n",
        "        # Convert the emotion counts to a DataFrame for display and plotting\n",
        "        emotion_df = pd.DataFrame(list(emotion_counts.items()), columns=['Emotion', 'Count'])\n",
        "        st.write(\"Emotion Analysis Summary:\")\n",
        "\n",
        "        # Add a bar chart for emotion counts\n",
        "        fig = px.bar(emotion_df, x='Emotion', y='Count',\n",
        "                    color='Emotion',\n",
        "                    title=\"Emotion Counts\",\n",
        "                    labels={'Emotion': 'Detected Emotions', 'Count': 'Frequency'})\n",
        "\n",
        "        unique_key = f\"emotion_chart_{int(time.time() * 1000)}\"\n",
        "        st.plotly_chart(fig, key=unique_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying emotion summary: {e}\")\n",
        "\n",
        "    # Return the dominant emotion\n",
        "    return max(emotion_counts, key=emotion_counts.get)\n",
        "\n",
        "def analyze_with_gemini(dominant_emotion, emotion_counts):\n",
        "    try:\n",
        "        # Start a chat session with the Gemini API\n",
        "        chat_session = gemini_model.start_chat(history=[])\n",
        "\n",
        "        # Create a detailed summary of emotion counts for the prompt\n",
        "        emotion_summary = \", \".join([f\"{emotion}: {count}\" for emotion, count in emotion_counts.items()])\n",
        "\n",
        "        # Craft the prompt with the dominant emotion and emotion summary\n",
        "        # prompt = (\n",
        "           # f\"The detected dominant emotion is '{dominant_emotion}', and the counts for each emotion are as follows: {emotion_summary}. \"\n",
        "          #  f\"Analyze this data in the context of possible mental health issues (e.g., depression, anxiety, PTSD, or bipolar) and provide a suggestion.\"\n",
        "        #)\n",
        "        prompt = f\"The detected dominant emotion is '{dominant_emotion}'. {emotion_summary}. Based on this information, analyze the potential implications for mental health conditions such as depression, anxiety, PTSD, or bipolar disorder. Provide insights into how these emotions might relate to these mental health issues and suggest actionable advice or strategies to improve mental well-being. Give only three lines.\"\n",
        "\n",
        "        # Send the prompt via the chat session\n",
        "        response = chat_session.send_message(prompt)\n",
        "        st.write(response.text)\n",
        "    except Exception as e:\n",
        "        # Log the error (optional, for debugging purposes)\n",
        "        print(f\"Error in Gemini API call: {e}\")\n",
        "\n",
        "        # Return a user-friendly error message\n",
        "        return \"An error occurred while communicating with the Gemini API. Please try again later.\"\n",
        "# ---------------------- Adding Facial Recognition for video\n",
        "\n",
        "# ---------------------- Adding Facial Recognition for image\n",
        "def detect_emotions_from_image(image):\n",
        "    try:\n",
        "        # Convert PIL image to NumPy array if needed\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        # Use DeepFace to analyze emotions\n",
        "        result = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
        "        return result[0]['dominant_emotion']\n",
        "    except Exception as e:\n",
        "        st.error(f\"No expression or error detecting emotion: {e}\")\n",
        "        print(f\"No expression or error detecting emotion: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_emotions_from_image(image):\n",
        "    try:\n",
        "        # Convert PIL image to NumPy array if needed\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        # Initialize emotion counters\n",
        "        emotion_counts = {'happy': 0, 'sad': 0, 'angry': 0, 'disgust': 0, 'fear': 0, 'surprise': 0, 'neutral': 0}\n",
        "        detected_emotions = []\n",
        "\n",
        "        # Use DeepFace to detect and analyze faces\n",
        "        results = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
        "\n",
        "        # Initialize a DataFrame to store emotion probabilities\n",
        "        emotion_data = []\n",
        "\n",
        "        # If results contain multiple faces, process each one\n",
        "        for result in results:\n",
        "            if 'emotion' in result:\n",
        "                emotion_data.append(result['emotion'])\n",
        "\n",
        "        # Convert emotion data to a DataFrame\n",
        "        emotion_df = pd.DataFrame(emotion_data)\n",
        "\n",
        "        # Calculate mean probabilities for each emotion (if multiple faces are detected)\n",
        "        mean_emotions = emotion_df.mean().reset_index()\n",
        "        mean_emotions.columns = ['Emotion', 'Average Probability']\n",
        "\n",
        "        # Create and display a bar chart\n",
        "        fig = px.bar(mean_emotions, x='Emotion', y='Average Probability',\n",
        "                    color='Emotion',\n",
        "                    title=\"Average Emotion Probabilities from Analyzed Faces\",\n",
        "                    labels={'Emotion': 'Detected Emotions', 'Average Probability': 'Probability'})\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "        # If results contain multiple faces, process each one\n",
        "        for result in results:\n",
        "            emotion = result.get('dominant_emotion')\n",
        "            if emotion:\n",
        "                detected_emotions.append(emotion)\n",
        "                if emotion in emotion_counts:\n",
        "                    emotion_counts[emotion] += 1\n",
        "\n",
        "        return emotion_counts, detected_emotions\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing emotions from image: {e}\")\n",
        "        return {}, []\n",
        "# ---------------------- Adding Facial Recognition for image\n",
        "\n",
        "# ---------------------- Get Image Description\n",
        "# Function to load the model (cached for efficiency)\n",
        "@st.cache_resource\n",
        "def load_model_img():\n",
        "    model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "    feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, feature_extractor, tokenizer\n",
        "\n",
        "# Load the model\n",
        "IDmodel, IDfeature_extractor, IDtokenizer = load_model_img()\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IDmodel.to(device)\n",
        "\n",
        "# Function to generate caption\n",
        "def generate_caption(image):\n",
        "    # Preprocess the image\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(mode=\"RGB\")\n",
        "    pixel_values = IDfeature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    # Generate caption (you can adjust max_length and num_beams as needed)\n",
        "    with torch.no_grad():\n",
        "        output_ids = IDmodel.generate(pixel_values, max_length=16, num_beams=4)\n",
        "    caption = IDtokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "def classify_text_with_desc(text,text2):\n",
        "    # Preprocess the input for each base model\n",
        "    lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "    svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "    nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "    xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "    lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "    transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "    # Pad sequences for LSTM\n",
        "    lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Get probabilities from all base models\n",
        "    lr_proba = lr_model.predict_proba(lr_features)\n",
        "    svm_proba = svm_model.predict_proba(svm_features)\n",
        "    nb_proba = nb_model.predict_proba(nb_features)\n",
        "    xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "    lstm_proba = lstm_model.predict(lstm_features)\n",
        "    transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "    # Combine probabilities as input for the meta-learner\n",
        "    stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "    # Predict using the meta-learner\n",
        "    final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "    final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    top_issue = label_encoder.inverse_transform(final_prediction)[0]\n",
        "    top_probability = final_prediction_proba[0].max()\n",
        "\n",
        "    probabilities = final_prediction_proba[0]\n",
        "    response = get_actual_issue(text+\" \"+text2,top_issue)\n",
        "    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "        # Find the probability of the response (new top issue)\n",
        "        response_index = np.where(label_encoder.classes_ == response)[0][0]\n",
        "        response_probability = probabilities[response_index]\n",
        "\n",
        "        # Find the index of the current top issue\n",
        "        top_issue_index = np.where(label_encoder.classes_ == top_issue)[0][0]\n",
        "\n",
        "        # Swap the probabilities\n",
        "        probabilities[top_issue_index] = response_probability\n",
        "        probabilities[response_index] = top_probability\n",
        "\n",
        "        # Update the top issue to the response\n",
        "        top_issue = response\n",
        "\n",
        "    # Display the results\n",
        "    st.success(f\"The most likely mental health concern from all the text obtained is: {top_issue} with a probability of {top_probability:.2%}\")\n",
        "\n",
        "    # Collect classifications and probabilities for the knowledge graph\n",
        "    classifications = label_encoder.classes_\n",
        "\n",
        "    # Show the knowledge graph\n",
        "    create_knowledge_graph(text+\" \"+text2, classifications, probabilities)\n",
        "\n",
        "    get_wellbeing_insight(text+\" \"+text2, top_issue)\n",
        "\n",
        "    # Show original table\n",
        "    st.subheader(\"Association Matrix\")\n",
        "    st.table(df)\n",
        "\n",
        "    weighted_sum_index, weighted_sum_row_name = weighted_sum_analysis(matrix, probabilities)\n",
        "    cosine_similarity_index, cosine_similarity_row_name = cosine_similarity_analysis(matrix, probabilities)\n",
        "    euclidian_distance_index, euclidian_distance_row_name = euclidian_distance_analysis(matrix, probabilities)\n",
        "\n",
        "    consensus_string = get_consensus_string(weighted_sum_row_name, cosine_similarity_row_name, euclidian_distance_row_name)\n",
        "    # st.info(consensus_string)\n",
        "    get_parameter_insight(consensus_string, top_issue)\n",
        "\n",
        "def classify_text_retrain_model_desc(text,text2):\n",
        "    # Preprocess the input for each base model\n",
        "    lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "    svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "    nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "    xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "    lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "    transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "    # Pad sequences for LSTM\n",
        "    lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Get probabilities from all base models\n",
        "    lr_proba = lr_model.predict_proba(lr_features)\n",
        "    svm_proba = svm_model.predict_proba(svm_features)\n",
        "    nb_proba = nb_model.predict_proba(nb_features)\n",
        "    xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "    lstm_proba = lstm_model.predict(lstm_features)\n",
        "    transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "    # Combine probabilities as input for the meta-learner\n",
        "    stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "    # Predict using the meta-learner\n",
        "    final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "    final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    top_issue = label_encoder.inverse_transform(final_prediction)[0]\n",
        "    top_probability = final_prediction_proba[0].max()\n",
        "\n",
        "    probabilities = final_prediction_proba[0]\n",
        "    response = get_actual_issue(text+\" \"+text2,top_issue)\n",
        "    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "        # Find the probability of the response (new top issue)\n",
        "        response_index = np.where(label_encoder.classes_ == response)[0][0]\n",
        "        response_probability = probabilities[response_index]\n",
        "\n",
        "        # Find the index of the current top issue\n",
        "        top_issue_index = np.where(label_encoder.classes_ == top_issue)[0][0]\n",
        "\n",
        "        # Swap the probabilities\n",
        "        probabilities[top_issue_index] = response_probability\n",
        "        probabilities[response_index] = top_probability\n",
        "\n",
        "        # Update the top issue to the response\n",
        "        top_issue = response\n",
        "\n",
        "    # Display the results\n",
        "    st.success(f\"The most likely mental health concern from all the text obtained is: {top_issue} with a probability of {top_probability:.2%}\")\n",
        "\n",
        "    # Collect classifications and probabilities for the knowledge graph\n",
        "    classifications = label_encoder.classes_\n",
        "\n",
        "    # Show the knowledge graph\n",
        "    create_knowledge_graph(text+\" \"+text2, classifications, probabilities)\n",
        "\n",
        "    get_wellbeing_insight(text+\" \"+text2, top_issue)\n",
        "\n",
        "    # Show original table\n",
        "    st.subheader(\"Association Matrix\")\n",
        "    st.table(df)\n",
        "\n",
        "    weighted_sum_index, weighted_sum_row_name = weighted_sum_analysis(matrix, probabilities)\n",
        "    cosine_similarity_index, cosine_similarity_row_name = cosine_similarity_analysis(matrix, probabilities)\n",
        "    euclidian_distance_index, euclidian_distance_row_name = euclidian_distance_analysis(matrix, probabilities)\n",
        "\n",
        "    consensus_string = get_consensus_string(weighted_sum_row_name, cosine_similarity_row_name, euclidian_distance_row_name)\n",
        "    # st.info(consensus_string)\n",
        "    get_parameter_insight(consensus_string, top_issue)\n",
        "\n",
        "    # Adding Model Retraining Functionality\n",
        "    update_and_retrain(text, top_issue)\n",
        "\n",
        "# ---------------- CHANGED AS PER ENSEMBLE MODEL -----------------\n",
        "# ---------------------- Get Image Description\n",
        "\n",
        "# ---------------------- Get Video Description\n",
        "# Function to load the model (cached for efficiency)\n",
        "@st.cache_resource\n",
        "def load_image_captioning_model():\n",
        "    model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "    feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, feature_extractor, tokenizer\n",
        "\n",
        "@st.cache_resource\n",
        "def load_summary_pipeline():\n",
        "    return pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Load models\n",
        "caption_model, Vfeature_extractor, Vtokenizer = load_image_captioning_model()\n",
        "summary_pipeline = load_summary_pipeline()\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "caption_model.to(device)\n",
        "\n",
        "# Function to generate captions for an image\n",
        "def generate_caption_video(image):\n",
        "    # Convert the numpy array (video frame) to a PIL Image\n",
        "    if isinstance(image, np.ndarray):\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(mode=\"RGB\")\n",
        "\n",
        "    # Preprocess the image\n",
        "    pixel_values = Vfeature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        output_ids = caption_model.generate(pixel_values, max_length=16, num_beams=4)\n",
        "    caption = Vtokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Function to generate an overall description of the video\n",
        "def describe_video(frames):\n",
        "    # Generate captions for each frame\n",
        "    captions = [generate_caption_video(frame) for frame in frames]\n",
        "    combined_captions = \" \".join(captions)\n",
        "\n",
        "    # Summarize the captions to get an overall description\n",
        "    summary = summary_pipeline(combined_captions, max_length=50, min_length=25, do_sample=False)[0][\"summary_text\"]\n",
        "    return captions, summary\n",
        "\n",
        "# ---------------------- Get Video Description\n",
        "def get_actual_issue(text,top_issue):\n",
        "    try:\n",
        "        chat_session = gemini_model.start_chat(history=[])\n",
        "        prompt = f\"The given text is : {text}. Is this statement normal or depression or ptsd or anxiety or bipolar? I need only one word answer. I need no explanation. My model gave {top_issue}. I want to confirm. Please give the exactly correct one word answer about what you think.\"\n",
        "        response = chat_session.send_message(prompt)\n",
        "        return response.text.strip().lower()\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# ---------------- reddit and twitter all combined text analysis\n",
        "def classify_alltext(text):\n",
        "    # Preprocess the input for each base model\n",
        "    lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "    svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "    nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "    xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "    lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "    transformer_features = t_vectorizer([text])  # For Transformer\n",
        "    # Pad sequences for LSTM\n",
        "    lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Get probabilities from all base models\n",
        "    lr_proba = lr_model.predict_proba(lr_features)\n",
        "    svm_proba = svm_model.predict_proba(svm_features)\n",
        "    nb_proba = nb_model.predict_proba(nb_features)\n",
        "    xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "    lstm_proba = lstm_model.predict(lstm_features)\n",
        "    transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "    # Combine probabilities as input for the meta-learner\n",
        "    stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "    # Predict using the meta-learner\n",
        "    final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "    final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    top_issue = label_encoder.inverse_transform(final_prediction)[0]\n",
        "    top_probability = final_prediction_proba[0].max()\n",
        "\n",
        "    probabilities = final_prediction_proba[0]\n",
        "    response = get_actual_issue(text,top_issue)\n",
        "    if response != \"\" and len(response.split()) == 1 and response != top_issue:\n",
        "        # Find the probability of the response (new top issue)\n",
        "        response_index = np.where(label_encoder.classes_ == response)[0][0]\n",
        "        response_probability = probabilities[response_index]\n",
        "\n",
        "        # Find the index of the current top issue\n",
        "        top_issue_index = np.where(label_encoder.classes_ == top_issue)[0][0]\n",
        "\n",
        "        # Swap the probabilities\n",
        "        probabilities[top_issue_index] = response_probability\n",
        "        probabilities[response_index] = top_probability\n",
        "\n",
        "        # Update the top issue to the response\n",
        "        top_issue = response\n",
        "\n",
        "    st.success(f\"The most frequently detected mental health concern from all the text obtained is: {top_issue} with a probability of {top_probability * 100:.2f}% from the analyzed text.\")\n",
        "\n",
        "    # Collect classifications and probabilities for the knowledge graph\n",
        "    classifications = label_encoder.classes_\n",
        "\n",
        "    # Show the knowledge graph\n",
        "    create_knowledge_graph(text, classifications, probabilities)\n",
        "\n",
        "    # Pass to a custom insight function if needed\n",
        "    get_wellbeing_insight(text, top_issue)\n",
        "\n",
        "    # Show original table\n",
        "    st.subheader(\"Association Matrix\")\n",
        "    st.table(df)\n",
        "\n",
        "    weighted_sum_index, weighted_sum_row_name = weighted_sum_analysis(matrix, probabilities)\n",
        "    cosine_similarity_index, cosine_similarity_row_name = cosine_similarity_analysis(matrix, probabilities)\n",
        "    euclidian_distance_index, euclidian_distance_row_name = euclidian_distance_analysis(matrix, probabilities)\n",
        "\n",
        "    consensus_string = get_consensus_string(weighted_sum_row_name, cosine_similarity_row_name, euclidian_distance_row_name)\n",
        "    # st.info(consensus_string)\n",
        "    get_parameter_insight(consensus_string, top_issue)\n",
        "\n",
        "    return top_issue\n",
        "\n",
        "# ------------------ reddit -----------------\n",
        "def download_video(video_url, save_path):\n",
        "    \"\"\"Download a video from the given URL and save it locally.\"\"\"\n",
        "    try:\n",
        "        video_data = requests.get(video_url)\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(video_data.content)\n",
        "        return save_path\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error downloading video: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_audio(audio_url, save_path):\n",
        "    try:\n",
        "        audio_data = requests.get(audio_url)\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(audio_data.content)\n",
        "        return save_path\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error downloading audio: {e}\")\n",
        "        return None\n",
        "\n",
        "def combine_video_audio(video_path, audio_path, output_path):\n",
        "    try:\n",
        "        # FFmpeg command to combine video and audio\n",
        "        ffmpeg_command = [\n",
        "            \"/usr/bin/ffmpeg\",\n",
        "            \"-i\", video_path,  # Input video file\n",
        "            \"-i\", audio_path,  # Input audio file\n",
        "            \"-c:v\", \"libx264\",  # Use libx264 codec for video\n",
        "            \"-c:a\", \"aac\",  # Use AAC codec for audio\n",
        "            \"-strict\", \"experimental\",  # Allow experimental AAC encoding\n",
        "            \"-shortest\",  # Use the shortest length (video or audio) to determine the output length\n",
        "            output_path  # Output file path\n",
        "        ]\n",
        "\n",
        "        # Run FFmpeg command\n",
        "        subprocess.run(ffmpeg_command, check=True)\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        st.write(f\"Error combining video and audio: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_user_posts_with_videos(username, max_items=10):\n",
        "    try:\n",
        "        # Attempt to fetch the user's posts\n",
        "        user = reddit.redditor(username)\n",
        "\n",
        "        post_data = []\n",
        "        for submission in user.submissions.new(limit=max_items):\n",
        "            videos = []\n",
        "\n",
        "            # Check if the post is a direct video\n",
        "            if submission.is_video:\n",
        "                # Get the URL of the hosted video (Reddit video URL)\n",
        "                video_url = submission.media['reddit_video']['fallback_url']\n",
        "\n",
        "                # Dynamically generate the audio URL by replacing the resolution part with _AUDIO_128.mp4\n",
        "                audio_url = video_url.split(\"DASH_\")[0] + \"DASH_AUDIO_128.mp4\"\n",
        "                videos.append({'video_url': video_url, 'audio_url': audio_url})\n",
        "\n",
        "            # Only add posts with videos\n",
        "            if videos:\n",
        "                post_data.append({\"text\": submission.title, \"videos\": videos})\n",
        "\n",
        "        return post_data\n",
        "\n",
        "    except praw.exceptions.RedditAPIException as e:\n",
        "        st.error(f\"Error fetching data from user '{username}': {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error fetching user data: {e}\")\n",
        "        return []\n",
        "\n",
        "# --------------- User response to image option -------------------\n",
        "def describe_image(image_path):\n",
        "    try:\n",
        "        chat_session = gemini_model.start_chat(history=[])\n",
        "        prompt = f\"\"\"You are given an image. Analyze the image and provide a detailed description.\"\"\"\n",
        "        img = Image.open(image_path)\n",
        "        response = gemini_model.generate_content([prompt, img])\n",
        "        return f\"**The image description is:** {response.text}\"\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error retrieving description: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Define the image directory\n",
        "IMAGE_DIR = \"images\"\n",
        "RANDOM_IMAGE_PATH = os.path.join(\"./\", \"randim.png\")\n",
        "filename = \"number.txt\"\n",
        "# st.write(RANDOM_IMAGE_PATH)\n",
        "randnum = 10\n",
        "# Function to generate or retrieve the random image\n",
        "def get_random_image():\n",
        "    if not os.path.exists(RANDOM_IMAGE_PATH):\n",
        "        # st.write(\"hello\")\n",
        "        random_number = random.randint(0, 9)\n",
        "        # st.write(random_number)\n",
        "        selected_image = os.path.join(IMAGE_DIR, f\"{random_number}.png\")\n",
        "\n",
        "        if os.path.exists(selected_image):\n",
        "            # Open selected image and write its content to \"randim.png\"\n",
        "            with open(selected_image, \"rb\") as src_file:\n",
        "                with open(RANDOM_IMAGE_PATH, \"wb\") as dest_file:\n",
        "                    # st.write(\"hello2\")\n",
        "                    dest_file.write(src_file.read())\n",
        "        else:\n",
        "            st.error(f\"Image {selected_image} not found in {IMAGE_DIR}.\")\n",
        "        return RANDOM_IMAGE_PATH, random_number\n",
        "    else:\n",
        "        # st.write(\"hello3\")\n",
        "        return RANDOM_IMAGE_PATH, randnum\n",
        "\n",
        "\n",
        "\n",
        "# for survey form\n",
        "# Getting the overall score from the responses\n",
        "def overall_score(responses):\n",
        "    scores = {\n",
        "        \"Self Acceptance\": responses[\"Q1\"] + abs(7 - responses[\"Q2\"]),\n",
        "        \"Positive Relations with Others\": responses[\"Q3\"] + abs(7 - responses[\"Q4\"]),\n",
        "        \"Autonomy\": responses[\"Q5\"] + abs(7 - responses[\"Q6\"]),\n",
        "        \"Environmental Mastery\": responses[\"Q7\"] + abs(7 - responses[\"Q8\"]),\n",
        "        \"Purpose in Life\": responses[\"Q9\"] + abs(7 - responses[\"Q10\"]),\n",
        "        \"Personal Growth\": responses[\"Q11\"] + abs(7 - responses[\"Q12\"]),\n",
        "    }\n",
        "\n",
        "    p_values = {\n",
        "        \"p1\": responses[\"Q1\"] + abs(7 - responses[\"Q2\"]),\n",
        "        \"p2\": responses[\"Q3\"] + abs(7 - responses[\"Q4\"]),\n",
        "        \"p3\": responses[\"Q5\"] + abs(7 - responses[\"Q6\"]),\n",
        "        \"p4\": responses[\"Q7\"] + abs(7 - responses[\"Q8\"]),\n",
        "        \"p5\": responses[\"Q9\"] + abs(7 - responses[\"Q10\"]),\n",
        "        \"p6\": responses[\"Q11\"] + abs(7 - responses[\"Q12\"])\n",
        "    }\n",
        "\n",
        "    # Assign p1 to p6 values\n",
        "    for key, value in p_values.items():\n",
        "        responses[key] = value\n",
        "\n",
        "    descriptions = {\n",
        "        \"Self Acceptance\": {\n",
        "            \"High\": \"Possesses a positive attitude toward the self; acknowledges and accepts multiple aspects of self, including good and bad qualities; feels positive about past life.\",\n",
        "            \"Medium\": \"Generally content with self but sometimes struggles with self-doubt; recognizes strengths but occasionally fixates on weaknesses.\",\n",
        "            \"Low\": \"Feels dissatisfied with self; is disappointed with past life; is troubled about certain personal qualities; wishes to be different.\"\n",
        "        },\n",
        "        \"Positive Relations with Others\": {\n",
        "            \"High\": \"Has warm, satisfying, trusting relationships; concerned about the welfare of others; capable of strong empathy, affection, and intimacy.\",\n",
        "            \"Medium\": \"Has meaningful relationships but sometimes struggles with trust or emotional openness; values connections but may not always nurture them deeply.\",\n",
        "            \"Low\": \"Has few close relationships; finds it difficult to be warm, open, and concerned about others; is isolated and frustrated in interpersonal relationships.\"\n",
        "        },\n",
        "        \"Autonomy\": {\n",
        "            \"High\": \"Is self-determining and independent; able to resist social pressures; regulates behavior from within and follows personal standards.\",\n",
        "            \"Medium\": \"Balances independence with societal expectations; makes personal choices but occasionally influenced by external opinions.\",\n",
        "            \"Low\": \"Is concerned about the expectations of others; relies on judgments of others; conforms to social pressures.\"\n",
        "        },\n",
        "        \"Environmental Mastery\": {\n",
        "            \"High\": \"Has a sense of mastery in managing the environment; makes effective use of opportunities; adapts surroundings to personal needs.\",\n",
        "            \"Medium\": \"Generally manages daily life well but sometimes struggles with external challenges; adapts but may not always feel in control.\",\n",
        "            \"Low\": \"Has difficulty managing everyday affairs; feels unable to change or improve surroundings; lacks sense of control over external world.\"\n",
        "        },\n",
        "        \"Purpose in Life\": {\n",
        "            \"High\": \"Has goals and a sense of directedness; finds meaning in present and past life; has aims and objectives for living.\",\n",
        "            \"Medium\": \"Seeks purpose but occasionally feels uncertain; has goals but may struggle with long-term direction or motivation.\",\n",
        "            \"Low\": \"Lacks a sense of meaning; has few goals or aims; does not see purpose in past life; has no outlook or beliefs that give life meaning.\"\n",
        "        },\n",
        "        \"Personal Growth\": {\n",
        "            \"High\": \"Feels continuous development; open to new experiences; realizes personal potential; sees self-improvement over time.\",\n",
        "            \"Medium\": \"Has a desire to grow but sometimes feels stuck; enjoys learning but may not actively seek change or self-improvement.\",\n",
        "            \"Low\": \"Feels personal stagnation; lacks a sense of improvement; feels uninterested with life; unable to develop new attitudes or behaviors.\"\n",
        "        },\n",
        "    }\n",
        "\n",
        "    st.subheader(\"Overall Scores (Max: 12 for each parameter) and Interpretation :\")\n",
        "    for category, score in scores.items():\n",
        "        score_level = score // 2  # Dividing score by 2\n",
        "\n",
        "        # Determine scorer type\n",
        "        if score_level in [1, 2]:\n",
        "            level = \"Low\"\n",
        "            color = \"red\"\n",
        "        elif score_level in [3, 4]:\n",
        "            level = \"Medium\"\n",
        "            color = \"orange\"\n",
        "        else:\n",
        "            level = \"High\"\n",
        "            color = \"green\"\n",
        "\n",
        "        # Display the result\n",
        "        st.markdown(f\"<span style='color:{color}; font-weight:bold;'>**{category}: {score} ({level} Scorer)**</span>\", unsafe_allow_html=True)\n",
        "        st.info(descriptions[category][level])  # Show description based on category and score level\n",
        "\n",
        "def update_am_csv(response_csv_path, am_csv_path):\n",
        "    # Mapping of response.csv columns to am.csv rows\n",
        "    param_mapping = {\n",
        "        \"p1\": \"self acceptance\",\n",
        "        \"p2\": \"positive relations with others\",\n",
        "        \"p3\": \"autonomy\",\n",
        "        \"p4\": \"environmental mastery\",\n",
        "        \"p5\": \"purpose in life\",\n",
        "        \"p6\": \"personal growth\"\n",
        "    }\n",
        "\n",
        "    # Load response.csv\n",
        "    response_df = pd.read_csv(response_csv_path)\n",
        "\n",
        "    # Load am.csv\n",
        "    am_df = pd.read_csv(am_csv_path, index_col=0)\n",
        "\n",
        "    # Get unique issue types and convert to lowercase\n",
        "    issue_types = response_df[\"issue\"].unique()\n",
        "    print(issue_types)\n",
        "\n",
        "    for issue in issue_types:\n",
        "        for param, am_row in param_mapping.items():\n",
        "            # Extract values where issue matches\n",
        "            param_values = response_df.loc[response_df[\"issue\"] == issue, param]\n",
        "\n",
        "            if param_values.empty:\n",
        "                continue  # Skip if no matching rows\n",
        "\n",
        "            # Divide by 2\n",
        "            divided_values = param_values // 2\n",
        "            print(issue)\n",
        "            print(divided_values)\n",
        "\n",
        "            # Compute mean value of divided values\n",
        "            mean_value = divided_values.mean()\n",
        "\n",
        "            # Ensure final_value is an integer\n",
        "            final_value = int(round(mean_value))\n",
        "\n",
        "            # Update am.csv and force integer type\n",
        "            am_df.loc[am_row, issue.lower()] = final_value\n",
        "\n",
        "            # Convert entire column to integer type\n",
        "            am_df[issue.lower()] = am_df[issue.lower()].astype(int)\n",
        "\n",
        "    # Save the updated am.csv\n",
        "    am_df.to_csv(am_csv_path)\n",
        "    st.success(\"Updated Association Matrix successfully.\")\n",
        "\n",
        "    st.subheader(\"Updated Association Matrix:\")\n",
        "    am_df = pd.read_csv(am_file_path, index_col=0)\n",
        "    # Display the dataframe as a table (no scrollbars)\n",
        "    st.table(am_df)\n",
        "\n",
        "# Using association matrix ----------------------------\n",
        "file_path = \"/content/am.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "# Define row names (assuming they are in order in the dataset)\n",
        "row_names = [\n",
        "    \"self acceptance\", \"positive relations with others\", \"autonomy\",\n",
        "    \"environmental mastery\", \"purpose in life\", \"personal growth\"\n",
        "]\n",
        "\n",
        "# Extract the probability columns (assumed order: anxiety, bipolar, depression, normal, ptsd)\n",
        "issue_columns = [\"anxiety\", \"bipolar\", \"depression\", \"normal\", \"ptsd\"]\n",
        "matrix = df[issue_columns].values  # Convert to NumPy matrix\n",
        "\n",
        "def weighted_sum_analysis(matrix,probabilities):\n",
        "    # Compute weighted sums for each row\n",
        "    weighted_sums = np.dot(matrix, probabilities)\n",
        "\n",
        "    # Find the index of the row with the highest weighted sum\n",
        "    max_index = np.argmax(weighted_sums)\n",
        "    max_row_name = row_names[max_index]\n",
        "\n",
        "    with st.expander(\"Weighted Sum Analysis\"):\n",
        "\n",
        "      # Display results in Streamlit\n",
        "      st.title(\"Weighted Sum Analysis\")\n",
        "\n",
        "      # Show computed weighted sums\n",
        "      st.subheader(\"Weighted Sums\")\n",
        "      df[\"Weighted Sum\"] = weighted_sums\n",
        "      st.dataframe(df[[\"Weighted Sum\"]])\n",
        "\n",
        "      # Display the highest weighted sum row\n",
        "      st.subheader(\"Row with Highest Weighted Sum\")\n",
        "      st.success(f\"\"\"\n",
        "      **Index:** {max_index}  \\n\n",
        "      **Row Name:** {max_row_name}  \\n\n",
        "      **Weighted Sum:** {weighted_sums[max_index]}\n",
        "      \"\"\")\n",
        "\n",
        "    return max_index, max_row_name\n",
        "\n",
        "def cosine_similarity_analysis(matrix, probabilities):\n",
        "    # Define example probabilities (vector to compare against)\n",
        "    probabilities = np.array(probabilities).reshape(1, -1)\n",
        "\n",
        "    # Compute cosine similarity for each row\n",
        "    cosine_similarities = cosine_similarity(matrix, probabilities).flatten()\n",
        "\n",
        "    # Find the index of the row with the highest cosine similarity\n",
        "    max_index = np.argmax(cosine_similarities)\n",
        "    max_row_name = row_names[max_index]\n",
        "\n",
        "    with st.expander(\"Cosine Similarity Analysis\"):\n",
        "      # Display results in Streamlit\n",
        "      st.title(\"Cosine Similarity Analysis\")\n",
        "\n",
        "      # Show computed cosine similarities\n",
        "      st.subheader(\"Cosine Similarities\")\n",
        "      df[\"Cosine Similarity\"] = cosine_similarities\n",
        "      st.dataframe(df[[\"Cosine Similarity\"]])\n",
        "\n",
        "      # Display the highest similarity row\n",
        "      st.subheader(\"Row with Highest Cosine Similarity\")\n",
        "      st.success(f\"\"\"\n",
        "      **Index:** {max_index}  \\n\n",
        "      **Row Name:** {max_row_name}  \\n\n",
        "      **Cosine Similarity Score:** {cosine_similarities[max_index]}\n",
        "      \"\"\")\n",
        "\n",
        "    return max_index, max_row_name\n",
        "\n",
        "def euclidian_distance_analysis(matrix, probabilities):\n",
        "    # Compute Euclidean distances for each row\n",
        "    euclidean_distances = np.array([euclidean(row, probabilities) for row in matrix])\n",
        "\n",
        "    # Find the index of the row with the smallest Euclidean distance\n",
        "    min_index = np.argmin(euclidean_distances)\n",
        "    min_row_name = row_names[min_index]\n",
        "\n",
        "    with st.expander(\"Euclidean Distance Analysis\"):\n",
        "\n",
        "      # Display results in Streamlit\n",
        "      st.title(\"Euclidean Distance Analysis\")\n",
        "\n",
        "      # Show computed Euclidean distances\n",
        "      st.subheader(\"Euclidean Distances\")\n",
        "      df[\"Euclidean Distance\"] = euclidean_distances\n",
        "      st.dataframe(df[[\"Euclidean Distance\"]])\n",
        "\n",
        "      # Display the closest row (smallest Euclidean distance)\n",
        "      st.subheader(\"Row with Smallest Euclidean Distance\")\n",
        "      st.success(f\"\"\"\n",
        "      **Index:** {min_index}  \\n\n",
        "      **Row Name:** {min_row_name}  \\n\n",
        "      **Euclidean Distance:** {euclidean_distances[min_index]}\n",
        "      \"\"\")\n",
        "\n",
        "    return min_index, min_row_name\n",
        "\n",
        "def get_consensus_string(weighted_sum_row_name, cosine_similarity_row_name, euclidean_distance_row_name):\n",
        "    row_names = [weighted_sum_row_name, cosine_similarity_row_name, euclidean_distance_row_name]\n",
        "    unique_names = list(set(row_names))  # Get unique row names\n",
        "\n",
        "    if len(unique_names) == 1:\n",
        "        return unique_names[0]  # All three are the same\n",
        "    elif len(unique_names) == 2:\n",
        "        return \" and \".join(unique_names)  # Two names are the same\n",
        "    else:\n",
        "        return \", \".join(unique_names)  # All three are different\n",
        "\n",
        "# Define the Streamlit app\n",
        "def run_app():\n",
        "    st.title(\"Mental Health Disorder Detection\")\n",
        "\n",
        "    option = st.sidebar.selectbox(\n",
        "       \"Choose an option\",\n",
        "       [\"Text Input\", \"Image Upload\", \"Video Upload\", \"PDF Upload\", \"Responses to Image\", \"Reddit Username Analysis\", \"Twitter Username Analysis\", \"Well-being Survey\"]\n",
        "    )\n",
        "\n",
        "    # Text Input\n",
        "    if option == \"Text Input\":\n",
        "        st.subheader(\"Enter Text to Classify Mental Health Issue\")\n",
        "        input_text = st.text_area(\"Enter your text here:\")\n",
        "\n",
        "        if st.button(\"Classify Text\"):\n",
        "            if input_text.strip() == \"\":\n",
        "                st.write(\"Please enter some text to classify.\")\n",
        "            else:\n",
        "                translated_text = GoogleTranslator(source='auto', target='en').translate(input_text)\n",
        "                st.write(\"Translated Text (to English):\")\n",
        "                st.write(translated_text)\n",
        "                classify_text(translated_text)\n",
        "        # Adding model retraining\n",
        "        elif st.button(\"Classify Text and Retrain Model\"):\n",
        "            if input_text.strip() == \"\":\n",
        "                st.write(\"Please enter some text to classify.\")\n",
        "            else:\n",
        "                translated_text = GoogleTranslator(source='auto', target='en').translate(input_text)\n",
        "                st.subheader(\"Translated Text (to English):\")\n",
        "                st.write(translated_text)\n",
        "                classify_text_retrain_model(translated_text)\n",
        "\n",
        "    # Image Upload\n",
        "    elif option == \"Image Upload\":\n",
        "        st.subheader(\"Upload an Image to Extract and Classify Text\")\n",
        "        uploaded_image = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"jpeg\", \"png\", \"webp\", \"bmp\", \"tiff\"])\n",
        "\n",
        "        if uploaded_image is not None:\n",
        "            image = Image.open(uploaded_image)\n",
        "            st.image(image, caption=\"Uploaded Image\", use_container_width=True)\n",
        "\n",
        "            # Generate and display the caption\n",
        "            caption = generate_caption(image)\n",
        "            st.success(caption)\n",
        "\n",
        "            # Step 1: Extract text from the image\n",
        "            extracted_text = extract_text_from_image(image)\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(\"\\n\".join(extracted_text))\n",
        "\n",
        "            st.subheader(\"Translated Text (to English)\")\n",
        "            st.text(translated_text)\n",
        "\n",
        "            # Step 2: Detect faces and analyze emotions\n",
        "            emotion_counts, detected_emotions = analyze_emotions_from_image(image)\n",
        "\n",
        "            if emotion_counts:\n",
        "                # Determine the dominant emotion\n",
        "                dominant_emotion = max(emotion_counts, key=emotion_counts.get)\n",
        "                st.success(f\"Dominant Emotion: **{dominant_emotion}**\")\n",
        "\n",
        "                # Step 2: Use Gemini API for mental health analysis\n",
        "                analyze_with_gemini(dominant_emotion, emotion_counts)\n",
        "\n",
        "            else:\n",
        "                st.error(\"No faces detected in the uploaded image.\")\n",
        "\n",
        "            # Step 3: Classify extracted text\n",
        "            if st.button(\"Classify Extracted Text\"):\n",
        "                if not translated_text or translated_text.strip() == \"\":\n",
        "                    st.write(\"It seems normal as there is no text in image\")\n",
        "                else:\n",
        "                    classify_text_with_desc(translated_text,caption)\n",
        "\n",
        "            # Adding model retraining option\n",
        "            elif st.button(\"Classify Extracted Text and Retrain Model\"):\n",
        "                if not translated_text or translated_text.strip() == \"\":\n",
        "                    st.write(\"It seems normal as there is no text in image\")\n",
        "                else:\n",
        "                    classify_text_retrain_model_desc(translated_text,caption)\n",
        "\n",
        "    # Video Upload\n",
        "    elif option == \"Video Upload\":\n",
        "        st.subheader(\"Upload a Video to Extract and Classify Text\")\n",
        "        # File upload widget\n",
        "        video_file = st.file_uploader(\"Choose a video file\", type=[\"mp4\", \"mov\", \"avi\"])\n",
        "\n",
        "        if video_file:\n",
        "            # Save the uploaded video file temporarily\n",
        "            video_path = \"/tmp/uploaded_video.mp4\"\n",
        "            with open(video_path, \"wb\") as f:\n",
        "                f.write(video_file.getbuffer())\n",
        "\n",
        "            st.video(video_file)  # Display the uploaded video\n",
        "\n",
        "            # Extract frames from the uploaded video\n",
        "            frames = extract_frames(video_path)\n",
        "            combined_text = \"\"\n",
        "\n",
        "            st.subheader(\"Extracting frames from video...\")\n",
        "\n",
        "            # Emotion recognition\n",
        "            emotion_counts, frame_emotions = analyze_emotions_from_frames(frames)\n",
        "\n",
        "            # Display results\n",
        "            for idx, emotion in enumerate(frame_emotions):\n",
        "                st.image(frames[idx], caption=f\"Frame {idx + 1} - Emotion: {emotion}\")\n",
        "\n",
        "            # Display summary table and most frequent emotion\n",
        "            dominant_emotion = display_emotion_summary(emotion_counts)\n",
        "            st.success(f\"Dominant Emotion: **{dominant_emotion}**\")\n",
        "\n",
        "            # Use the dominant emotion and emotion counts to craft a Gemini API prompt\n",
        "            analyze_with_gemini(dominant_emotion, emotion_counts)\n",
        "\n",
        "            for idx, frame in enumerate(frames):\n",
        "                # st.image(frame, caption=f\"Frame {idx + 1}\", use_column_width=True)\n",
        "                text_from_frame = extract_text_from_image_video(frame)\n",
        "\n",
        "                if text_from_frame and text_from_frame not in combined_text:\n",
        "                    combined_text += text_from_frame + \" \"\n",
        "\n",
        "            # Generate and display descriptions\n",
        "            frame_captions, overall_description = describe_video(frames)\n",
        "            st.subheader(\"Overall Description\")\n",
        "            st.success(overall_description)\n",
        "\n",
        "            st.subheader(\"Text Extracted from Video Frames:\")\n",
        "            st.text(combined_text)\n",
        "\n",
        "            # Translate the extracted text from frames\n",
        "            translated_frame_text = translate_text(combined_text)\n",
        "            # Extract audio and transcribe it\n",
        "            transcribed_audio_text = transcribe_audio_from_video(video_file)\n",
        "\n",
        "            st.subheader(\"Transcribed Audio Text:\")\n",
        "            st.text(transcribed_audio_text)\n",
        "            translated_audio_text = translate_text(transcribed_audio_text)\n",
        "\n",
        "            # Combine the text extracted from both images and audio\n",
        "            full_combined_text = combined_text + \" \" + transcribed_audio_text\n",
        "            st.subheader(\"Combined Extracted Text (from both video frames and audio):\")\n",
        "            st.text(full_combined_text)\n",
        "\n",
        "            translated_combined_text = translate_text(full_combined_text)\n",
        "            st.subheader(\"Translated Combined Text (Frames + Audio):\")\n",
        "            st.text(translated_combined_text)\n",
        "\n",
        "            # Analyze audio mood\n",
        "            st.subheader(\"Analyzing Audio Mood...\")\n",
        "            analyze_audio_mood(video_path)\n",
        "            # st.write(mood_result)\n",
        "            cleaned_text = re.sub(r\"[^a-zA-Z0-9.,!? ]\", \"\", translated_combined_text)\n",
        "\n",
        "            if st.button(\"Classify Extracted Text\"):\n",
        "                if not cleaned_text or cleaned_text.strip() == \"\":\n",
        "                    # If audio_text is empty or contains only whitespace\n",
        "                    st.write(\"It is normal with probability 100%\")\n",
        "                else:\n",
        "                    classify_text_with_desc(cleaned_text,overall_description)\n",
        "            # Adding model retraining\n",
        "            elif st.button(\"Classify Extracted Text and Retrain Model\"):\n",
        "                if not cleaned_text or cleaned_text.strip() == \"\":\n",
        "                    # If audio_text is empty or contains only whitespace\n",
        "                    st.write(\"It is normal with probability 100%\")\n",
        "                else:\n",
        "                    classify_text_retrain_model_desc(cleaned_text,overall_description)\n",
        "\n",
        "    # PDF Upload\n",
        "    elif option == \"PDF Upload\":\n",
        "        st.subheader(\"Upload a PDF to Extract and Classify Text\")\n",
        "\n",
        "        # File uploader\n",
        "        uploaded_file = st.file_uploader(\"Upload a PDF\", type=\"pdf\")\n",
        "\n",
        "        input_text = \"\"\n",
        "        if uploaded_file:\n",
        "\n",
        "            # Convert PDF to images\n",
        "            images = convert_from_bytes(uploaded_file.read())\n",
        "\n",
        "            extracted_text = \"\"\n",
        "\n",
        "            # Display pages in an expander\n",
        "            with st.expander(\"Extracted Pages\", expanded=False):\n",
        "                for i, img in enumerate(images):\n",
        "                    # Display each page as an image inside the expander\n",
        "                    st.image(img, caption=f\"Page {i+1}\", use_container_width=True)\n",
        "\n",
        "            # Extract text from each image\n",
        "            for i, img in enumerate(images):\n",
        "                text = pytesseract.image_to_string(img)\n",
        "                extracted_text += text + \"\\n\\n\"\n",
        "\n",
        "            if not extracted_text:\n",
        "                st.warning(\"No text was extracted from the PDF.\")\n",
        "            else:\n",
        "                # Display extracted text\n",
        "                st.text_area(\"Detected Text\", extracted_text, height=300)\n",
        "\n",
        "                translated_text = GoogleTranslator(source='auto', target='en').translate(extracted_text)\n",
        "                st.subheader(\"Translated Text (English):\")\n",
        "                st.text_area(\"Translated Text\", translated_text, height=300)\n",
        "                input_text = translated_text\n",
        "\n",
        "        if st.button(\"Classify Text\"):\n",
        "            if input_text.strip() == \"\":\n",
        "                st.write(\"No text found in the PDF\")\n",
        "            else:\n",
        "                translated_text = GoogleTranslator(source='auto', target='en').translate(input_text)\n",
        "                classify_text(translated_text)\n",
        "        # Adding model retraining\n",
        "        elif st.button(\"Classify Text and Retrain Model\"):\n",
        "            if input_text.strip() == \"\":\n",
        "                st.write(\"No text found in the PDF\")\n",
        "            else:\n",
        "                translated_text = GoogleTranslator(source='auto', target='en').translate(input_text)\n",
        "                classify_text_retrain_model(translated_text)\n",
        "\n",
        "\n",
        "    # User respond to Image\n",
        "    if option == 'Responses to Image':\n",
        "        st.subheader(\"Describe Image and Classify Responses\")\n",
        "\n",
        "        # Define Rorschach test questions\n",
        "        questions = [\n",
        "            \"What do you see in this image?\",\n",
        "            \"What emotions does this image evoke in you?\",\n",
        "            \"Does this image remind you of anything from your past?\",\n",
        "            \"If this image had a story, what would it be?\",\n",
        "            \"Do you see anything changing in the image over time?\"\n",
        "        ]\n",
        "\n",
        "        # Display the selected image\n",
        "        rand_num = 10\n",
        "        image_path, randnum = get_random_image()\n",
        "\n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(filename):\n",
        "            # Create the file and write the number\n",
        "            with open(filename, \"w\") as file:\n",
        "                file.write(str(randnum))\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        if randnum != 10:\n",
        "          rand_num = randnum\n",
        "\n",
        "        if os.path.exists(image_path):\n",
        "            st.image(image_path, caption=f\"Look at the image and answer the questions.\", use_container_width=True)\n",
        "        else:\n",
        "            st.error(f\"Image not found in {IMAGE_DIR}.\")\n",
        "\n",
        "        # Collect responses\n",
        "        responses = {}\n",
        "        st.subheader(\"Answer the following questions:\")\n",
        "        for i, question in enumerate(questions):\n",
        "            responses[f\"Q{i+1}\"] = st.text_area(question, key=f\"q{i+1}\")\n",
        "\n",
        "        # Submit button\n",
        "        combined_response = \"\"\n",
        "        if st.button(\"Classify Responses\"):\n",
        "            st.success(\"Responses submitted successfully!\")\n",
        "            st.write(\"Here are your responses:\")\n",
        "\n",
        "            with st.expander(\"Your Responses\", expanded=False):\n",
        "              i=0\n",
        "              for q, ans in responses.items():\n",
        "                combined_response += f\"{ans}\\n\"\n",
        "                st.write(f\"**{q}** ANS : {ans}\")\n",
        "                i+=1\n",
        "                # st.write(f\"\\n\\n {combined_response}\")\n",
        "\n",
        "            image_description = describe_image(image_path)\n",
        "            st.info(image_description)\n",
        "            st.image(image_path, use_container_width=True)\n",
        "            # response_from_gemini = get_image_description(RANDOM_IMAGE_PATH, combined_response)\n",
        "\n",
        "            TEST = \"\"\n",
        "            stored_number = 10\n",
        "\n",
        "            with open(filename, \"r\") as file:\n",
        "                stored_number = int(file.read())\n",
        "\n",
        "            if stored_number == 0:\n",
        "                TEST = \"anxiety\"\n",
        "            elif stored_number == 1:\n",
        "                TEST = \"anxiety\"\n",
        "            elif stored_number == 2:\n",
        "                TEST = \"bipolar\"\n",
        "            elif stored_number == 3:\n",
        "                TEST = \"bipolar\"\n",
        "            elif stored_number == 4:\n",
        "                TEST = \"depression\"\n",
        "            elif stored_number == 5:\n",
        "                TEST = \"depression\"\n",
        "            elif stored_number == 6:\n",
        "                TEST = \"normal\"\n",
        "            elif stored_number == 7:\n",
        "                TEST = \"normal\"\n",
        "            elif stored_number == 8:\n",
        "                TEST = \"ptsd\"\n",
        "            else:\n",
        "                TEST = \"ptsd\"\n",
        "\n",
        "            st.warning(f\"The image was used to test: **{TEST}**\")\n",
        "\n",
        "            # Clean up the temporary files at the end of execution\n",
        "            if os.path.exists(RANDOM_IMAGE_PATH) and os.path.exists(\"./number.txt\"):\n",
        "                os.remove(RANDOM_IMAGE_PATH)\n",
        "                os.remove(\"./number.txt\")\n",
        "\n",
        "            if combined_response.strip() == \"\":\n",
        "                st.write(\"Please enter your responses.\")\n",
        "            else:\n",
        "                translated_text = GoogleTranslator(source='auto', target='en').translate(combined_response)\n",
        "                st.write(\"Combined Responses (Translated if not in English):\")\n",
        "                st.info(translated_text)\n",
        "                st.header(\"Based on the responses :\")\n",
        "                classify_text(translated_text)\n",
        "\n",
        "        # Adding model retraining\n",
        "        elif st.button(\"Classify Responses and Retrain Model\"):\n",
        "            st.success(\"Responses submitted successfully!\")\n",
        "            st.write(\"Here are your responses:\")\n",
        "\n",
        "            with st.expander(\"Your Responses\", expanded=False):\n",
        "              i=0\n",
        "              for q, ans in responses.items():\n",
        "                combined_response += f\"{ans}\\n\"\n",
        "                st.write(f\"**{q}** ANS : {ans}\")\n",
        "                i+=1\n",
        "                # st.write(f\"\\n\\n {combined_response}\")\n",
        "\n",
        "            image_description = describe_image(image_path)\n",
        "            st.info(image_description)\n",
        "            st.image(image_path, use_container_width=True)\n",
        "            # response_from_gemini = get_image_description(RANDOM_IMAGE_PATH, combined_response)\n",
        "\n",
        "            TEST = \"\"\n",
        "            stored_number = 10\n",
        "\n",
        "            with open(filename, \"r\") as file:\n",
        "                stored_number = int(file.read())\n",
        "\n",
        "            if stored_number == 0:\n",
        "                TEST = \"anxiety\"\n",
        "            elif stored_number == 1:\n",
        "                TEST = \"anxiety\"\n",
        "            elif stored_number == 2:\n",
        "                TEST = \"bipolar\"\n",
        "            elif stored_number == 3:\n",
        "                TEST = \"bipolar\"\n",
        "            elif stored_number == 4:\n",
        "                TEST = \"depression\"\n",
        "            elif stored_number == 5:\n",
        "                TEST = \"depression\"\n",
        "            elif stored_number == 6:\n",
        "                TEST = \"normal\"\n",
        "            elif stored_number == 7:\n",
        "                TEST = \"normal\"\n",
        "            elif stored_number == 8:\n",
        "                TEST = \"ptsd\"\n",
        "            else:\n",
        "                TEST = \"ptsd\"\n",
        "\n",
        "            st.warning(f\"The image was used to test: **{TEST}**\")\n",
        "\n",
        "            # Clean up the temporary files at the end of execution\n",
        "            if os.path.exists(RANDOM_IMAGE_PATH) and os.path.exists(\"./number.txt\"):\n",
        "                os.remove(RANDOM_IMAGE_PATH)\n",
        "                os.remove(\"./number.txt\")\n",
        "\n",
        "            if combined_response.strip() == \"\":\n",
        "                st.write(\"Please enter your responses.\")\n",
        "            else:\n",
        "                translated_text = GoogleTranslator(source='auto', target='en').translate(combined_response)\n",
        "                st.subheader(\"Combined Responses ( Translated if not in English):\")\n",
        "                st.info(translated_text)\n",
        "                st.header(\"Based on the responses :\")\n",
        "                classify_text_retrain_model(translated_text)\n",
        "\n",
        "\n",
        "    # Reddit Username Analysis\n",
        "    elif option == \"Reddit Username Analysis\":\n",
        "        st.subheader(\"Enter Reddit Username for Analysis\")\n",
        "        username = st.text_input(\"Enter Reddit username:\")\n",
        "\n",
        "        if st.button(\"Analyze\"):\n",
        "            if username.strip() == \"\":\n",
        "                st.write(\"Please enter a Reddit username.\")\n",
        "            else:\n",
        "                # Fetch and display text posts\n",
        "                text_posts = fetch_user_text_posts(username)\n",
        "                if text_posts:\n",
        "                    st.write(\"Recent Text Posts:\")\n",
        "                    st.write(text_posts)  # Display a few posts for review\n",
        "\n",
        "                # Fetch and display image-based posts with extracted text\n",
        "                image_texts, image_caption = fetch_user_images_and_extract_text(username)\n",
        "\n",
        "                # for videos\n",
        "                st.header(\"Latest Videos from posts:\")\n",
        "                posts_with_videos = get_user_posts_with_videos(username, max_items=10)\n",
        "                combined_video_text = \"\"\n",
        "                if posts_with_videos:\n",
        "                    for i, post in enumerate(posts_with_videos, start=1):\n",
        "                        # Check if video and/or audio are available and process accordingly\n",
        "                        for vid_data in post[\"videos\"]:\n",
        "                            # Download Video\n",
        "                            video_path = f\"video_{i}.mp4\"\n",
        "                            downloaded_video_path = download_video(vid_data['video_url'], video_path)\n",
        "\n",
        "                            # Download Audio\n",
        "                            audio_path = f\"audio_{i}.mp4\"\n",
        "                            downloaded_audio_path = download_audio(vid_data['audio_url'], audio_path)\n",
        "\n",
        "                            # If both video and audio are available, combine them\n",
        "                            if downloaded_video_path and downloaded_audio_path:\n",
        "                                combined_video_path = f\"combined_video_{i}.mp4\"\n",
        "                                final_video = combine_video_audio(downloaded_video_path, downloaded_audio_path, combined_video_path)\n",
        "\n",
        "                                if final_video:\n",
        "                                    st.video(final_video)\n",
        "                                    combined_video_text += process_video(final_video) + \" \"\n",
        "                                    os.remove(final_video)  # Clean up after displaying\n",
        "                                    os.remove(downloaded_video_path)  # Clean up after displaying\n",
        "                                    os.remove(downloaded_audio_path)  # Clean up after displaying\n",
        "                                else:\n",
        "                                    # st.warning(\"Could not combine video and audio.\")\n",
        "                                    if downloaded_video_path:\n",
        "                                        # If only the video is available, display the video directly\n",
        "                                        st.video(downloaded_video_path)\n",
        "                                        combined_video_text += process_video(downloaded_video_path) + \" \"\n",
        "                                        os.remove(downloaded_video_path)  # Clean up after displaying\n",
        "                                    elif downloaded_audio_path:\n",
        "                                        # If only the audio is available, display the audio directly\n",
        "                                        st.audio(downloaded_audio_path)\n",
        "                                        os.remove(downloaded_audio_path)  # Clean up after displaying\n",
        "                            else:\n",
        "                                st.warning(\"No video or audio found.\")\n",
        "\n",
        "                            for file in os.listdir():\n",
        "                                if file.endswith(\".mp4\"):\n",
        "                                    os.remove(file)\n",
        "                else:\n",
        "                    st.warning(\"No videos found in this user's posts!\")\n",
        "\n",
        "                # Combine text from both text posts and image text\n",
        "                all_text = text_posts + image_texts\n",
        "                if all_text:\n",
        "                    predictions = []\n",
        "\n",
        "                    for text in all_text:\n",
        "                        # Preprocess the input for each base model\n",
        "                        lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "                        svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "                        nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "                        xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "                        lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "                        transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "                        # Pad sequences for LSTM\n",
        "                        lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "                        # Get probabilities from all base models\n",
        "                        lr_proba = lr_model.predict_proba(lr_features)\n",
        "                        svm_proba = svm_model.predict_proba(svm_features)\n",
        "                        nb_proba = nb_model.predict_proba(nb_features)\n",
        "                        xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "                        lstm_proba = lstm_model.predict(lstm_features)\n",
        "                        transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "                        # Combine probabilities as input for the meta-learner\n",
        "                        stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "                        # Predict using the meta-learner\n",
        "                        final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "                        final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "                        decoded_prediction = label_encoder.inverse_transform(final_prediction)[0]\n",
        "\n",
        "                        # Append the prediction\n",
        "                        predictions.append(decoded_prediction)\n",
        "\n",
        "                    # Count the most common mental health issue\n",
        "                    issue_counts = Counter(predictions)\n",
        "                    combined_all_text = \" \".join(all_text)+\" \"+combined_video_text+\" Image captions are as follows : \"+image_caption\n",
        "                    # Display results\n",
        "                    issue_distribution = pd.DataFrame(issue_counts.items(), columns=['Mental Health Issue', 'Count'])\n",
        "                    st.write(\"Mental health issue distribution across posts:\")\n",
        "                    st.write(issue_distribution)\n",
        "                    # Add a bar chart\n",
        "                    st.bar_chart(issue_distribution.set_index('Mental Health Issue')['Count'])\n",
        "                    top_issue = classify_alltext(combined_all_text)\n",
        "\n",
        "                else:\n",
        "                    st.write(\"No valid text found for analysis.\")\n",
        "        # Adding Model Retraining\n",
        "        elif st.button(\"Analyze and Retrain Model\"):\n",
        "            if username.strip() == \"\":\n",
        "                st.write(\"Please enter a Reddit username.\")\n",
        "            else:\n",
        "                # Fetch and display text posts\n",
        "                text_posts = fetch_user_text_posts(username)\n",
        "                if text_posts:\n",
        "                    st.write(\"Recent Text Posts:\")\n",
        "                    st.write(text_posts)  # Display a few posts for review\n",
        "\n",
        "                # Fetch and display image-based posts with extracted text\n",
        "                image_texts, image_caption = fetch_user_images_and_extract_text(username)\n",
        "\n",
        "                # for videos\n",
        "                st.header(\"Latest Videos from posts:\")\n",
        "                posts_with_videos = get_user_posts_with_videos(username, max_items=10)\n",
        "                combined_video_text = \"\"\n",
        "                if posts_with_videos:\n",
        "                    for i, post in enumerate(posts_with_videos, start=1):\n",
        "                        # Check if video and/or audio are available and process accordingly\n",
        "                        for vid_data in post[\"videos\"]:\n",
        "                            # Download Video\n",
        "                            video_path = f\"video_{i}.mp4\"\n",
        "                            downloaded_video_path = download_video(vid_data['video_url'], video_path)\n",
        "\n",
        "                            # Download Audio\n",
        "                            audio_path = f\"audio_{i}.mp4\"\n",
        "                            downloaded_audio_path = download_audio(vid_data['audio_url'], audio_path)\n",
        "\n",
        "                            # If both video and audio are available, combine them\n",
        "                            if downloaded_video_path and downloaded_audio_path:\n",
        "                                combined_video_path = f\"combined_video_{i}.mp4\"\n",
        "                                final_video = combine_video_audio(downloaded_video_path, downloaded_audio_path, combined_video_path)\n",
        "\n",
        "                                if final_video:\n",
        "                                    st.video(final_video)\n",
        "                                    combined_video_text += process_video(final_video) + \" \"\n",
        "                                    os.remove(final_video)  # Clean up after displaying\n",
        "                                    os.remove(downloaded_video_path)  # Clean up after displaying\n",
        "                                    os.remove(downloaded_audio_path)  # Clean up after displaying\n",
        "                                else:\n",
        "                                    # st.warning(\"Could not combine video and audio.\")\n",
        "                                    if downloaded_video_path:\n",
        "                                        # If only the video is available, display the video directly\n",
        "                                        st.video(downloaded_video_path)\n",
        "                                        combined_video_text += process_video(downloaded_video_path) + \" \"\n",
        "                                        os.remove(downloaded_video_path)  # Clean up after displaying\n",
        "                                    elif downloaded_audio_path:\n",
        "                                        # If only the audio is available, display the audio directly\n",
        "                                        st.audio(downloaded_audio_path)\n",
        "                                        os.remove(downloaded_audio_path)  # Clean up after displaying\n",
        "                            else:\n",
        "                                st.warning(\"No video or audio found.\")\n",
        "\n",
        "                            for file in os.listdir():\n",
        "                                if file.endswith(\".mp4\"):\n",
        "                                    os.remove(file)\n",
        "                else:\n",
        "                    st.warning(\"No videos found in this user's posts!\")\n",
        "\n",
        "                # Combine text from both text posts and image text\n",
        "                all_text = text_posts + image_texts\n",
        "                if all_text:\n",
        "                    predictions = []\n",
        "\n",
        "                    for text in all_text:\n",
        "                        # Preprocess the input for each base model\n",
        "                        lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "                        svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "                        nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "                        xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "                        lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "                        transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "                        # Pad sequences for LSTM\n",
        "                        lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "                        # Get probabilities from all base models\n",
        "                        lr_proba = lr_model.predict_proba(lr_features)\n",
        "                        svm_proba = svm_model.predict_proba(svm_features)\n",
        "                        nb_proba = nb_model.predict_proba(nb_features)\n",
        "                        xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "                        lstm_proba = lstm_model.predict(lstm_features)\n",
        "                        transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "                        # Combine probabilities as input for the meta-learner\n",
        "                        stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "                        # Predict using the meta-learner\n",
        "                        final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "                        final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "                        decoded_prediction = label_encoder.inverse_transform(final_prediction)[0]\n",
        "                        # Append the prediction\n",
        "                        predictions.append(decoded_prediction)\n",
        "\n",
        "                    # Count the most common mental health issue\n",
        "                    issue_counts = Counter(predictions)\n",
        "                    combined_all_text = \" \".join(all_text)+\" \"+combined_video_text+\" Image captions are as follows : \"+image_caption\n",
        "                    # Display results\n",
        "                    issue_distribution = pd.DataFrame(issue_counts.items(), columns=['Mental Health Issue', 'Count'])\n",
        "                    st.write(\"Mental health issue distribution across posts:\")\n",
        "                    st.write(issue_distribution)\n",
        "                    # Add a bar chart\n",
        "                    st.bar_chart(issue_distribution.set_index('Mental Health Issue')['Count'])\n",
        "                    top_issue = classify_alltext(combined_all_text)\n",
        "\n",
        "                    # Adding Model Retraining Functionality\n",
        "                    update_and_retrain(\" \".join(all_text), top_issue)\n",
        "\n",
        "                else:\n",
        "                    st.write(\"No valid text found for analysis.\")\n",
        "\n",
        "    # Twitter Username Analysis\n",
        "    elif option == \"Twitter Username Analysis\":\n",
        "        st.subheader(\"Enter Twitter Username for Analysis\")\n",
        "        username = st.text_input(\"Enter Twitter username:\")\n",
        "\n",
        "        if st.button(\"Analyze\"):\n",
        "            if username.strip() == \"\":\n",
        "                st.write(\"Please enter a Twitter username.\")\n",
        "            else:\n",
        "                # Fetch the latest tweets with associated images\n",
        "                # tweets_with_images = get_latest_tweets_with_images(username)\n",
        "                tweets_with_videos =  get_latest_tweets_with_videos(username)\n",
        "\n",
        "                # Extract text content from tweets\n",
        "                text_posts = [tweet['text'] for tweet in tweets_with_videos if tweet['text']]\n",
        "                st.write(\"Recent Text Posts from Tweets:\")\n",
        "                st.write(text_posts)  # Display a few posts for review\n",
        "\n",
        "                video_text = \"\"\n",
        "                st.header(\"Latest Videos from tweets:\")\n",
        "                if tweets_with_videos:\n",
        "                    for i, tweet in enumerate(tweets_with_videos, start=1):\n",
        "                        # Display videos\n",
        "                        for vid_url in tweet[\"videos\"]:\n",
        "                            video_path = f\"video_{i}.mp4\"\n",
        "                            downloaded_path = download_video(vid_url, video_path)\n",
        "                            if downloaded_path:\n",
        "                                st.video(downloaded_path)\n",
        "                                video_text += process_video(downloaded_path) + \" \"\n",
        "                                os.remove(downloaded_path)  # Clean up after displaying\n",
        "                            else:\n",
        "                                st.warning(\"Could not download or display video.\")\n",
        "                else:\n",
        "                    st.warning(\"No videos found!\")\n",
        "\n",
        "                # Extract and process text from associated images\n",
        "                image_texts = []\n",
        "                all_emotions = {'happy': 0, 'sad': 0, 'angry': 0, 'disgust': 0, 'fear': 0, 'surprise': 0, 'neutral': 0}\n",
        "                combined_caption = \"\"\n",
        "                for tweet in tweets_with_videos:\n",
        "                    for image_url in tweet['images']:\n",
        "                        image = fetch_image_content(image_url)\n",
        "                        if image:\n",
        "                            st.image(image, caption=f\"Image from Tweet\", use_container_width=True)\n",
        "                            # Generate and display the caption\n",
        "                            caption = generate_caption(image)\n",
        "                            st.success(caption)\n",
        "                            combined_caption += caption + \" \"\n",
        "                        if image:\n",
        "                            extracted_text = extract_text_from_image(image)  # Assuming a text extraction function is defined\n",
        "                            if extracted_text:\n",
        "                                image_texts.append(extracted_text)\n",
        "                            # Analyze facial emotions in the image\n",
        "                            dominant_emotion = detect_emotions_from_image(image)\n",
        "                            if dominant_emotion:\n",
        "                                st.success(f\"Dominant Emotion Detected: {dominant_emotion}\")\n",
        "                                all_emotions[dominant_emotion] += 1\n",
        "                            else:\n",
        "                                st.error(\"No faces detected or error in emotion analysis.\")\n",
        "\n",
        "                # After processing all images, analyze the emotion counts and provide a suggestion\n",
        "                if all_emotions:\n",
        "                    # Convert `all_emotions` to a DataFrame\n",
        "                    emotion_df = pd.DataFrame(list(all_emotions.items()), columns=['Emotion', 'Count'])\n",
        "\n",
        "                    # Create and display a bar chart for all emotions\n",
        "                    fig = px.bar(emotion_df, x='Emotion', y='Count',\n",
        "                                color='Emotion',\n",
        "                                title=\"Aggregated Emotion Counts Across All Images\",\n",
        "                                labels={'Emotion': 'Detected Emotions', 'Count': 'Frequency'},\n",
        "                                text='Count')  # Display count on bars for better clarity\n",
        "\n",
        "                    # Show the graph in Streamlit\n",
        "                    st.plotly_chart(fig)\n",
        "\n",
        "                    dominant_emotion = max(all_emotions, key=all_emotions.get)\n",
        "                    st.success(f\"Most Frequent Emotion Across All Images or no Images(Default): {dominant_emotion}\")\n",
        "                    emotion_summary = \", \".join([f\"{emotion}: {count}\" for emotion, count in all_emotions.items()])\n",
        "                    analyze_with_gemini(dominant_emotion, all_emotions)\n",
        "                else:\n",
        "                    st.error(\"No images processed or error in emotion analysis.\")\n",
        "\n",
        "                # Combine text from both tweet text and extracted image text\n",
        "                all_text = text_posts + image_texts\n",
        "\n",
        "                # Ensure all entries in all_text are strings\n",
        "                all_text = [str(text) for text in all_text if text]\n",
        "\n",
        "                if all_text:\n",
        "                    predictions = []\n",
        "\n",
        "                    for text in all_text:\n",
        "                        try:\n",
        "                            # Preprocess the input for each base model\n",
        "                            lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "                            svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "                            nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "                            xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "                            lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "                            transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "                            # Pad sequences for LSTM\n",
        "                            lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "                            # Get probabilities from all base models\n",
        "                            lr_proba = lr_model.predict_proba(lr_features)\n",
        "                            svm_proba = svm_model.predict_proba(svm_features)\n",
        "                            nb_proba = nb_model.predict_proba(nb_features)\n",
        "                            xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "                            lstm_proba = lstm_model.predict(lstm_features)\n",
        "                            transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "                            # Combine probabilities as input for the meta-learner\n",
        "                            stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "                            # Predict using the meta-learner\n",
        "                            final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "                            final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "                            decoded_prediction = label_encoder.inverse_transform(final_prediction)[0]\n",
        "\n",
        "                            # Append the prediction\n",
        "                            predictions.append(decoded_prediction)\n",
        "\n",
        "                        except Exception as e:\n",
        "                            st.write(f\"Error processing text: {text[:50]}... - {e}\")\n",
        "                            continue\n",
        "\n",
        "                    # Count the most common mental health issue\n",
        "                    issue_counts = Counter(predictions)\n",
        "                    combined_all_text = \" \".join(all_text)+\" \"+video_text+\" Image captions are as follows : \"+combined_caption\n",
        "                    # Display results\n",
        "                    issue_distribution = pd.DataFrame(issue_counts.items(), columns=['Mental Health Issue', 'Count'])\n",
        "                    st.write(\"Mental health issue distribution across posts:\")\n",
        "                    st.write(issue_distribution)\n",
        "                    # Add a bar chart\n",
        "                    st.bar_chart(issue_distribution.set_index('Mental Health Issue')['Count'])\n",
        "                    top_issue = classify_alltext(combined_all_text)\n",
        "\n",
        "                else:\n",
        "                    st.write(\"No valid text found for analysis.\")\n",
        "\n",
        "        # Adding Retrain Model\n",
        "        elif st.button(\"Analyze and Retrain Model\"):\n",
        "            if username.strip() == \"\":\n",
        "                st.write(\"Please enter a Twitter username.\")\n",
        "            else:\n",
        "                # Fetch the latest tweets with associated images\n",
        "                # tweets_with_images = get_latest_tweets_with_images(username)\n",
        "                tweets_with_videos =  get_latest_tweets_with_videos(username)\n",
        "\n",
        "                # Extract text content from tweets\n",
        "                text_posts = [tweet['text'] for tweet in tweets_with_videos if tweet['text']]\n",
        "                st.write(\"Recent Text Posts from Tweets:\")\n",
        "                st.write(text_posts)  # Display a few posts for review\n",
        "\n",
        "                video_text = \"\"\n",
        "                st.header(\"Latest Videos from tweets:\")\n",
        "                if tweets_with_videos:\n",
        "                    for i, tweet in enumerate(tweets_with_videos, start=1):\n",
        "                        # Display videos\n",
        "                        for vid_url in tweet[\"videos\"]:\n",
        "                            video_path = f\"video_{i}.mp4\"\n",
        "                            downloaded_path = download_video(vid_url, video_path)\n",
        "                            if downloaded_path:\n",
        "                                st.video(downloaded_path)\n",
        "                                video_text += process_video(downloaded_path) + \" \"\n",
        "                                os.remove(downloaded_path)  # Clean up after displaying\n",
        "                            else:\n",
        "                                st.warning(\"Could not download or display video.\")\n",
        "                else:\n",
        "                    st.warning(\"No videos found!\")\n",
        "\n",
        "                # Extract and process text from associated images\n",
        "                image_texts = []\n",
        "                all_emotions = {'happy': 0, 'sad': 0, 'angry': 0, 'disgust': 0, 'fear': 0, 'surprise': 0, 'neutral': 0}\n",
        "                combined_caption = \"\"\n",
        "                for tweet in tweets_with_videos:\n",
        "                    for image_url in tweet['images']:\n",
        "                        image = fetch_image_content(image_url)\n",
        "                        if image:\n",
        "                            st.image(image, caption=f\"Image from Tweet\", use_container_width=True)\n",
        "                            # Generate and display the caption\n",
        "                            caption = generate_caption(image)\n",
        "                            st.success(caption)\n",
        "                            combined_caption += caption + \" \"\n",
        "                        if image:\n",
        "                            extracted_text = extract_text_from_image(image)  # Assuming a text extraction function is defined\n",
        "                            if extracted_text:\n",
        "                                image_texts.append(extracted_text)\n",
        "                            # Analyze facial emotions in the image\n",
        "                            dominant_emotion = detect_emotions_from_image(image)\n",
        "                            if dominant_emotion:\n",
        "                                st.success(f\"Dominant Emotion Detected: {dominant_emotion}\")\n",
        "                                all_emotions[dominant_emotion] += 1\n",
        "                            else:\n",
        "                                st.error(\"No faces detected or error in emotion analysis.\")\n",
        "\n",
        "                # After processing all images, analyze the emotion counts and provide a suggestion\n",
        "                if all_emotions:\n",
        "                    # Convert `all_emotions` to a DataFrame\n",
        "                    emotion_df = pd.DataFrame(list(all_emotions.items()), columns=['Emotion', 'Count'])\n",
        "\n",
        "                    # Create and display a bar chart for all emotions\n",
        "                    fig = px.bar(emotion_df, x='Emotion', y='Count',\n",
        "                                color='Emotion',\n",
        "                                title=\"Aggregated Emotion Counts Across All Images\",\n",
        "                                labels={'Emotion': 'Detected Emotions', 'Count': 'Frequency'},\n",
        "                                text='Count')  # Display count on bars for better clarity\n",
        "\n",
        "                    # Show the graph in Streamlit\n",
        "                    st.plotly_chart(fig)\n",
        "\n",
        "                    dominant_emotion = max(all_emotions, key=all_emotions.get)\n",
        "                    st.success(f\"Most Frequent Emotion Across All Images or no Images(Default): {dominant_emotion}\")\n",
        "                    emotion_summary = \", \".join([f\"{emotion}: {count}\" for emotion, count in all_emotions.items()])\n",
        "                    analyze_with_gemini(dominant_emotion, all_emotions)\n",
        "                else:\n",
        "                    st.error(\"No images processed or error in emotion analysis.\")\n",
        "\n",
        "                # Combine text from both tweet text and extracted image text\n",
        "                all_text = text_posts + image_texts\n",
        "\n",
        "                # Ensure all entries in all_text are strings\n",
        "                all_text = [str(text) for text in all_text if text]\n",
        "\n",
        "                if all_text:\n",
        "                    predictions = []\n",
        "\n",
        "                    for text in all_text:\n",
        "                        try:\n",
        "                            # Preprocess the input for each base model\n",
        "                            lr_features = lr_vectorizer.transform([text])  # For Logistic Regression\n",
        "                            svm_features = svm_vectorizer.transform([text])  # For SVM\n",
        "                            nb_features = nb_vectorizer.transform([text])  # For Naive Bayes\n",
        "                            xgb_features = tfidf_vectorizer.transform([text])  # For XGBoost\n",
        "                            lstm_features = lstm_tokenizer.texts_to_sequences([text])  # For LSTM\n",
        "                            transformer_features = t_vectorizer([text])  # For Transformer\n",
        "\n",
        "                            # Pad sequences for LSTM\n",
        "                            lstm_features = pad_sequences(lstm_features, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "                            # Get probabilities from all base models\n",
        "                            lr_proba = lr_model.predict_proba(lr_features)\n",
        "                            svm_proba = svm_model.predict_proba(svm_features)\n",
        "                            nb_proba = nb_model.predict_proba(nb_features)\n",
        "                            xgb_proba = xgb_model.predict_proba(xgb_features)\n",
        "                            lstm_proba = lstm_model.predict(lstm_features)\n",
        "                            transformer_proba = transformer_model.predict(transformer_features)\n",
        "\n",
        "                            # Combine probabilities as input for the meta-learner\n",
        "                            stacked_features = np.hstack((lr_proba, svm_proba, nb_proba, xgb_proba, lstm_proba, transformer_proba))\n",
        "\n",
        "                            # Predict using the meta-learner\n",
        "                            final_prediction_proba = meta_learner_rf.predict_proba(stacked_features)\n",
        "                            final_prediction = meta_learner_rf.predict(stacked_features)\n",
        "                            decoded_prediction = label_encoder.inverse_transform(final_prediction)[0]\n",
        "\n",
        "                            # Append the prediction\n",
        "                            predictions.append(decoded_prediction)\n",
        "\n",
        "                        except Exception as e:\n",
        "                            st.write(f\"Error processing text: {text[:50]}... - {e}\")\n",
        "                            continue\n",
        "\n",
        "                    # Count the most common mental health issue\n",
        "                    issue_counts = Counter(predictions)\n",
        "                    combined_all_text = \" \".join(all_text)+\" \"+video_text+\" Image captions are as follows : \"+combined_caption\n",
        "                    # Display results\n",
        "                    issue_distribution = pd.DataFrame(issue_counts.items(), columns=['Mental Health Issue', 'Count'])\n",
        "                    st.write(\"Mental health issue distribution across posts:\")\n",
        "                    st.write(issue_distribution)\n",
        "                    # Add a bar chart\n",
        "                    st.bar_chart(issue_distribution.set_index('Mental Health Issue')['Count'])\n",
        "                    top_issue = classify_alltext(combined_all_text)\n",
        "\n",
        "                    # Adding Model Retraining Functionality\n",
        "                    update_and_retrain(\" \".join(all_text), top_issue)\n",
        "\n",
        "                else:\n",
        "                    st.write(\"No valid text found for analysis.\")\n",
        "\n",
        "    # Well-being Survey\n",
        "    elif option == \"Well-being Survey\":\n",
        "        st.subheader(\"Well-being Survey\")\n",
        "\n",
        "        # Display the image\n",
        "        if os.path.exists(image_path_survey):\n",
        "            st.image(image_path_survey, use_container_width=True)\n",
        "        else:\n",
        "            st.warning(\"Intro image not found!\")\n",
        "\n",
        "        st.info(\"If you are not sure, predict your probable mental issue using any one of the 6 options available on the left before filling.\")\n",
        "\n",
        "        st.warning(\n",
        "            \"There a total of 12 questions : 2 for each of the 6 paramters from Ryff's Scale of Psychological Wellbeing. The Overall Scores are displayed at the end along with the updated Association Matrix.  \\n\\n\"\n",
        "            \"Questions with (R) are reversed scored. \\n\\n\"\n",
        "            \"1 → Strongly Disagree  \\n\"\n",
        "            \"2 → Disagree  \\n\"\n",
        "            \"3 → Slightly Disagree  \\n\"\n",
        "            \"4 → Slightly Agree  \\n\"\n",
        "            \"5 → Agree  \\n\"\n",
        "            \"6 → Strongly Agree\"\n",
        "        )\n",
        "\n",
        "        # Load existing data or create a new CSV file if it doesn't exist\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file_path)\n",
        "        except FileNotFoundError:\n",
        "            df = pd.DataFrame(columns=[\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\", \"Q6\", \"Q7\", \"Q8\", \"Q9\", \"Q10\", \"Q11\", \"Q12\",\n",
        "                                      \"issue\", \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"Date\"])\n",
        "\n",
        "        # Dictionary to store responses\n",
        "        responses = {}\n",
        "\n",
        "        # First question with different options\n",
        "        st.markdown(f\"**Q00.** What is Your Predicted Mental Issue?\")\n",
        "        responses[\"issue\"] = st.radio(\"\", options=[\"Anxiety\", \"Bipolar\", \"Depression\", \"Normal\", \"PTSD\"], index=2)\n",
        "\n",
        "        # Display remaining questions with radio buttons (1-5 scale)\n",
        "        questions = [\n",
        "            \" When I look at the story of my life, I am pleased with how things have turned out.\",\n",
        "            \" In many ways I feel disappointed about my achievements in life. (R)\",\n",
        "            \" People would describe me as a giving person, willing to share my time with others.\",\n",
        "            \" Maintaining close relationships has been difficult and frustrating for me. (R)\",\n",
        "            \" I have confidence in my own opinions, even if they are different from the way most other people think.\",\n",
        "            \" I tend to be influenced by people with strong opinions. (R)\",\n",
        "            \" In general, I feel I am in charge of the situation in which I live.\",\n",
        "            \" The demands of everyday life often get me down. (R)\",\n",
        "            \" Some people wander aimlessly through life, but I am not one of them.\",\n",
        "            \" I sometimes feel as if I’ve done all there is to do in life. (R)\",\n",
        "            \" For me, life has been a continuous process of learning, changing, and growth.\",\n",
        "            \" I gave up trying to make big improvements or changes in my life a long time ago. (R)\"\n",
        "        ]\n",
        "\n",
        "        for i, question in enumerate(questions, start=1):\n",
        "            st.markdown(f\"**Q{i:02}.** {question}\")  # Fix: Display full question correctly\n",
        "            responses[f\"Q{i}\"] = st.radio(\"\", options=[1, 2, 3, 4, 5, 6], index=2, horizontal=True, key=f\"q{i}\")\n",
        "            st.write(\"\\n\")  # Add a blank line after each question\n",
        "\n",
        "        # Add current date\n",
        "        responses[\"Date\"] = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        # Submit button\n",
        "        if st.button(\"Submit Responses\"):\n",
        "            st.success(\"Responses Submitted Successfully!\")\n",
        "\n",
        "            # Display overall scores\n",
        "            overall_score(responses)\n",
        "\n",
        "            # Convert responses to DataFrame\n",
        "            new_entry = pd.DataFrame([responses])\n",
        "\n",
        "            # Append new entry to existing DataFrame\n",
        "            df = pd.concat([df, new_entry], ignore_index=True)\n",
        "\n",
        "            # Save updated DataFrame back to CSV\n",
        "            df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "            # Show last five responses\n",
        "            with st.expander(\"View Last 5 Responses:\"):\n",
        "                last_five = df.tail(5)\n",
        "                st.dataframe(last_five)\n",
        "\n",
        "            total_respondents = sum(1 for _ in open(csv_file_path)) - 1\n",
        "\n",
        "            # Count total respondents for today\n",
        "            today_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "            total_today = df[df[\"Date\"] == today_date].shape[0]\n",
        "            st.write(f\"**Total Number of Respondents:** {total_respondents}\")\n",
        "            st.write(f\"**Total Number of Respondents on ({today_date}):** {total_today}\")\n",
        "\n",
        "            update_am_csv(csv_file_path, am_file_path)\n",
        "\n",
        "\n",
        "# Run the app\n",
        "if __name__ == '__main__':\n",
        "    run_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDpTEyJaxJRz",
        "outputId": "a3979426-98b7-4f56-cd19-cb39969f998c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing v13.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NGROK TUNNEL"
      ],
      "metadata": {
        "id": "W5-7LHvHxjR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "from pyngrok import conf\n",
        "conf.get_default().skip_browser_warning = True\n",
        "\n",
        "# Set your authtoken\n",
        "ngrok.set_auth_token(\"2ohUKqk37HcGbvwN0s8Y1E2WNxE_39z1gVF3bYq9vFSEm7Wzq\") # Replace YOUR_AUTHTOKEN with your actual authtoken\n",
        "\n",
        "# Kill any existing ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit with nohup\n",
        "!nohup streamlit run v13.py &\n",
        "\n",
        "# Create a public URL with ngrok to access the app\n",
        "# public_url = ngrok.connect(addr='8501')\n",
        "public_url = ngrok.connect(8501, hostname=\"becoming-positively-tapir.ngrok-free.app\")\n",
        "\n",
        "\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAIWgyj0xnw-",
        "outputId": "0719d62b-5f32-42e3-c9d9-63c183fd5e72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "Public URL: NgrokTunnel: \"https://becoming-positively-tapir.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "oF1nALKPxpgV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "1cfb8784-c8f1-48d1-ad5e-e795eb9a9efc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ngrok' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-582e608e03de>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ngrok' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import tiktoken\n",
        "    print(\"tiktoken is installed.\")\n",
        "except ImportError:\n",
        "    print(\"tiktoken is NOT installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRLIXyT9kEJQ",
        "outputId": "0babf1e0-29ea-45d0-fc6c-3cd90bf0baab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken is installed.\n"
          ]
        }
      ]
    }
  ]
}