{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "KkQg0KzlRzxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF"
      ],
      "metadata": {
        "id": "GvjRyDYFR4dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the TfidfVectorizer and fit/transform the cleaned text\n",
        "TFIDFvectorizer = TfidfVectorizer()\n",
        "X = TFIDFvectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "LRmodel = LogisticRegression(multi_class='ovr', max_iter=5000)\n",
        "\n",
        "# Train the model\n",
        "LRmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = LRmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Oz5PiSqR89K",
        "outputId": "15097ae8-866b-4729-9ec2-e0487a667f60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 86.02%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.85      0.70      0.77       379\n",
            "     bipolar       0.85      0.47      0.60       384\n",
            "  depression       0.75      0.76      0.76       373\n",
            "      normal       0.87      1.00      0.93      2183\n",
            "        ptsd       0.91      0.73      0.81       394\n",
            "\n",
            "    accuracy                           0.86      3713\n",
            "   macro avg       0.85      0.73      0.77      3713\n",
            "weighted avg       0.86      0.86      0.85      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 266    7   32   67    7]\n",
            " [   6  180   33  157    8]\n",
            " [  20   19  284   39   11]\n",
            " [   1    2    3 2176    1]\n",
            " [  21    5   27   53  288]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIWC"
      ],
      "metadata": {
        "id": "hsJ2qvOmSZKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install empath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMhg6sbtSm_r",
        "outputId": "1e6e80c4-759d-418e-b541-892ba51dcd43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting empath\n",
            "  Downloading empath-0.89.tar.gz (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from empath) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->empath) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->empath) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->empath) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->empath) (2024.8.30)\n",
            "Building wheels for collected packages: empath\n",
            "  Building wheel for empath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empath: filename=empath-0.89-py3-none-any.whl size=57798 sha256=71173d3f3659c6d8d02dff6c2076564436526401cddc902e4342944e2c4c2b9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/b3/83/9eb2c6199881e2385a59d99bd911363475060ebeb4bdb27242\n",
            "Successfully built empath\n",
            "Installing collected packages: empath\n",
            "Successfully installed empath-0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from empath import Empath\n",
        "\n",
        "# Initialize Empath\n",
        "lexicon = Empath()\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Function to get Empath features\n",
        "def get_empath_features(text):\n",
        "    analysis = lexicon.analyze(text, normalize=True)  # Normalize to get proportions\n",
        "    return analysis\n",
        "\n",
        "# Generate Empath features for each text\n",
        "empath_features = dataset['cleaned_text'].apply(get_empath_features)\n",
        "\n",
        "# Convert Empath features to a DataFrame\n",
        "X = pd.DataFrame(empath_features.tolist())\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "LRmodel = LogisticRegression(multi_class='ovr', max_iter=5000)\n",
        "\n",
        "# Train the model\n",
        "LRmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = LRmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-F8uEyYSrCz",
        "outputId": "30db5e70-5387-458e-8a34-506741664f9a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 66.36%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.64      0.50      0.56       379\n",
            "     bipolar       0.47      0.05      0.09       384\n",
            "  depression       0.52      0.25      0.33       373\n",
            "      normal       0.68      0.97      0.80      2183\n",
            "        ptsd       0.59      0.12      0.20       394\n",
            "\n",
            "    accuracy                           0.66      3713\n",
            "   macro avg       0.58      0.38      0.40      3713\n",
            "weighted avg       0.63      0.66      0.59      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 191    4   11  165    8]\n",
            " [  30   18   33  292   11]\n",
            " [  29    4   92  244    4]\n",
            " [  24   10   23 2115   11]\n",
            " [  24    2   19  301   48]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORD2VEC"
      ],
      "metadata": {
        "id": "lx7Es_YLUzZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "# Download NLTK punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Tokenize the text data into words\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "dataset['tokens'] = dataset['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "# Train a Word2Vec model using the tokenized data\n",
        "word2vec_model = gensim.models.Word2Vec(sentences=dataset['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to average Word2Vec vectors for each document\n",
        "def get_document_vector(tokens):\n",
        "    # Filter out words not in the Word2Vec vocabulary\n",
        "    valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "    if len(valid_tokens) == 0:\n",
        "        return [0] * word2vec_model.vector_size  # Return a zero vector if no valid tokens\n",
        "    # Average the Word2Vec vectors of the words in the document\n",
        "    vectors = [word2vec_model.wv[word] for word in valid_tokens]\n",
        "    return list(np.mean(vectors, axis=0))\n",
        "\n",
        "# Convert the text data into document vectors\n",
        "X = dataset['tokens'].apply(get_document_vector)\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(list(X), y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "LRmodel = LogisticRegression(multi_class='ovr', max_iter=5000)\n",
        "\n",
        "# Train the model\n",
        "LRmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = LRmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju3dpL1KVaZk",
        "outputId": "f4eeaf22-4eb1-4758-eecd-703b9bd21d2b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 79.40%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.75      0.66      0.70       379\n",
            "     bipolar       0.65      0.42      0.51       384\n",
            "  depression       0.59      0.66      0.63       373\n",
            "      normal       0.86      0.96      0.91      2183\n",
            "        ptsd       0.70      0.49      0.57       394\n",
            "\n",
            "    accuracy                           0.79      3713\n",
            "   macro avg       0.71      0.64      0.66      3713\n",
            "weighted avg       0.78      0.79      0.78      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 252   18   41   53   15]\n",
            " [  20  161   51  122   30]\n",
            " [  24   35  248   49   17]\n",
            " [  16   21   31 2095   20]\n",
            " [  24   11   48  119  192]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-GRAM (N=3)"
      ],
      "metadata": {
        "id": "ENJOwSFsWP5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the CountVectorizer with n-grams (e.g., bi-grams)\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))  # Change (1, 2) to (1, 3) for tri-grams or higher-order n-grams\n",
        "\n",
        "# Fit and transform the cleaned text data\n",
        "X = vectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "LRmodel = LogisticRegression(multi_class='ovr', max_iter=5000)\n",
        "\n",
        "# Train the model\n",
        "LRmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = LRmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uw-LGNBWUoE",
        "outputId": "2e7a9154-8d1e-433f-9095-04bce5d279ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 87.13%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.83      0.77      0.80       379\n",
            "     bipolar       0.75      0.53      0.62       384\n",
            "  depression       0.76      0.75      0.75       373\n",
            "      normal       0.91      0.99      0.95      2183\n",
            "        ptsd       0.89      0.74      0.81       394\n",
            "\n",
            "    accuracy                           0.87      3713\n",
            "   macro avg       0.83      0.76      0.79      3713\n",
            "weighted avg       0.87      0.87      0.86      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 293   17   24   33   12]\n",
            " [   7  205   32  131    9]\n",
            " [  24   31  279   26   13]\n",
            " [   2    8    4 2167    2]\n",
            " [  27   11   28   37  291]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NAIVE BAYES"
      ],
      "metadata": {
        "id": "wp7ACyTPWtHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF"
      ],
      "metadata": {
        "id": "_ZZovl_MWy4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the TfidfVectorizer and fit/transform the cleaned text\n",
        "TFIDFvectorizer = TfidfVectorizer()\n",
        "X = TFIDFvectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Naive Bayes model\n",
        "NBmodel = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "NBmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = NBmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZTWjfwyW8Cr",
        "outputId": "8f615678-6aca-4523-8dcf-83513ab6da9a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 79.42%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.84      0.50      0.63       379\n",
            "     bipolar       0.96      0.17      0.30       384\n",
            "  depression       0.69      0.73      0.71       373\n",
            "      normal       0.80      1.00      0.89      2183\n",
            "        ptsd       0.77      0.60      0.68       394\n",
            "\n",
            "    accuracy                           0.79      3713\n",
            "   macro avg       0.81      0.60      0.64      3713\n",
            "weighted avg       0.81      0.79      0.76      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 190    0   43  122   24]\n",
            " [  16   67   49  213   39]\n",
            " [  11    1  274   79    8]\n",
            " [   0    0    3 2180    0]\n",
            " [   9    2   27  118  238]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIWC"
      ],
      "metadata": {
        "id": "gmcEsLwdXFbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from empath import Empath\n",
        "\n",
        "# Initialize Empath\n",
        "lexicon = Empath()\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Function to get Empath features\n",
        "def get_empath_features(text):\n",
        "    analysis = lexicon.analyze(text, normalize=True)  # Normalize to get proportions\n",
        "    return analysis\n",
        "\n",
        "# Generate Empath features for each text\n",
        "empath_features = dataset['cleaned_text'].apply(get_empath_features)\n",
        "\n",
        "# Convert Empath features to a DataFrame\n",
        "X = pd.DataFrame(empath_features.tolist())\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Naive Bayes model\n",
        "NBmodel = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "NBmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = NBmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FISdhfFyXH_t",
        "outputId": "994924b6-62ae-43b9-fc85-59e54ae57695"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 59.63%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.55      0.08      0.14       379\n",
            "     bipolar       0.00      0.00      0.00       384\n",
            "  depression       0.48      0.03      0.06       373\n",
            "      normal       0.60      0.99      0.75      2183\n",
            "        ptsd       1.00      0.00      0.01       394\n",
            "\n",
            "    accuracy                           0.60      3713\n",
            "   macro avg       0.53      0.22      0.19      3713\n",
            "weighted avg       0.56      0.60      0.46      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[  31    0    1  347    0]\n",
            " [  11    0    6  367    0]\n",
            " [   2    0   11  360    0]\n",
            " [   7    2    3 2171    0]\n",
            " [   5    0    2  386    1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORD2VEC"
      ],
      "metadata": {
        "id": "2Pc9JqbeXXQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Download NLTK punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Tokenize the text data into words\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "dataset['tokens'] = dataset['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "# Train a Word2Vec model using the tokenized data\n",
        "word2vec_model = gensim.models.Word2Vec(sentences=dataset['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to average Word2Vec vectors for each document\n",
        "def get_document_vector(tokens):\n",
        "    # Filter out words not in the Word2Vec vocabulary\n",
        "    valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "    if len(valid_tokens) == 0:\n",
        "        return [0] * word2vec_model.vector_size  # Return a zero vector if no valid tokens\n",
        "    # Average the Word2Vec vectors of the words in the document\n",
        "    vectors = [word2vec_model.wv[word] for word in valid_tokens]\n",
        "    return list(np.mean(vectors, axis=0))\n",
        "\n",
        "# Convert the text data into document vectors\n",
        "X = dataset['tokens'].apply(get_document_vector)\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(list(X), y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Scale the training and test data to the range [0, 1]\n",
        "X_train = scaler.fit_transform(X_train) # Fit the scaler on training data and transform\n",
        "X_test = scaler.transform(X_test) # Transform the test data using the fitted scaler\n",
        "\n",
        "\n",
        "# Initialize the Naive Bayes model (MultinomialNB)\n",
        "NBmodel = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "NBmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = NBmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrC70cn8Xe5r",
        "outputId": "da8b0ff5-9640-443f-e603-c3b15fdd0112"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.44%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.55      0.03      0.06       379\n",
            "     bipolar       0.00      0.00      0.00       384\n",
            "  depression       0.44      0.19      0.26       373\n",
            "      normal       0.61      0.99      0.76      2183\n",
            "        ptsd       0.33      0.00      0.01       394\n",
            "\n",
            "    accuracy                           0.60      3713\n",
            "   macro avg       0.39      0.24      0.22      3713\n",
            "weighted avg       0.50      0.60      0.48      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[  11    0   25  343    0]\n",
            " [   6    0   26  351    1]\n",
            " [   1    0   71  301    0]\n",
            " [   0    0   21 2161    1]\n",
            " [   2    0   20  371    1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-GRAM (N=3)"
      ],
      "metadata": {
        "id": "Y3OuGivyXhu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the CountVectorizer with n-grams (e.g., bi-grams)\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))  # Change (1, 2) to (1, 3) for tri-grams or higher-order n-grams\n",
        "\n",
        "# Fit and transform the cleaned text data\n",
        "X = vectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Naive Bayes model (MultinomialNB)\n",
        "NBmodel = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "NBmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = NBmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xKEk6PdXlJb",
        "outputId": "ea7db2ed-b9e3-4179-8858-dbacac6698f4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 81.71%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.72      0.68      0.70       379\n",
            "     bipolar       0.94      0.23      0.37       384\n",
            "  depression       0.54      0.89      0.67       373\n",
            "      normal       0.95      0.93      0.94      2183\n",
            "        ptsd       0.64      0.84      0.72       394\n",
            "\n",
            "    accuracy                           0.82      3713\n",
            "   macro avg       0.76      0.71      0.68      3713\n",
            "weighted avg       0.85      0.82      0.81      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 257    0   72    4   46]\n",
            " [  36   89  104   92   63]\n",
            " [  22    1  331    0   19]\n",
            " [  27    3   66 2026   61]\n",
            " [  14    2   44    3  331]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "0jtXt6OyY46q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF"
      ],
      "metadata": {
        "id": "2PUi6XSMY7cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the TfidfVectorizer and fit/transform the cleaned text\n",
        "TFIDFvectorizer = TfidfVectorizer()\n",
        "X = TFIDFvectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM model\n",
        "SVMmodel = SVC(kernel='linear', random_state=42)  # You can experiment with other kernels like 'rbf'\n",
        "\n",
        "# Train the model\n",
        "SVMmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = SVMmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSzsfsPRY9ZX",
        "outputId": "6bbb58fb-a896-46d4-d94a-37d09c978c25"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 88.26%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.81      0.78      0.80       379\n",
            "     bipolar       0.76      0.61      0.68       384\n",
            "  depression       0.74      0.77      0.76       373\n",
            "      normal       0.93      0.99      0.96      2183\n",
            "        ptsd       0.90      0.76      0.82       394\n",
            "\n",
            "    accuracy                           0.88      3713\n",
            "   macro avg       0.83      0.78      0.80      3713\n",
            "weighted avg       0.88      0.88      0.88      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 295   18   32   23   11]\n",
            " [   8  236   31  101    8]\n",
            " [  30   30  286   14   13]\n",
            " [   2   10    7 2162    2]\n",
            " [  27   16   28   25  298]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIWC"
      ],
      "metadata": {
        "id": "JIwxrNgKZSdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from empath import Empath\n",
        "\n",
        "# Initialize Empath\n",
        "lexicon = Empath()\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Function to get Empath features\n",
        "def get_empath_features(text):\n",
        "    analysis = lexicon.analyze(text, normalize=True)  # Normalize to get proportions\n",
        "    return analysis\n",
        "\n",
        "# Generate Empath features for each text\n",
        "empath_features = dataset['cleaned_text'].apply(get_empath_features)\n",
        "\n",
        "# Convert Empath features to a DataFrame\n",
        "X = pd.DataFrame(empath_features.tolist())\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Support Vector Classifier (SVC) model\n",
        "SVCmodel = SVC(kernel='linear', random_state=42)  # You can try other kernels like 'rbf' as well\n",
        "\n",
        "# Train the model\n",
        "SVCmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = SVCmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlWxI_D6ZcCF",
        "outputId": "a0e14fc4-8a56-43d0-ae7c-01c0deacc46f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 68.14%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.69      0.56      0.62       379\n",
            "     bipolar       0.41      0.08      0.13       384\n",
            "  depression       0.56      0.30      0.39       373\n",
            "      normal       0.70      0.97      0.81      2183\n",
            "        ptsd       0.56      0.15      0.24       394\n",
            "\n",
            "    accuracy                           0.68      3713\n",
            "   macro avg       0.58      0.41      0.44      3713\n",
            "weighted avg       0.64      0.68      0.62      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 214    6   12  134   13]\n",
            " [  23   30   38  277   16]\n",
            " [  23   15  112  215    8]\n",
            " [  24   15   19 2114   11]\n",
            " [  28    7   19  280   60]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORD2VEC"
      ],
      "metadata": {
        "id": "I8VtDwVEZdBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Download NLTK punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Tokenize the text data into words\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "dataset['tokens'] = dataset['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "# Train a Word2Vec model using the tokenized data\n",
        "word2vec_model = gensim.models.Word2Vec(sentences=dataset['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to average Word2Vec vectors for each document\n",
        "def get_document_vector(tokens):\n",
        "    # Filter out words not in the Word2Vec vocabulary\n",
        "    valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "    if len(valid_tokens) == 0:\n",
        "        return [0] * word2vec_model.vector_size  # Return a zero vector if no valid tokens\n",
        "    # Average the Word2Vec vectors of the words in the document\n",
        "    vectors = [word2vec_model.wv[word] for word in valid_tokens]\n",
        "    return list(np.mean(vectors, axis=0))\n",
        "\n",
        "# Convert the text data into document vectors\n",
        "X = dataset['tokens'].apply(get_document_vector)\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(list(X), y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Scale the training and test data to the range [0, 1]\n",
        "X_train = scaler.fit_transform(X_train)  # Fit the scaler on training data and transform\n",
        "X_test = scaler.transform(X_test)  # Transform the test data using the fitted scaler\n",
        "\n",
        "# Initialize the Support Vector Classifier (SVC) model\n",
        "SVCmodel = SVC(kernel='linear', random_state=42)  # You can also try 'rbf' kernel for better performance\n",
        "\n",
        "# Train the model\n",
        "SVCmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = SVCmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uauKD_oZZf3x",
        "outputId": "361a8777-0e9e-4312-b2ff-a47354a52ae6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 77.89%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.68      0.59      0.63       379\n",
            "     bipolar       0.55      0.37      0.44       384\n",
            "  depression       0.56      0.68      0.62       373\n",
            "      normal       0.89      0.95      0.92      2183\n",
            "        ptsd       0.57      0.51      0.54       394\n",
            "\n",
            "    accuracy                           0.78      3713\n",
            "   macro avg       0.65      0.62      0.63      3713\n",
            "weighted avg       0.77      0.78      0.77      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 222   39   50   39   29]\n",
            " [  37  143   50  102   52]\n",
            " [  22   36  255   29   31]\n",
            " [  17   20   38 2070   38]\n",
            " [  30   23   59   80  202]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-GRAM (N=3)"
      ],
      "metadata": {
        "id": "5aWv0_OBZqNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the CountVectorizer with n-grams (e.g., bi-grams)\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))  # Change (1, 2) to (1, 3) for tri-grams or higher-order n-grams\n",
        "\n",
        "# Fit and transform the cleaned text data\n",
        "X = vectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Support Vector Classifier (SVC) model\n",
        "SVCmodel = SVC(kernel='linear', random_state=42)  # You can also try 'rbf' kernel for better performance\n",
        "\n",
        "# Train the model\n",
        "SVCmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = SVCmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEcK-RhVZtJh",
        "outputId": "420d4a69-7625-47ab-8fd8-3b088a9d0fe4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 84.33%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.74      0.74      0.74       379\n",
            "     bipolar       0.58      0.56      0.57       384\n",
            "  depression       0.71      0.73      0.72       373\n",
            "      normal       0.93      0.95      0.94      2183\n",
            "        ptsd       0.83      0.73      0.77       394\n",
            "\n",
            "    accuracy                           0.84      3713\n",
            "   macro avg       0.76      0.74      0.75      3713\n",
            "weighted avg       0.84      0.84      0.84      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 279   26   33   24   17]\n",
            " [  19  215   39   98   13]\n",
            " [  31   34  272   16   20]\n",
            " [  17   70    8 2079    9]\n",
            " [  30   25   33   20  286]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "ikPyALxHd3do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF"
      ],
      "metadata": {
        "id": "QGEe5dEYd6Cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the TfidfVectorizer and fit/transform the cleaned text\n",
        "TFIDFvectorizer = TfidfVectorizer()\n",
        "X = TFIDFvectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the KNN model\n",
        "KNNmodel = KNeighborsClassifier(n_neighbors=5)  # You can experiment with different values for 'n_neighbors'\n",
        "\n",
        "# Train the model\n",
        "KNNmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = KNNmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2VqQt52d8lw",
        "outputId": "c100c783-0e5c-402c-c4d9-5173a7ce7aa7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 75.06%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.68      0.54      0.60       379\n",
            "     bipolar       0.45      0.53      0.48       384\n",
            "  depression       0.55      0.54      0.54       373\n",
            "      normal       0.84      0.95      0.89      2183\n",
            "        ptsd       0.82      0.26      0.39       394\n",
            "\n",
            "    accuracy                           0.75      3713\n",
            "   macro avg       0.67      0.56      0.58      3713\n",
            "weighted avg       0.75      0.75      0.73      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 204   41   40   89    5]\n",
            " [  23  202   37  120    2]\n",
            " [  20   70  201   69   13]\n",
            " [  14   78   11 2078    2]\n",
            " [  38   58   76  120  102]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIWC"
      ],
      "metadata": {
        "id": "mMKzX44zeV-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from empath import Empath\n",
        "\n",
        "# Initialize Empath\n",
        "lexicon = Empath()\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Function to get Empath features\n",
        "def get_empath_features(text):\n",
        "    analysis = lexicon.analyze(text, normalize=True)  # Normalize to get proportions\n",
        "    return analysis\n",
        "\n",
        "# Generate Empath features for each text\n",
        "empath_features = dataset['cleaned_text'].apply(get_empath_features)\n",
        "\n",
        "# Convert Empath features to a DataFrame\n",
        "X = pd.DataFrame(empath_features.tolist())\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the KNN model\n",
        "KNNmodel = KNeighborsClassifier(n_neighbors=5)  # You can experiment with different values for 'n_neighbors'\n",
        "\n",
        "# Train the model\n",
        "KNNmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = KNNmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pfU4uyIe7dh",
        "outputId": "a28c5e37-93fd-47ed-e9f8-700bf44e146b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 70.70%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.54      0.59      0.56       379\n",
            "     bipolar       0.31      0.23      0.26       384\n",
            "  depression       0.44      0.47      0.45       373\n",
            "      normal       0.86      0.91      0.88      2183\n",
            "        ptsd       0.50      0.36      0.42       394\n",
            "\n",
            "    accuracy                           0.71      3713\n",
            "   macro avg       0.53      0.51      0.52      3713\n",
            "weighted avg       0.69      0.71      0.70      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 225   33   47   48   26]\n",
            " [  43   89   48  170   34]\n",
            " [  52   64  175   42   40]\n",
            " [  33   63   53 1995   39]\n",
            " [  66   39   76   72  141]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORD2VEC"
      ],
      "metadata": {
        "id": "_3cPyMWwfInw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Download NLTK punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Tokenize the text data into words\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "dataset['tokens'] = dataset['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "# Train a Word2Vec model using the tokenized data\n",
        "word2vec_model = gensim.models.Word2Vec(sentences=dataset['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to average Word2Vec vectors for each document\n",
        "def get_document_vector(tokens):\n",
        "    # Filter out words not in the Word2Vec vocabulary\n",
        "    valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "    if len(valid_tokens) == 0:\n",
        "        return [0] * word2vec_model.vector_size  # Return a zero vector if no valid tokens\n",
        "    # Average the Word2Vec vectors of the words in the document\n",
        "    vectors = [word2vec_model.wv[word] for word in valid_tokens]\n",
        "    return list(np.mean(vectors, axis=0))\n",
        "\n",
        "# Convert the text data into document vectors\n",
        "X = dataset['tokens'].apply(get_document_vector)\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(list(X), y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Scale the training and test data to the range [0, 1]\n",
        "X_train = scaler.fit_transform(X_train)  # Fit the scaler on training data and transform\n",
        "X_test = scaler.transform(X_test)  # Transform the test data using the fitted scaler\n",
        "\n",
        "# Initialize the KNN model\n",
        "KNNmodel = KNeighborsClassifier(n_neighbors=5)  # You can experiment with different values for 'n_neighbors'\n",
        "\n",
        "# Train the model\n",
        "KNNmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = KNNmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEr7malbfLiv",
        "outputId": "b4ab42b0-2dda-47d6-f8c0-c9515fc27673"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 75.52%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.52      0.60      0.56       379\n",
            "     bipolar       0.46      0.36      0.41       384\n",
            "  depression       0.47      0.59      0.52       373\n",
            "      normal       0.94      0.93      0.93      2183\n",
            "        ptsd       0.54      0.47      0.50       394\n",
            "\n",
            "    accuracy                           0.76      3713\n",
            "   macro avg       0.59      0.59      0.58      3713\n",
            "weighted avg       0.76      0.76      0.75      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 226   33   63   18   39]\n",
            " [  59  138   53   85   49]\n",
            " [  67   40  221    6   39]\n",
            " [  24   38   55 2035   31]\n",
            " [  57   48   77   28  184]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained KNN model as a pickle file\n",
        "with open('KNNmodel.pkl', 'wb') as model_file:\n",
        "    pickle.dump(KNNmodel, model_file)\n",
        "\n",
        "# Save the fitted scaler as a pickle file\n",
        "with open('KNNscaler.pkl', 'wb') as scaler_file:\n",
        "    pickle.dump(scaler, scaler_file)\n",
        "\n",
        "print(\"Model and scaler have been saved as 'KNNmodel.pkl' and 'scaler.pkl' respectively.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_ALBkMw7zhC",
        "outputId": "4f202abf-02da-4bee-eda7-bcc95d50ad5b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and scaler have been saved as 'KNNmodel.pkl' and 'scaler.pkl' respectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-GRAM (N=3)"
      ],
      "metadata": {
        "id": "5Nwx2PsXf6vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the CountVectorizer with n-grams (e.g., bi-grams)\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))  # Change (1, 2) to (1, 3) for tri-grams or higher-order n-grams\n",
        "\n",
        "# Fit and transform the cleaned text data\n",
        "X = vectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the KNN model\n",
        "KNNmodel = KNeighborsClassifier(n_neighbors=5)  # You can experiment with different values of 'n_neighbors'\n",
        "\n",
        "# Train the model\n",
        "KNNmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = KNNmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maNFN93uf9er",
        "outputId": "df7963ed-776b-42c7-9e7b-f6d2a1323f79"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 43.06%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.59      0.12      0.20       379\n",
            "     bipolar       0.13      0.60      0.22       384\n",
            "  depression       0.57      0.10      0.18       373\n",
            "      normal       0.70      0.59      0.64      2183\n",
            "        ptsd       1.00      0.01      0.02       394\n",
            "\n",
            "    accuracy                           0.43      3713\n",
            "   macro avg       0.60      0.29      0.25      3713\n",
            "weighted avg       0.65      0.43      0.44      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[  47  191    6  135    0]\n",
            " [   4  231    7  142    0]\n",
            " [  10  209   39  115    0]\n",
            " [   4  900    1 1278    0]\n",
            " [  15  197   16  162    4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RANDOM FOREST"
      ],
      "metadata": {
        "id": "qgHSlVkUhCq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF"
      ],
      "metadata": {
        "id": "kc7K7MaYhJtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the TfidfVectorizer and fit/transform the cleaned text\n",
        "TFIDFvectorizer = TfidfVectorizer()\n",
        "X = TFIDFvectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest model with the provided hyperparameters\n",
        "RFmodel = RandomForestClassifier(\n",
        "    max_depth=None,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "RFmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = RFmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2TMftaGhJUP",
        "outputId": "6467d2e3-7ead-4bfa-fa50-ac607ad3fc29"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.73%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.79      0.70      0.74       379\n",
            "     bipolar       0.93      0.49      0.64       384\n",
            "  depression       0.71      0.75      0.73       373\n",
            "      normal       0.88      0.99      0.93      2183\n",
            "        ptsd       0.87      0.72      0.79       394\n",
            "\n",
            "    accuracy                           0.86      3713\n",
            "   macro avg       0.84      0.73      0.77      3713\n",
            "weighted avg       0.86      0.86      0.85      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 264    1   34   66   14]\n",
            " [  19  189   42  124   10]\n",
            " [  27    3  280   45   18]\n",
            " [   2   10    3 2168    0]\n",
            " [  21    1   35   55  282]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained Random Forest model as a pickle file\n",
        "with open('RFmodel.pkl', 'wb') as model_file:\n",
        "    pickle.dump(RFmodel, model_file)\n",
        "\n",
        "# Save the fitted TfidfVectorizer as a pickle file\n",
        "with open('RFvectorizer.pkl', 'wb') as vectorizer_file:\n",
        "    pickle.dump(TFIDFvectorizer, vectorizer_file)\n",
        "\n",
        "print(\"Random Forest model and TfidfVectorizer have been saved as 'RFmodel.pkl' and 'TFIDFvectorizer.pkl' respectively.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK4EQ9HgCTRw",
        "outputId": "37093533-b0c3-489f-a60a-e937a40d9820"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest model and TfidfVectorizer have been saved as 'RFmodel.pkl' and 'TFIDFvectorizer.pkl' respectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIWC"
      ],
      "metadata": {
        "id": "6a2jC58BhqIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from empath import Empath\n",
        "\n",
        "# Initialize Empath\n",
        "lexicon = Empath()\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Function to get Empath features\n",
        "def get_empath_features(text):\n",
        "    analysis = lexicon.analyze(text, normalize=True)  # Normalize to get proportions\n",
        "    return analysis\n",
        "\n",
        "# Generate Empath features for each text\n",
        "empath_features = dataset['cleaned_text'].apply(get_empath_features)\n",
        "\n",
        "# Convert Empath features to a DataFrame\n",
        "X = pd.DataFrame(empath_features.tolist())\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest model with provided parameters\n",
        "RFmodel = RandomForestClassifier(\n",
        "    max_depth=None,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "RFmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = RFmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtR7S_pfhuB5",
        "outputId": "7939c61e-5b43-46e0-e9ae-4d7b73898522"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 76.73%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.66      0.67      0.66       379\n",
            "     bipolar       0.47      0.14      0.22       384\n",
            "  depression       0.59      0.64      0.62       373\n",
            "      normal       0.84      0.96      0.90      2183\n",
            "        ptsd       0.63      0.51      0.56       394\n",
            "\n",
            "    accuracy                           0.77      3713\n",
            "   macro avg       0.64      0.58      0.59      3713\n",
            "weighted avg       0.74      0.77      0.74      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 253    3   28   66   29]\n",
            " [  36   54   60  187   47]\n",
            " [  37   10  239   57   30]\n",
            " [   9   42   19 2103   10]\n",
            " [  51    7   57   79  200]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORD2VEC"
      ],
      "metadata": {
        "id": "pwxKEDGxhu4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Download NLTK punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Tokenize the text data into words\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "dataset['tokens'] = dataset['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "# Train a Word2Vec model using the tokenized data\n",
        "word2vec_model = gensim.models.Word2Vec(sentences=dataset['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to average Word2Vec vectors for each document\n",
        "def get_document_vector(tokens):\n",
        "    # Filter out words not in the Word2Vec vocabulary\n",
        "    valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "    if len(valid_tokens) == 0:\n",
        "        return [0] * word2vec_model.vector_size  # Return a zero vector if no valid tokens\n",
        "    # Average the Word2Vec vectors of the words in the document\n",
        "    vectors = [word2vec_model.wv[word] for word in valid_tokens]\n",
        "    return list(np.mean(vectors, axis=0))\n",
        "\n",
        "# Convert the text data into document vectors\n",
        "X = dataset['tokens'].apply(get_document_vector)\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(list(X), y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Scale the training and test data to the range [0, 1]\n",
        "X_train = scaler.fit_transform(X_train)  # Fit the scaler on training data and transform\n",
        "X_test = scaler.transform(X_test)  # Transform the test data using the fitted scaler\n",
        "\n",
        "# Initialize the Random Forest model with the provided parameters\n",
        "RFmodel = RandomForestClassifier(\n",
        "    max_depth=None,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "RFmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = RFmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9XmhQ0vhwsr",
        "outputId": "fde7058a-6890-4769-99fd-cd370f4dc29b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 79.88%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.64      0.59      0.61       379\n",
            "     bipolar       0.58      0.38      0.46       384\n",
            "  depression       0.59      0.68      0.63       373\n",
            "      normal       0.91      0.97      0.94      2183\n",
            "        ptsd       0.64      0.55      0.59       394\n",
            "\n",
            "    accuracy                           0.80      3713\n",
            "   macro avg       0.67      0.63      0.65      3713\n",
            "weighted avg       0.79      0.80      0.79      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 222   31   55   43   28]\n",
            " [  36  145   42  116   45]\n",
            " [  38   26  255   18   36]\n",
            " [   5   23   14 2126   15]\n",
            " [  44   23   65   44  218]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-GRAM (N=3)"
      ],
      "metadata": {
        "id": "SGLrfZqXiTCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the CountVectorizer with n-grams (e.g., bi-grams)\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))  # Change (1, 2) to (1, 3) for tri-grams or higher-order n-grams\n",
        "\n",
        "# Fit and transform the cleaned text data\n",
        "X = vectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest model with default parameters or specific ones as needed\n",
        "RFmodel = RandomForestClassifier(\n",
        "    max_depth=None,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "RFmodel.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = RFmodel.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3FUCvi8iVAo",
        "outputId": "ed21242c-7f28-4a9c-898e-c670d50d09cf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 79.02%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.77      0.52      0.62       379\n",
            "     bipolar       0.95      0.33      0.49       384\n",
            "  depression       0.69      0.63      0.66       373\n",
            "      normal       0.79      1.00      0.88      2183\n",
            "        ptsd       0.85      0.50      0.63       394\n",
            "\n",
            "    accuracy                           0.79      3713\n",
            "   macro avg       0.81      0.60      0.66      3713\n",
            "weighted avg       0.80      0.79      0.77      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 196    1   29  142   11]\n",
            " [  15  127   39  188   15]\n",
            " [  26    2  236  101    8]\n",
            " [   0    3    1 2178    1]\n",
            " [  18    1   37  141  197]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBOOST"
      ],
      "metadata": {
        "id": "RFdenMrukA3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BOW"
      ],
      "metadata": {
        "id": "25mlE0Zyk4Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = data['text']\n",
        "y = data['mental_health_issue']\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to numerical data using CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 3))  # Using unigrams, bigrams, and trigrams\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', num_class=5, eval_metric='mlogloss', use_label_encoder=False)\n",
        "\n",
        "# Train the model\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vKjLLQek7tB",
        "outputId": "7ce3e8de-d6ce-42bc-ef5d-e50435f4d673"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:42:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 87.42%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.83      0.74      0.78       403\n",
            "     bipolar       0.78      0.60      0.68       397\n",
            "  depression       0.74      0.79      0.76       387\n",
            "      normal       0.92      0.99      0.95      2137\n",
            "        ptsd       0.86      0.76      0.81       396\n",
            "\n",
            "    accuracy                           0.87      3720\n",
            "   macro avg       0.83      0.78      0.80      3720\n",
            "weighted avg       0.87      0.87      0.87      3720\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 299   24   32   30   18]\n",
            " [   9  237   37  104   10]\n",
            " [  30   12  307   20   18]\n",
            " [   3   17    6 2107    4]\n",
            " [  20   13   34   27  302]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LISW"
      ],
      "metadata": {
        "id": "P9lvreThlWl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "from empath import Empath\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "\n",
        "# Initialize Empath\n",
        "lexicon = Empath()\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Function to get Empath features\n",
        "def get_empath_features(text):\n",
        "    analysis = lexicon.analyze(text, normalize=True)  # Normalize to get proportions\n",
        "    return analysis\n",
        "\n",
        "# Generate Empath features for each text\n",
        "empath_features = dataset['cleaned_text'].apply(get_empath_features)\n",
        "\n",
        "# Convert Empath features to a DataFrame\n",
        "X = pd.DataFrame(empath_features.tolist())\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Encode target labels using LabelEncoder\n",
        "label_encoder = LabelEncoder() # Initialize LabelEncoder\n",
        "y = label_encoder.fit_transform(y) # Fit and transform the target variable\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=5,  # Adjust the number of classes based on your dataset\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "# Use label_encoder.classes_ to get the original labels for the report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoVnoaEsqKdc",
        "outputId": "f0ad5add-854f-4af2-dfee-b654889d8e74"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:53:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 78.56%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.68      0.64      0.66       379\n",
            "     bipolar       0.47      0.23      0.31       384\n",
            "  depression       0.63      0.62      0.63       373\n",
            "      normal       0.87      0.97      0.92      2183\n",
            "        ptsd       0.65      0.58      0.61       394\n",
            "\n",
            "    accuracy                           0.79      3713\n",
            "   macro avg       0.66      0.61      0.62      3713\n",
            "weighted avg       0.76      0.79      0.77      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 241   15   32   51   40]\n",
            " [  30   90   52  174   38]\n",
            " [  31   36  233   34   39]\n",
            " [   9   27   14 2125    8]\n",
            " [  45   24   40   57  228]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORD2VEC"
      ],
      "metadata": {
        "id": "aJ-cZmcqqVe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "\n",
        "# Download NLTK punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Tokenize the text data into words\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "dataset['tokens'] = dataset['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "# Train a Word2Vec model using the tokenized data\n",
        "word2vec_model = gensim.models.Word2Vec(sentences=dataset['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to average Word2Vec vectors for each document\n",
        "def get_document_vector(tokens):\n",
        "    # Filter out words not in the Word2Vec vocabulary\n",
        "    valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "    if len(valid_tokens) == 0:\n",
        "        return [0] * word2vec_model.vector_size  # Return a zero vector if no valid tokens\n",
        "    # Average the Word2Vec vectors of the words in the document\n",
        "    vectors = [word2vec_model.wv[word] for word in valid_tokens]\n",
        "    return list(np.mean(vectors, axis=0))\n",
        "\n",
        "# Convert the text data into document vectors\n",
        "X = dataset['tokens'].apply(get_document_vector)\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Encode target labels using LabelEncoder\n",
        "label_encoder = LabelEncoder() # Initialize LabelEncoder\n",
        "y = label_encoder.fit_transform(y) # Fit and transform the target variable\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(list(X), y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Scale the training and test data to the range [0, 1]\n",
        "X_train = scaler.fit_transform(X_train)  # Fit the scaler on training data and transform\n",
        "X_test = scaler.transform(X_test)  # Transform the test data using the fitted scaler\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=5,  # Adjust the number of classes based on your dataset\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4d4hUMLqZDd",
        "outputId": "d590eccb-dbcd-48c7-ac80-47c5ca5c95dd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:09:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 81.01%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.65      0.67       379\n",
            "           1       0.55      0.46      0.50       384\n",
            "           2       0.62      0.69      0.65       373\n",
            "           3       0.93      0.96      0.94      2183\n",
            "           4       0.65      0.58      0.62       394\n",
            "\n",
            "    accuracy                           0.81      3713\n",
            "   macro avg       0.69      0.67      0.68      3713\n",
            "weighted avg       0.80      0.81      0.81      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 245   31   42   28   33]\n",
            " [  29  176   36   98   45]\n",
            " [  37   39  257   10   30]\n",
            " [   9   40   19 2100   15]\n",
            " [  34   35   61   34  230]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-GRAM (N=3)"
      ],
      "metadata": {
        "id": "gUMrt0y5qbWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_mental_health.csv')\n",
        "\n",
        "# Check if 'cleaned_text' and 'mental_health_issue' columns exist\n",
        "if 'cleaned_text' not in dataset.columns or 'mental_health_issue' not in dataset.columns:\n",
        "    raise ValueError(\"The dataset must have 'cleaned_text' and 'mental_health_issue' columns.\")\n",
        "\n",
        "# Remove rows with missing values in 'cleaned_text' column\n",
        "dataset.dropna(subset=['cleaned_text'], inplace=True)\n",
        "\n",
        "# Initialize the CountVectorizer with n-grams (e.g., bi-grams)\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))  # Change (1, 2) to (1, 3) for tri-grams or higher-order n-grams\n",
        "\n",
        "# Fit and transform the cleaned text data\n",
        "X = vectorizer.fit_transform(dataset['cleaned_text'])\n",
        "\n",
        "# Prepare the target variable\n",
        "y = dataset['mental_health_issue']\n",
        "\n",
        "# Encode target labels using LabelEncoder\n",
        "label_encoder = LabelEncoder() # Initialize LabelEncoder\n",
        "y = label_encoder.fit_transform(y) # Fit and transform the target variable\n",
        "\n",
        "# Split the dataset into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='multi:softmax',  # Multi-class classification\n",
        "    num_class=len(label_encoder.classes_),  # Number of unique classes # use label_encoder.classes_ to determine num_class\n",
        "    eval_metric='mlogloss',  # Multi-class log loss evaluation metric\n",
        "    use_label_encoder=False,  # To suppress warnings\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "# Use label_encoder.classes_ to get the original labels for the report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEgL9vblqniP",
        "outputId": "8a2d0e34-c0d6-40e4-a3c6-05cdf878c53f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [08:05:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 87.96%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anxiety       0.85      0.75      0.80       379\n",
            "     bipolar       0.81      0.52      0.64       384\n",
            "  depression       0.76      0.80      0.78       373\n",
            "      normal       0.91      0.99      0.95      2183\n",
            "        ptsd       0.90      0.79      0.84       394\n",
            "\n",
            "    accuracy                           0.88      3713\n",
            "   macro avg       0.84      0.77      0.80      3713\n",
            "weighted avg       0.88      0.88      0.87      3713\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 285   15   28   37   14]\n",
            " [   8  201   31  139    5]\n",
            " [  26   20  299   15   13]\n",
            " [   2    3    7 2168    3]\n",
            " [  16   10   28   27  313]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained XGBoost model as a pickle file\n",
        "with open('XGBmodel.pkl', 'wb') as model_file:\n",
        "    pickle.dump(xgb_model, model_file)\n",
        "\n",
        "# Save the fitted CountVectorizer as a pickle file\n",
        "with open('XGBvectorizer.pkl', 'wb') as vectorizer_file:\n",
        "    pickle.dump(vectorizer, vectorizer_file)\n",
        "\n",
        "# Save the fitted LabelEncoder as a pickle file\n",
        "with open('XGBlabel_encoder.pkl', 'wb') as label_encoder_file:\n",
        "    pickle.dump(label_encoder, label_encoder_file)\n",
        "\n",
        "print(\"XGBoost model, CountVectorizer, and LabelEncoder have been saved as 'xgb_model.pkl', 'vectorizer.pkl', and 'label_encoder.pkl' respectively.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fa83LrWBqX1",
        "outputId": "60ae619a-02a8-4dc7-f5a7-1b640a4a6a3f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost model, CountVectorizer, and LabelEncoder have been saved as 'xgb_model.pkl', 'vectorizer.pkl', and 'label_encoder.pkl' respectively.\n"
          ]
        }
      ]
    }
  ]
}